{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('python_modules')",
   "metadata": {
    "interpreter": {
     "hash": "9634d4b890a726f71d8044046bc71cdc391c406dc663847f104c7db04bcb4cdc"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel, sigmoid_kernel, rbf_kernel, polynomial_kernel, laplacian_kernel, cosine_similarity\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from parseit.data import load_pickle, save_pickle\n",
    "import pandas as pd\n",
    "\n",
    "pd.options.display.min_rows = 650\n",
    "pd.options.display.max_rows = 1000\n",
    "pd.options.display.max_columns = 20\n",
    "\n",
    "# ranksnl stopwords (large set) was chosen; read about stopwords here (and why nltk stopwords should not be chosen):\n",
    "# https://scikit-learn.org/stable/modules/feature_extraction.html#stop-words\n",
    "# https://www.aclweb.org/anthology/W18-2502/\n",
    "def stopwords():\n",
    "    filename = os.path.join(os.getcwd(), \"datasets\", \"ranksnl_large.csv\")\n",
    "    print(filename)\n",
    "    stop_words = []\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            for word in line.split(\",\"):\n",
    "                stop_words.append(word.lower().strip())\n",
    "    return set(stop_words)\n",
    "\n",
    "# There's many lists for bad words (for spam deteciton, moderation, etc). This is one of the largest I've found so far:\n",
    "# https://www.freewebheaders.com/bad-words-list-and-page-moderation-words-list-for-facebook/\n",
    "def badwords():\n",
    "    filename = os.path.join(os.getcwd(), \"datasets\", \"fb-bad-words.csv\")\n",
    "    bad_words = []\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            for word in line.split(\",\"):\n",
    "                bad_words.append(word.lower().strip())\n",
    "    return set(bad_words)\n",
    "\n",
    "# From https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0144296:\n",
    "# http://kt.ijs.si/data/Emoji_sentiment_ranking/\n",
    "# Other: https://research.utwente.nl/files/5482763/sac13-senticon.pdf\n",
    "# http://emojitracker.com/\n",
    "def emoticons():\n",
    "    filename = os.path.join(os.getcwd(), \"datasets\", \"emoticons.csv\")\n",
    "    emoticons = {\n",
    "        \"pos\": [\":)\", \":))\", \":D\", \":DD\", \"xD\", \"xDD\", \":d\" \"=)\", \"=))\", \":')\", \"=')\", \":}\", \":}}\", \":]\", \":]]\", \"(:\", \"C:\", \":P\"],\n",
    "        \"neu\": [\":|\", \":/\", \":\\\\\", \"\"],\n",
    "        \"neg\": [\":(\", \":((\", \":(((\", \":((((\", \":C\", \":CC\", \":'C\", \"=(\", \"=((\", \":'(\", \"='(\", \":[\", \":{\", \"):\"]\n",
    "    }\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            word = line.split(\",\")\n",
    "            smil = word[0]\n",
    "            #unicodesmil = chr(int(word[1], 0))\n",
    "            neg = float(word[3])\n",
    "            neu = float(word[4])\n",
    "            pos = float(word[5])\n",
    "            if neu > pos and neu > neg:\n",
    "                emoticons[\"neu\"].append(smil)\n",
    "            elif pos > neg:\n",
    "                emoticons[\"pos\"].append(smil)\n",
    "                #emoticons[\"pos\"].append(unicodesmil)\n",
    "            else:\n",
    "                emoticons[\"neg\"].append(smil)\n",
    "    return emoticons\n",
    "\n",
    "# Implement our own tokenizer compatible with sklearn; we want to be able to define stopwords and vocabulary\n",
    "class LemmaTokenizer:\n",
    "    ignore_tokens = [',', '.', ';', ':', '\"', '``', \"''\", '`', '[removed]', '>', '*', '_', \"&\", \"$\"]\n",
    "    stopwords = []\n",
    "    vocab = []\n",
    "    def __init__(self, stopwords=[], vocabulary=[]):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "        self.stopwords = stopwords\n",
    "        self.vocab = vocabulary\n",
    "    def __call__(self, document):\n",
    "        sig_words = []\n",
    "        for word, tag in pos_tag(word_tokenize(document)):\n",
    "            lower_cased_tag = tag[0].lower()\n",
    "            wn_tag = lower_cased_tag if lower_cased_tag in ['a', 'r', 'n', 'v'] else None\n",
    "            if not wn_tag:\n",
    "                lemma = word\n",
    "            else:\n",
    "                lemma = self.wnl.lemmatize(word, wn_tag)\n",
    "            if lemma not in list(self.stopwords) + self.ignore_tokens: # and word in self.vocab:\n",
    "                sig_words.append(lemma.lower())\n",
    "        return sig_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_file_name = \"99\"\n",
    "pickle_df = load_pickle(f\"{pickle_file_name}.p\")\n",
    "pickle_df = pickle_df[pickle_df[\"body\"] != \"[removed]\"]\n",
    "pickle_df = pickle_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/home/halpdesk/CODE/reddit-parser/datasets/ranksnl_large.csv\n43556 comments in 698 submissions\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'filename' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-5b926afaec4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{s_length} comments in {len(submissions)} submissions\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mcva\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLemmaTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0masdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcva\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"body\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mvocabs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcva\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-32d162c0b988>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, stopwords, vocabulary)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwnl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'filename' is not defined"
     ]
    }
   ],
   "source": [
    "df = pickle_df.copy()\n",
    "\n",
    "swords = stopwords()\n",
    "bwords = badwords()\n",
    "smil = emoticons()\n",
    "\n",
    "\n",
    "s_length = len(df.index)\n",
    "submissions = set(df.get(\"submission\"))\n",
    "print(f\"{s_length} comments in {len(submissions)} submissions\")\n",
    "\n",
    "cva = CountVectorizer(lowercase=True, tokenizer=LemmaTokenizer())\n",
    "asdf = cva.fit_transform(df.get(\"body\"))\n",
    "vocabs = cva.get_feature_names()\n",
    "print(\"Created vocabs\")\n",
    "\n",
    "cvf = CountVectorizer(stop_words=list(swords)+list(bwords), lowercase=True, tokenizer=LemmaTokenizer(stopwords=list(swords)+list(bwords)))\n",
    "asdf = cvf.fit_transform(df.get(\"body\"))\n",
    "filtered_vocabs = cvf.get_feature_names()\n",
    "print(\"Created filtered vocabs\")\n",
    "\n",
    "# Significant words count\n",
    "cv = CountVectorizer(vocabulary=vocabs, stop_words=list(swords)+list(bwords), lowercase=True, tokenizer=LemmaTokenizer(stopwords=list(swords)+list(bwords)))\n",
    "wc_data = cv.fit_transform(df.get(\"body\"))\n",
    "wc_df = pd.DataFrame(wc_data.sum(axis=1))\n",
    "wc_df.columns = [\"wc\"]\n",
    "print(\"Feature: Word count [DONE]\")\n",
    "\n",
    "# Bad words count\n",
    "cv = CountVectorizer(vocabulary=bwords, stop_words=None, lowercase=True, tokenizer=LemmaTokenizer(), ngram_range=(1,2)) # finds \"old bag\"\n",
    "bw_data = cv.fit_transform(df.get(\"body\"))\n",
    "bw_df = pd.DataFrame(bw_data.sum(axis=1))\n",
    "bw_df.columns = [\"bw\"]\n",
    "print(\"Feature: Bad words [DONE]\")\n",
    "\n",
    "# Stop words count\n",
    "cv = CountVectorizer(vocabulary=swords, stop_words=None, lowercase=True, tokenizer=LemmaTokenizer())\n",
    "sw_data = cv.fit_transform(df.get(\"body\"))\n",
    "sw_df = pd.DataFrame(sw_data.sum(axis=1))\n",
    "sw_df.columns = [\"sw\"]\n",
    "print(\"Feature: Stop words [DONE]\")\n",
    "\n",
    "# Positive smilies\n",
    "cv = CountVectorizer(vocabulary=smil[\"pos\"], analyzer=\"char\", ngram_range=(1,2), stop_words=None, lowercase=False) # char + 2 ngram = \":D\"\n",
    "smilp_data = cv.fit_transform(df.get(\"body\"))\n",
    "smilp_df = pd.DataFrame(smilp_data.sum(axis=1))\n",
    "smilp_df.columns = [\"smil+\"]\n",
    "print(\"Feature: positive smiles [DONE]\")\n",
    "\n",
    "# Negative smilies count\n",
    "cv = CountVectorizer(vocabulary=smil[\"neg\"], analyzer=\"char\", ngram_range=(1,2), stop_words=None, lowercase=False) # char + 2 ngram = \":D\"\n",
    "sniln_data = cv.fit_transform(df.get(\"body\"))\n",
    "smiln_df = pd.DataFrame(sniln_data.sum(axis=1))\n",
    "smiln_df.columns = [\"smil-\"]\n",
    "print(\"Feature: Negative smiles [DONE]\")\n",
    "\n",
    "# Neutral smilies count\n",
    "cv = CountVectorizer(vocabulary=smil[\"neu\"], analyzer=\"char\", ngram_range=(1,2), stop_words=None, lowercase=False) # char + 2 ngram = \":D\"\n",
    "smile_data = cv.fit_transform(df.get(\"body\"))\n",
    "smile_df = pd.DataFrame(smile_data.sum(axis=1))\n",
    "smile_df.columns = [\"smil&\"]\n",
    "print(\"Feature: Neutral smiles [DONE]\")\n",
    "\n",
    "\n",
    "# TF-IDF cosine similarity toawrd topic\n",
    "df[\"top-cos-sim\"] = pd.Series(np.zeros(s_length), index=df.index)\n",
    "for submission in submissions:\n",
    "    sub_df = df[df.submission == submission][[\"index\", \"body\"]]\n",
    "    submission_with_comments = [submission] + list(sub_df.get(\"body\").array)\n",
    "    tfidfv = TfidfVectorizer(vocabulary=filtered_vocabs, lowercase=True, ngram_range=(1,1), smooth_idf=True, tokenizer=LemmaTokenizer(stopwords=list(swords)))\n",
    "    tfidf_data = tfidfv.fit_transform(submission_with_comments)\n",
    "    cosine_similarities = cosine_similarity(tfidf_data[0:1], tfidf_data[1:]).flatten()\n",
    "    top_simi_df = pd.DataFrame(cosine_similarities, index=sub_df.index)\n",
    "    top_simi_df.columns = [\"top-cos-sim\"]\n",
    "    sub_df = pd.concat([sub_df, top_simi_df], axis=1)\n",
    "    df.update(sub_df)\n",
    "print(\"Feature: Cosine similarity with topic [DONE]\")\n",
    "\n",
    "\n",
    "# TF-IDF cosine similarity towards all documents in a submission\n",
    "df[\"cos-sim\"] = pd.Series(np.zeros(s_length), index=df.index)\n",
    "for submission in submissions:\n",
    "    sub_df = df[df.submission == submission][[\"index\", \"body\"]]\n",
    "    submission_with_comments = [\" \".join(vocabs)] + list(sub_df.get(\"body\").array)\n",
    "    tfidfv = TfidfVectorizer(vocabulary=filtered_vocabs, lowercase=True, ngram_range=(1,1), smooth_idf=True, tokenizer=LemmaTokenizer(stopwords=list(swords)))\n",
    "    tfidf_data = tfidfv.fit_transform(submission_with_comments)\n",
    "    cosine_similarities = cosine_similarity(tfidf_data[0:1], tfidf_data[1:]).flatten()\n",
    "    all_simi_df = pd.DataFrame(cosine_similarities, index=sub_df.index)\n",
    "    all_simi_df.columns = [\"cos-sim\"]\n",
    "    sub_df = pd.concat([sub_df, all_simi_df], axis=1)\n",
    "    df.update(sub_df)\n",
    "print(\"Feature: Cosine similarity with rest [DONE]\")\n",
    "\n",
    "\n",
    "# TF-IDF mean value (checks across all documents in a submission)\n",
    "df[\"tfidf-mean\"] = pd.Series(np.zeros(s_length), index=df.index)\n",
    "for submission in submissions:\n",
    "    sub_df = df[df.submission == submission][[\"index\", \"body\"]]\n",
    "    tfidfv = TfidfVectorizer(vocabulary=vocabs, lowercase=True, ngram_range=(1,1), smooth_idf=True, tokenizer=LemmaTokenizer(stopwords=list(swords)))\n",
    "    tfidf_data = tfidfv.fit_transform(sub_df.get(\"body\")).todense()\n",
    "    means = [0]*tfidf_data.shape[0]\n",
    "    for i in range(0, tfidf_data.shape[0]):\n",
    "        word_count = wc_df.get(\"wc\")[i] + bw_df.get(\"bw\")[i] + sw_df.get(\"sw\")[i]\n",
    "        means[i] = tfidf_data[i].sum()/word_count\n",
    "    tfidf_df = pd.DataFrame(means, index=sub_df.index)\n",
    "    tfidf_df.columns = [\"tfidf-mean\"]\n",
    "    sub_df = pd.concat([sub_df, tfidf_df], axis=1)\n",
    "    df.update(sub_df)\n",
    "print(\"Feature: Mean TF-IDF [DONE]\")\n",
    "\n",
    "# Update the the rest\n",
    "df = pd.concat([df, wc_df, sw_df, bw_df, smilp_df, smiln_df, smile_df], axis=1)\n",
    "print(\"Updated word counts\")\n",
    "\n",
    "save_pickle(df, f\"{pickle_file_name}-with-features.p\")\n",
    "df\n",
    "\n"
   ]
  }
 ]
}