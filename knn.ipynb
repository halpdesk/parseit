{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('python_modules')",
   "metadata": {
    "interpreter": {
     "hash": "9634d4b890a726f71d8044046bc71cdc391c406dc663847f104c7db04bcb4cdc"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from math import sqrt\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, KBinsDiscretizer\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score\n",
    "import math\n",
    "from parseit.data import load_pickle\n",
    "\n",
    "# Load pickle\n",
    "pickle_df_with_features = load_pickle(f\"99-with-features.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy (SFK): 9.40% (1.28%)\n",
      "\n",
      "Accuracy: 7.45% (14 comments of 187) for k=50\n",
      "\n",
      "[    5     2     2     5     2     2     5     5     2     2   118     2\n",
      "   194     2     2     2     2   272   163     2   596     2     5     2\n",
      "     5   194     2     2     2     2  5220   230     5   272     2   194\n",
      "     2     2     2   596     2     2     2     2     2   163     2     2\n",
      "     2     2     2     2     2     2    19     3   163     2     2     2\n",
      "  8718     2     2     2     2   111   194  9216     2     2     5     4\n",
      "     2    18     7     2     2     2     2     2     5     2    37    75\n",
      "     2     2     2    75     2    62     2   194     2     2     2     2\n",
      " 13588  3050     2     5     2     2   118     2     2     2   410     2\n",
      "     2     2     2     2     2   111    75     2     2   111     2     2\n",
      "     2   163     2     2     2  5220     2     2     2     5     2     5\n",
      "     2     2     2     2     2     2     2    73     5     2 34290     2\n",
      "     2     2     2   202     3    75     5     2    37 14518     2     2\n",
      "     2     2     2     2  1062     2     2     2     2     2     2     2\n",
      "     2     2     2    62  1713     2   163   127     2 14518     2     3\n",
      "     2     2     2     2    93     2     2     2]\n",
      "Linear SVC Accuracy: 6.91% (13 comments of 187)\n",
      "Accuracy: 0.53% (1 comments of 187) for k=1\n",
      "Accuracy: 3.72% (7 comments of 187) for k=6\n",
      "Accuracy: 5.85% (11 comments of 187) for k=11\n",
      "Accuracy: 7.45% (14 comments of 187) for k=16\n",
      "Accuracy: 6.91% (13 comments of 187) for k=21\n",
      "Accuracy: 6.38% (12 comments of 187) for k=26\n",
      "Accuracy: 6.38% (12 comments of 187) for k=31\n",
      "Accuracy: 6.91% (13 comments of 187) for k=36\n",
      "Accuracy: 7.45% (14 comments of 187) for k=41\n",
      "Accuracy: 7.45% (14 comments of 187) for k=46\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "label\n2        61\n3        35\n5        23\n4        15\n6        13\n7         8\n12        7\n18        7\n9         6\n26        5\n15        5\n14        5\n8         5\n10        5\n20        5\n62        4\n78        4\n36        4\n17        4\n41        4\n60        3\n27        3\n45        3\n63        3\n336       3\n136       3\n21        3\n24        3\n23        3\n19        3\n11        3\n150       2\n163       2\n66        2\n53        2\n167       2\n370       2\n88        2\n58        2\n238       2\n185       2\n54        2\n190       2\n52        2\n84        2\n13        2\n16        2\n127       2\n28        2\n34        2\n206       2\n37        2\n40        2\n106       2\n105       2\n236       2\n83        2\n295       1\n290       1\n286       1\n298       1\n285       1\n284       1\n281       1\n293       1\n63821     1\n280       1\n307       1\n224       1\n227       1\n229       1\n230       1\n231       1\n234       1\n239       1\n277       1\n253       1\n255       1\n258       1\n259       1\n272       1\n275       1\n302       1\n393       1\n308       1\n462       1\n425       1\n445       1\n448       1\n454       1\n456       1\n460       1\n495       1\n416       1\n499       1\n505       1\n508       1\n510       1\n529       1\n530       1\n421       1\n410       1\n312       1\n350       1\n314       1\n317       1\n321       1\n326       1\n327       1\n338       1\n362       1\n400       1\n364       1\n366       1\n384       1\n388       1\n392       1\n220       1\n223       1\n147       1\n217       1\n102       1\n99        1\n98        1\n94        1\n93        1\n92        1\n90        1\n89        1\n81        1\n79        1\n76        1\n75        1\n74        1\n73        1\n71        1\n70        1\n64        1\n61        1\n59        1\n56        1\n55        1\n51        1\n49        1\n48        1\n47        1\n44        1\n33        1\n32        1\n31        1\n22        1\n101       1\n104       1\n209       1\n109       1\n208       1\n202       1\n201       1\n199       1\n196       1\n194       1\n182       1\n174       1\n166       1\n164       1\n161       1\n160       1\n159       1\n158       1\n157       1\n155       1\n149       1\n148       1\n579       1\n137       1\n135       1\n134       1\n132       1\n131       1\n126       1\n118       1\n117       1\n115       1\n111       1\n549       1\n596       1\n46697     1\n11854     1\n9516      1\n9524      1\n9898      1\n10035     1\n10327     1\n10889     1\n10892     1\n11314     1\n11402     1\n11965     1\n5109      1\n11971     1\n12149     1\n12243     1\n12436     1\n12474     1\n12816     1\n13076     1\n13195     1\n13588     1\n9216      1\n8838      1\n8718      1\n8656      1\n5239      1\n5354      1\n5454      1\n5458      1\n5496      1\n5549      1\n5845      1\n6091      1\n6597      1\n6662      1\n6691      1\n6924      1\n6990      1\n7242      1\n7245      1\n7356      1\n8292      1\n8362      1\n8508      1\n13685     1\n13815     1\n13915     1\n23916     1\n25197     1\n25355     1\n25626     1\n28826     1\n28935     1\n29966     1\n31027     1\n31528     1\n33101     1\n34290     1\n36120     1\n37791     1\n38755     1\n42485     1\n43314     1\n45018     1\n45038     1\n45583     1\n45597     1\n25105     1\n23491     1\n14316     1\n23127     1\n14426     1\n14518     1\n14594     1\n14643     1\n14662     1\n14726     1\n15341     1\n15932     1\n16402     1\n16553     1\n16706     1\n17313     1\n17502     1\n17539     1\n19929     1\n20399     1\n22147     1\n22237     1\n22653     1\n5220      1\n5070      1\n602       1\n1223      1\n1039      1\n1054      1\n1056      1\n1062      1\n1071      1\n1146      1\n1183      1\n1206      1\n1209      1\n1226      1\n4931      1\n1292      1\n1336      1\n1343      1\n1413      1\n1433      1\n1489      1\n1519      1\n1536      1\n1562      1\n1029      1\n1016      1\n971       1\n969       1\n626       1\n640       1\n650       1\n663       1\n667       1\n669       1\n678       1\n684       1\n702       1\n705       1\n741       1\n746       1\n758       1\n820       1\n839       1\n917       1\n930       1\n957       1\n959       1\n1594      1\n1612      1\n1615      1\n2885      1\n3050      1\n3223      1\n3265      1\n3374      1\n3410      1\n3513      1\n3796      1\n3852      1\n3893      1\n4083      1\n4090      1\n4118      1\n4333      1\n4379      1\n4439      1\n4574      1\n4624      1\n4675      1\n4782      1\n3032      1\n2691      1\n1620      1\n2689      1\n1630      1\n1636      1\n1713      1\n1723      1\n1769      1\n1816      1\n1817      1\n1911      1\n1927      1\n1982      1\n2082      1\n2086      1\n2122      1\n2126      1\n2298      1\n2377      1\n2428      1\n2481      1\n2645      1\n1         1\ndtype: int64"
     },
     "metadata": {}
    }
   ],
   "source": [
    "df = pickle_df_with_features.copy()\n",
    "\n",
    "\n",
    "k = 50\n",
    "bins = 10\n",
    "test_size = 0.3\n",
    "X = dfwf[[\"top-cos-sim\", \"cos-sim\", \"tfidf-mean\", \"wc\", \"sw\", \"bw\", \"smil+\", \"smil-\", \"smil&\"]]\n",
    "#X = dfwf[[\"top-cos-sim\", \"wc\", \"bw\", \"smil+\"]]\n",
    "y = dfwf[[\"label\"]]\n",
    "\n",
    "\n",
    "# Bin all of y first\n",
    "# uniform = All bins in each feature have identical widths.\n",
    "# quantile = All bins in each feature have the same number of points.\n",
    "# kmeans = Values in each bin have the same nearest center of a 1D k-means cluster.\n",
    "#est = KBinsDiscretizer(n_bins=bins, encode=\"ordinal\", strategy=\"uniform\") #\n",
    "#est.fit(y)\n",
    "#y = pd.DataFrame(data=est.transform(y), index=y.index)\n",
    "\n",
    "#drop = y[y == 0][100:]\n",
    "#y = y.drop(index=drop.index)\n",
    "#X = X.drop(index=drop.index)\n",
    "#display(y)\n",
    "\n",
    "X_unscaled_train, X_unscaled_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# All variables must be in the same scale: normalization or min-max-scaler\n",
    "# How to scale so that each distance is meaningful: https://medium.com/analytics-vidhya/why-is-scaling-required-in-knn-and-k-means-8129e4d88ed7\n",
    "scaler = StandardScaler()\n",
    "#scaler = MinMaxScaler()\n",
    "scaler.fit(X_unscaled_train)\n",
    "X_train = scaler.transform(X_unscaled_train)\n",
    "\n",
    "\n",
    "# Try with SKF\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)\n",
    "knn = KNeighborsClassifier(n_neighbors=k)\n",
    "n_scores = cross_val_score(knn, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score=\"raise\")\n",
    "# report model performance\n",
    "print(\"Accuracy (SFK): %.2f%% (%.2f%%)\\n\" % (mean(n_scores)*100, std(n_scores)*100))\n",
    "\n",
    "\n",
    "# Try without SKF\n",
    "knn = KNeighborsClassifier(n_neighbors=k)\n",
    "clf = knn.fit(X_train, y_train)\n",
    "y_pred = clf.predict(scaler.transform(X_unscaled_valid))\n",
    "#print(y_pred)\n",
    "accuracy = metrics.accuracy_score(y_valid, y_pred, normalize=False)\n",
    "accuracy_norm = metrics.accuracy_score(y_valid, y_pred, normalize=True)\n",
    "print(f'Accuracy: {\"%.2f\" % (accuracy_norm * 100)}% ({accuracy} comments of {math.floor(len(y.index)*test_size)}) for k={k}\\n')\n",
    "\n",
    "# Try with LinearSVC\n",
    "svc = LinearSVC(random_state=0, tol=1e-05)\n",
    "clf = svc.fit(X_train, y_train)\n",
    "# Scale X_valid after fitting the model: https://datascience.stackexchange.com/questions/38395/standardscaler-before-and-after-splitting-data\n",
    "y_pred = clf.predict(scaler.transform(X_unscaled_valid))\n",
    "print(y_pred)\n",
    "accuracy = metrics.accuracy_score(y_valid, y_pred, normalize=False)\n",
    "accuracy_norm = metrics.accuracy_score(y_valid, y_pred, normalize=True)\n",
    "print(f'Linear SVC Accuracy: {\"%.2f\" % (accuracy_norm * 100)}% ({accuracy} comments of {math.floor(len(y.index)*test_size)})')\n",
    "\n",
    "# Try different ranges for k\n",
    "k_range = range(1, k, math.floor(k/10))\n",
    "for kk in k_range:\n",
    "    knn = KNeighborsClassifier(n_neighbors=kk)\n",
    "    clf = knn.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(scaler.transform(X_unscaled_valid))\n",
    "    accuracy = metrics.accuracy_score(y_valid, y_pred, normalize=False)\n",
    "    accuracy_norm = metrics.accuracy_score(y_valid, y_pred, normalize=True)\n",
    "    print(f'Accuracy: {\"%.2f\" % (accuracy_norm * 100)}% ({accuracy} comments of {math.floor(len(y.index)*test_size)}) for k={kk}')\n",
    "    \n",
    "\n",
    "\n",
    "display(y.value_counts())"
   ]
  }
 ]
}