{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('python_modules')",
   "metadata": {
    "interpreter": {
     "hash": "9634d4b890a726f71d8044046bc71cdc391c406dc663847f104c7db04bcb4cdc"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parseit.data import load_pickle, save_pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "from functools import lru_cache\n",
    "\n",
    "pd.options.display.min_rows = 650\n",
    "pd.options.display.max_rows = 1000\n",
    "pd.options.display.max_columns = 20\n",
    "\n",
    "# ranksnl stopwords (large set) was chosen; read about stopwords here (and why nltk stopwords should not be chosen):\n",
    "# https://scikit-learn.org/stable/modules/feature_extraction.html#stop-words\n",
    "# https://www.aclweb.org/anthology/W18-2502/\n",
    "def stopwords():\n",
    "    filename = os.path.join(os.getcwd(), \"datasets\", \"ranksnl_large.csv\")\n",
    "    stop_words = []\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            for word in line.split(\",\"):\n",
    "                stop_words.append(word.lower().strip())\n",
    "    return set(stop_words)\n",
    "\n",
    "def ignoretokens():\n",
    "    tokens = [',', '.', ';', ':', '\"', '``', \"''\", '`', '[removed]', '>', '*', '_', \"&\", \"$\", \"!\", \"#\", \"%\", \"'\", \"”\", \"“\", \"’\", \"‘\", \"―\", \"—\", \"~\", \"–\", \"+\", \"/\", \"-\", \"\\\\\", \"(\", \")\", \" \", \"  \" , \"\\n\", \"\\t\", \"\\r\\n\", \"\\r\", \"\t\"]\n",
    "    return ''.join(tokens)\n",
    "\n",
    "# Implement our own tokenizer compatible with sklearn; we want to be able to define stopwords\n",
    "class LemmaTokenizer:\n",
    "    ignoretokens = \"\"\n",
    "    stopwords = []\n",
    "    def __init__(self, stopwords=[], ignoretokens=\"\"):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "        self.stopwords = stopwords\n",
    "        self.ignoretokens = ignoretokens\n",
    "    def __call__(self, document):\n",
    "        sig_words = []\n",
    "        for word, tag in pos_tag(word_tokenize(document)):\n",
    "            lower_cased_tag = tag[0].lower()\n",
    "            word = word.strip(self.ignoretokens)\n",
    "            wn_tag = lower_cased_tag if lower_cased_tag in ['a', 'r', 'n', 'v'] else None\n",
    "            if not wn_tag:\n",
    "                lemma = word\n",
    "            else:\n",
    "                lemma = self.wnl.lemmatize(word, wn_tag)\n",
    "            if lemma not in list(self.stopwords):\n",
    "                sig_words.append(lemma.lower())\n",
    "        return sig_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_file_name = \"comments\"\n",
    "pickle_df = load_pickle(f\"{pickle_file_name}.p\")\n",
    "pickle_df = pickle_df[pickle_df[\"body\"] != \"[removed]\"]\n",
    "pickle_df = pickle_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Features (bow): 5021\n",
      "Features (bobigrams): 18924\n"
     ]
    }
   ],
   "source": [
    "df = pickle_df.copy()\n",
    "\n",
    "swords = stopwords()\n",
    "itokens = ignoretokens()\n",
    "\n",
    "# bag of words\n",
    "cvb = CountVectorizer(stop_words=swords, tokenizer=LemmaTokenizer(stopwords=list(swords), ignoretokens=itokens))\n",
    "bow_data = cvb.fit_transform(df.get(\"body\"))\n",
    "bow_df = pd.DataFrame(bow_data.toarray(), columns=cvb.get_feature_names())\n",
    "\n",
    "print(f\"Features (bow): {len(cvb.get_feature_names())}\")\n",
    "df_bow = pd.concat([df, bow_df], axis=1, sort=False)\n",
    "\n",
    "\n",
    "# bigram of words\n",
    "cvbi = CountVectorizer(ngram_range=(2,2), stop_words=swords, tokenizer=LemmaTokenizer(stopwords=list(swords), ignoretokens=itokens))\n",
    "bobi_data = cvbi.fit_transform(df.get(\"body\"))\n",
    "bobi_df = pd.DataFrame(bobi_data.toarray(), columns=cvbi.get_feature_names())\n",
    "\n",
    "print(f\"Features (bobigrams): {len(cvbi.get_feature_names())}\")\n",
    "df_bobi = pd.concat([df, bobi_df], axis=1, sort=False)\n",
    "\n",
    "#df_bow\n",
    "#df_bowi\n"
   ]
  }
 ]
}