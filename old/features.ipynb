{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('python_modules')",
   "metadata": {
    "interpreter": {
     "hash": "9634d4b890a726f71d8044046bc71cdc391c406dc663847f104c7db04bcb4cdc"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[{'pos': [':)', ':))', ':D', ':DD', 'xD', 'xDD', ':d=)', '=))', \":')\", \"=')\", ':}', ':}}', ':]', ':]]', '(:', 'C:', ':P', '😂', '❤', '♥', '😍', '😘', '😊', '👌', '💕', '👏', '😁', '☺', '♡', '👍', '🙏', '✌', '😏', '😉', '🙌', '🙈', '💪', '😄', '💃', '💖', '😃', '😱', '🎉', '😜', '🌸', '💜', '💙', '😳', '💗', '☀', '😎', '😢', '💋', '😋', '🙊', '🎶', '💞', '😌', '💯', '💛', '💁', '💚', '😆', '😝', '😅', '👊', '😀', '😚', '😻', '💘', '👋', '✋', '🎊', '🍕', '❄', '😥', '😈', '🔝', '⚽', '👑', '😹', '🍃', '🎁', '🐧', '🎈', '✊', '💤', '💓', '💦', '🙋', '🎄', '🎵', '😛', '😬', '👯', '💎', '🎂', '👫', '🏆', '☝', '😙', '⛄', '👅', '♪', '🍂', '💏', '🌴', '👈', '🌹', '🙆', '🍻', '🌞', '🍁', '⭐', '🎀', '🙉', '🌺', '💅', '🐶', '🌚', '🎤', '👭', '🎧', '👆', '🍸', '🍉', '😇', '🏃', '🍺', '🎸', '🍹', '💫', '📚', '🌷', '💝', '💨', '🏈', '💍', '☔', '👸', '🇪', '🍩', '☁', '🌻', '😵', '↿', '🐯', '👼', '🍔', '😸', '👶', '↾', '💐', '🌊', '🍦', '🍓', '💆', '🍴', '🇸', '😮', '😽', '🌈', '🙀', '🎮', '🍆', '🍰', '🙇', '🍟', '🍌', '💑', '🐣', '🎃', '😟', '🐾', '🎓', '🏊', '🍫', '📷', '👄', '🌼', '🐱', '🇺', '🚬', '📖', '🐒', '🌍', '┊', '🐥', '💄', '💸', '⛔', '🏀', '💉', '💟', '😯', '♦', '🌙', '🐟', '👣', '🗿', '🍝', '🍭', '❌', '🐰', '💊', '🚨', '🍪', '🎆', '🎎', '🇩', '✅', '🍑', '🔊', '🌌', '🍎', '🐻', '💇', '🍊', '🍒', '🐭', '👟', '🌎', '🍍', '🐮', '📲', '🌅', '🇷', '👠', '🌽', '🍬', '😺', '🚀', '¦', '🍧', '🍜', '🐏', '👧', '🏄', '🍋', '🆗', '📺', '🍅', '⛅', '👙', '🏡', '🌾', '✏', '🐬', '🇹', '♣', '🇮', '🐍', '♔', '🍳', '🔵', '🌕', '🐨', '🔐', '💿', '🌳', '👰', '⚓', '🚴', '👗', '➕', '💬', '🔜', '🍨', '🍙', '🍗', '🍲', '😼', '🐙', '👨', '🍚', '🍖', '♨', '▃', '🚘', '👩', '🐠', '🚹', '💵', '✰', '👛', '🌱', '🌏', '🌲', '👴', '🏠', '🍇', '🍘', '🍛', '🐇', '👵', '🌵', '🎇', '🐎', '🐤', '🛀', '🌑', '🚲', '🏁', '🎾', '🐵', '◕', '🗼', '🍵', '🍯', '⇨', '🌓', '🔒', '👳', '♩', '💌', '🌜', '🚿', '🔆', '🌛', '🏩', '🇫', '📢', '🐦', '♻', '🌘', '🍐', '🌔', '╥', '👖', '😗', '🐄', '⬇', '🚼', '🌗', '🌖', '🔅', '👜', '🐌', '💼', '🐹', '🌠', '⚫', '♧', '🎢', '🎷', '🌇', '⏰', '◠', '🎿', '🆔', '🌒', '🐪', '╝', '👔', '🐋', '▽', '🐛', '👕', '💳', '🏧', '💡', '⬅', '🇱', '📹', '👞', '🚑', '🆘', '👚', '🚍', '🚣', '🏉', '🗻', '⛺', '🏂', '👡', '📻', '🌰', '🎒', '⌒', '📴', '🚢', '🔔', '◢', '🏥', '🃏', '💒', '🐐', '🔚', '🔓', '🎽', '📅', '🎺', '✉', '◤', '○', '🍼', '📣', '🐗', '⛳', '┛', '┃', '💺', '☻', '📞', '🌉', '✎', '📃', '💷', '🚄', '▲', '⛵', '⌛', '🚜', '👒', '❕', '🔛', '🇲', '❅', '👝', '✞', '🎋', '👥', '◆', '🔭', '🐜', '♌', '👷', '📄', '🚐', '🌋', '📡', '🚳', '✘', '🅰', '🇼', '┓', '┣', 'Ⓛ', 'Ⓔ', '👤', '🎠', '📗', '🔩', '👢', '📰', 'Ⓜ'], 'neu': [':|', ':/', ':\\\\', '', '☯', '✨', '★', '█', '🔥', '♫', '�', '©', '👀', '🐓', '☕', '💥', '►', '✈', '👉', '☆', '🍀', '🎅', '✔', '⚡', '➡', '🌿', '🌟', '🔮', '❗', '✖', '🔪', '➜', '👻', '💰', '▪', '━', '☷', '🐷', '👽', '🍷', '®', '☑', '│', '💣', '▶', '░', '👾', '📒', '👇', '▓', '⚠', '╯', '✓', '▬', '🚶', '║', '🐸', '✿', '🌀', '🐼', '🎥', '●', '🚗', '📝', '═', '💭', '☞', '🌃', '╭', '✧', '╮', '👹', '📱', '🎼', '─', '╰', '♬', '♚', '🔴', '☼', '❓', '🐴', '💢', '🎬', '🐘', '⠀', '➤', '⬆', '⚪', '🐢', '◉', '🍤', '🐝', '🌝', '❁', '❀', '▀', '▒', '💲', '⛽', '▸', '♛', '🎹', '♕', '🍏', '👦', '🇬', '🇧', '☠', '╠', '🚙', '💻', '▄', '👓', '◄', '🔞', '◀', '🔙', '🐽', '➔', '💶', '╩', '🐑', '🍞', '╚', '🈹', '🐳', '✪', '▐', '♠', '🐚', '👂', '🗽', '🆒', '🐺', '➨', '╬', '🌂', '🚌', '🍡', '❥', '🎡', '🐩', '⌚', '🐖', '🐔', '🐲', '❊', '🚺', '◟', '🍢', '🎨', '⛲', '▁', '🇴', '🚕', '🐈', '⇧', '☎', '🌁', '🏰', '🚵', '🎐', '╗', '╱', '⇩', '🚂', '✦', '⛪', '╔', '🔱', '🆓', '▂', '🚋', '🌆', '🔹', '🐫', '🏪', '۩', '🐂', '✳', '🐀', '╦', '🐕', '✒', '🏢', '🚚', '🐉', '❒', '🐊', '🚖', '▼', '☛', '✩', '🚤', '🎻', '🔷', '🚦', '✯', '╣', '📀', '🚛', '📓', '☉', '💴', '┼', '🐃', '🍄', '📕', '🚓', '↪', '👱', 'Ⓐ', '🏨', '♎', '🔸', '🐆', '♢', '◡', '📵', '🐡', '🏯', '☂', '🎪', '↳', '🔈', '📍', '🚔', '⏩', '۞', '☾', '📥', '🔦', '🚁', '🐁', '♂', '◞', '📯', '◂', '📶', '🚥', '🌄', '🗾', '🔶', '🏤', '🎩', '🐅', '♮', '🔄', '☄', '☨'], 'neg': [':(', ':((', ':(((', ':((((', ':C', ':CC', \":'C\", '=(', '=((', \":'(\", \"='(\", ':[', ':{', '):', '😭', '😩', '😒', '😔', '😡', '😴', '🔫', '😞', '😪', '😫', '💀', '😕', '💔', '😤', '😰', '😑', '😠', '😓', '😣', '😐', '😨', '😖', '👎', '😷', '💩', '🙅', '😿', '😲', '😶', '😧', '🚫', '👐', '👬', '￼', '👿', '✂', '👪', '😦', '🍣', '🙍', '🍱', '💧', '🔋', '😾', '🍥', '⚾', '🍮', '👮', '☹', '┳', '👺', '💂', '🙎', '🔨', '🎭', '┈', '🍠', '□', '🏫', '❔', '▌', '■', '🍈', '➰', '🔌', '┻', '⏳', '🏇', '🚩', '📌', '☐', '┐', '☮', '🔧', '🅾']}, {'date rape', 'pust', 'nudity', 'punanny', 'panty', 'two fingers with tongue', 'fubar', 'chinc', 'testes', 'mothafucks', 'chesticle', 'cockhead', 'tinkle', 'boobs', 'raper', 'slutbag', 'fucker', 'shite', 'phukking', 'pimp', 'tittyfuck', 'camel toe', 'wank', 'cum', 'zubb', 'shag', 'grope', 'dick shy', 'c0cksucker', 'girl on top', 'seks', 'cockknocker', 'snatch', 'bigtits', 'bawdy', 'bod', 'thug', 'bastard', 'b17ch', 'whore', 'venus mound', 'looney', 'viagra', 'fisted', 'peepee', 'fuckme', 'asshopper', 'bollocks', 'boong', 'mothafuckaz', 'butthole', 'trashy', 'shaggin', 'tittie5', 'fuckboy', 'buttcheeks', 'tawdry', 'brown showers', 'dong', 'sodomy', 'pube', 'd1ld0', 'fleshflute', 'gash', 'dipship', 'how to kill', 'kill', 'blonde on blonde action', 'crappy', 'tribadism', 'guro', 'violet wand', 'humping', 'pecker', 'fanyy', 'nambla', 'bitchtits', 'pthc', 'female squirting', 'testee', 'jizz', 'middle finger', 'punkass', 'feltcher', 'fingering', 'vulva', 'a55hole', 'penial', 'jackasses', 'urophilia', 'shitbreath', 'v14gra', 'coonnass', 'wrapping men', 'kunt', 'teste', 'nigga', 'asswad', 'penis', 'faggitt', 'punky', 'arse', 'cooter', 'ass-hat', 'pedobear', 'corksucker', 'cyberfucking', 'aeolus', 'knob end', 'tit', '5hit', 'whoar', 'scroat', 'dumshit', 'shiz', 'shitbrains', 'doggy-style', 'climax', 'shited', 'div', 'fag', 'nigger', 'mr hands', 'menage a trois', 'fistfucked', 'fvck', 'mothafucka', 'godamn', 'cockholster', 'jizm', 'cockbite', 'strip', 'coons', 'nymphomania', 'pussys', 'jerk', 'polack', 'rapist', 'sissy', 'teez', 'kum', 'gai', 'cunilingus', 'fuckoff', 'paki', 'pimpis', 'facial', 'piss pig', 'gender bender', 'kinbaku', 'dickbeaters', 'puss', 'dickzipper', 'golliwog', 'bullet vibe', 'fecker', 'sodomize', 'motherfucka', 'tw4t', 'weiner', 'asslicker', 'taste my', 'blow mud', 'fingerfucked', 'arrse', 'menstruation', 'shitings', 'he-she', 'blow me', 'eunuch', 'fisting', 'bloodclaat', 'dicksucker', 'whorebag', 'blowjobs', 'spunk', 'masterbation', 'cacafuego', 'suicide girls', 'cumdumpster', 'cumjockey', 'phonesex', 'raging boner', 'faggs', 'black cock', 'fucks', 'fack', 'whorealicious', 'kummer', 'cumming', 'whorehouse', 'cockmunch', 'sexy', 'dickwod', 'booooobs', 'condom', 'fuckbutt', 'gangbanged', 'wog', 'nut butter', 'flaps', 'muff', 'cocksucked', 'jagoff', 'ball licking', 'sleazy', 'knobjokey', 'g-spot', 'slanteye', 'pedophile', 'mothafuck', 'brunette action', 'unwed', 'azz', 'nobhead', 'double dong', 'm0fo', 'fucka', 'camslut', 'herpy', 'mams', 'jerked', 'fuckheads', 'cop some wood', 'gonads', 'dolcett', 'chodes', 'lezzie', 'lesbo', 'shittiest', 'donkeypunch', 'phuck', 'dickish', 'yellow showers', 'fingerfuck', 'dog style', 'dumass', 'ejaculated', 'psycho', 'shiznit', 'asswhole', 'assbangs', 'breeder', 'fucktard', 'tubgirl', 'knobed', 'dumbcunt', 'kock', 'cocksuck', 'how to murder', 'juggs', 'muff diver', 'pissed', 'dickflipper', 'gay sex', 'fuckbag', 'penetrate', 'nobjokey', 'cumdump', 'fistfucker', 'creampie', 'penisbanger', 's.o.b.', 'fuckbutter', 'blue waffle', 'bangbros', 'jerkoff', 'gook', 'asscracker', 'bbw', 'vorarephilia', 'mothafuckings', 'fucking', 'nobjocky', 'cum chugger', 'cumshots', 'p.u.s.s.y.', 'faggit', 'dickbag', 'daterape', 'booty call', 'fingerfucking', 'crotte', 'twinkie', 'corp whore', 'pissin', 'make me come', 'fuckhead', 'cocaine', 'assfucker', 'dickmonger', 'gfy', 'rusty trombone', 'mick', 'shrimping', 'doublelift', 'cyberfucked', 'kumming', 'swinger', 'pornography', 'beastial', 'cuntlick', 'dicks', 'porn', 'doofus', 'coon', 'willy', 'shitheads', 'blumpkin', 'semen', 'douche-fag', 'bodily', 'sucked', 'f.u.c.k', 'piss', 'sand nigger', 'chi-chi man', 'gtfo', 'rosy palm and her 5 sisters', 'ass-jabber', 'xx', 'cock sucker', 'bampot', 'titties', 'hoer', 'clover clamps', 'nude', 'uzi', 'arsehole', 'domination', 'moron', 'queero', 'fannybandit', 'fudge-packer', 'sniper', 'clusterfuck', 'c.o.c.k.', 'scat', 'toke', 's_h_i_t', 'jizzed', 'hell', 'shitty', 'scissoring', 'motherfuckin', 'floozy', 'beaner', 'hootch', 'lovemaking', 'masterbat*', 'shaved pussy', 'nob', 'bastardo', 'cuntbag', 'throating', 'motherfucker', 'scrot', 'cockwaffle', 'gassy ass', 'flog the log', 'crackwhore', 'tush', 'gaysex', 'fcuk', 'mutha', 'bung hole', 'skag', 'santorum', 'sucking', 'gigolo', 'ruski', 'dinks', 'gokkun', 'wad', 'jackass', 'cuntface', 'penile', 'cok', 'goddamned', 'shitt', 'douche', 'phuks', 'wet dream', 'shithouse', 'asscock', 'cocklump', 'handjob', 'mothafuckers', 'veqtable', 'nig nog', 'tramp', 'meth', 'h0mo', 'jungle bunny', 'pubic', 'dookie', 'nig-nog', 'homodumbshit', 'ma5terbate', 'pisser', 'bullshits', 'cock-sucker', 'undies', 'vajayjay', 'ponyplay', 'assbanger', 'omg', 'cumtart', 'cum freak', 'snuff', 'motherfuckka', 'bollok', 'holy shit', 'feltch', 'douchewaffle', 'titt', 'rectus', 'cumshot', 'bloody', 'fagtard', 'assfuck', 'gay', 'big black', 'pisses', 'window licker', 'dammit', 'frigg', 'cumstain', 'asssucker', 'fags', 'renob', 'assbanged', 'bukkake', 'fartknocker', 'skeet', 'hebe', 'foad', 'rapey', 'dimwit', 'virgin', 'fistfuckings', 'beaver lips', 'choade', 'mothafuckin', 'orgy', 'twink', 'cl1t', 'fagging', 'thrust', 'shitspitter', 'homoerotic', 'gaywad', 'extasy', 'bunny fucker', 'fellatio', 'tub girl', 'fudgepacker', 'dickwhipper', 'potty', 'cocks', 'fuckface', 'slut bucket', 'one guy one jar', 'prick', 'goodpoop', 'pissing', 'choc ice', 'stoned', 'girls gone wild', 'kooches', 'spooge', 'camwhore', 'orgasm', 'titi', 'tittywank', 'douch3', 'boozy', 'cunthole', 'topless', 'pigfucker', 'lmfao', 'cocksukka', 'omorashi', 'humped', 'suck', 'murder', 'godamnit', 'bitchin', 'duche', 'poof', 'vibrator', 'wiseasses', 'bull shit', 'vagina', 'knobead', 'shitbag', 'cyberfucker', 'mofo', '2 girls 1 cup', 'white power', 'menstruate', 'bosom', 'dirty sanchez', 'lezza/lesbo', 'queef', 'fagot', 'dommes', 'tea bagging', 'hoar', 'buttfuck', 'cockmuncher', 'clitoris', 'f4nny', 'shagging', 'male squirting', 'masochist', 'screwed', 'pole smoker', 'hymen', 'queerhole', 'coffin dodger', 'pikey', 'dicksipper', 'son-of-a-bitch', 'cockknoker', 'booby', 'coprophilia', 'tard', 'fooker', 'hot chick', 'nimphomania', 'felch', 'beaners', 'mothafucker', 'spiks', 'fuck off', 'prod', 'dickfuck', 'pussy fart', 'two girls one cup', 'turd', 'wh0re', 'bollox', 'tampon', 'vag', 'dink', 'naked', 'douchebags', 'jap', 'moo moo foo foo', 'sandbar', 'gang bang', 'beastiality', 'stfu', 'pussies', 'shagger', 'bitch tit', 'big tits', 'nutsack', 'pussylicking', 'loin', 'twunt', 'dogging', 'diligaf', 'bitchass', 'dumbshit', 'masterbat3', 'fagbag', 'ass fuck', 'sandler', 'wetback', 'piss-off', 'jack-off', 'b00bs', 'hardcoresex', 'snowballing', 'smutty', 'gooch', 'penisfucker', 'herpes', 'escort', 'lesbos', 'taig', 'fuck puppet', 'azazel', 'god-damned', 'piss off', 'arian', 'brotherfucker', 's-h-i-t', 'zoophilia', 'assmuncher', 'poonany', 'shithead', 'areola', 'corpulent', 't1tties', 'dicksucking', 'cockmaster', 'cunt-struck', 'bitches', 'ahole', 'fuck yo mama', 'porno', 'nawashi', 'cum dumpster', 'cocksucking', 'hot carl', 'carpet muncher', 'cock pocket', 'bitch', 'dumb ass', 'quim', 'wh0reface', 'cockeye', 'mo-fo', 'cocksuckers', 'asswipe', 'lmao', 'bosomy', 'buttfucker', 'sod off', 'strip club', 'deep throat', 'fanny', 'cyberfuckers', 'hooch', 'toots', 'shirt lifter', 'r-tard', 'jiz', 'mothafucking', 'whored', 'napalm', 'assbag', 'scrotum', 'hooter', 'beef curtains', 'unclefucker', 'weed', 'bookie', 'he11', 'assho1e', 'cuntlicking', 'heroin', 'knobhead', 'dirsa', 'coital', 'caca', 'kwif', 'bestial', 'panties', 'urethra play', 'scum', 'jack off', 'boozer', 'twatlips', 'vixen', 'erect', 'fistfuckers', 'scantily', 'valium', 'punany', 't1t', 'nipple', 'doggie-style', 'femdom', 'splooge', 'iap', 'pedophilia', 'homo', 'bust a load', 'crikey', 'prig', 'fannyflaps', 'crack', 'jerkass', 'bugger', 'labia', 'ejakulate', 'fucknut', 'fingerfuckers', 'booty', 'clitface', 'motherfuckers', 'molest', 'stupid', 'chincs', 'pedo', 'penispuffer', 'blonde action', 'eat my ass', 'doggystyle', 'birdlock', 'butt plug', 'assmaster', 'milf', 'hoare', 'poopchute', 'beaver', 'bollock', 'tranny', 'bareback', 'dick-ish', 'paedophile', 'goddam', 'nazism', 'cumbubble', 'cleveland steamer', 'fuckbrain', 'butt-pirate', 'rump', 'ass-fucker', 'rimjob', 'booger', 'seamen', 'seduce', 'nigg4h', 'booobs', 'shits', 'boners', 'group sex', 'assclown', 'dickweed', 'sultry women', 'hookah', 'kawk', 'beotch', 'ball kicking', 'whoring', 'dawgie-style', 'gey', 'suckass', 'punani', 'skank', 'ball sack', 'kondums', 'freex', 'ballbag', 'titty', 'nut sack', 'cretin', 'munging', 'wop', 'cockmongler', 'schizo', 'shitter', 'swastika', 'smegma', 'shit ass', 'm-fucking', 'kootch', 'darn', 'teat', 'dlck', 'towelhead', 'vomit', 'shamedame', 'slag', 'fuckin', 'mothafucked', 'cocksmoke', 'need the dick', 'rimming', 'hircismus', 'splooge moose', 'cumslut', 'w00se', 'sucks', 'orgies', 'penetration', 'nigg3r', 'doochbag', 'kondum', 'feck', 'assjacker', 'beatch', 'pisspig', 'dvda', 'ghay', 'dumbasses', 'quicky', 'cockburger', 'cockmonkey', 'cuntrag', 'testis', 'fuckmeat', 'leather restraint', 'dipshit', 'asslick', 'pegging', 'dickhole', 'dry hump', 'c.0.c.k', 'c0ck', 'fucktart', 'spook', 'panooch', 'midget', 'tainted love', 'boned', 'doggy style', 'shiteater', 'cum guzzler', 'coochie', 'dike', 'prince albert piercing', 'spade', 'scrog', 'bestiality', 'spac', 'reich', 'dillweed', 'ovums', 'junglebunny', 'bootee', 'pron', 'god-dam', 'urine', 'ass', 'sexual', 'hiv', 'boob', 'a54', 'how to murdep', 'cunthunter', 'hump', 'masturbating', 'whoralicious', 'sh1t', 'gooks', 'lusty', 'fukwhit', 'assfaces', 'goatcx', 'rimjaw', 'booze', 'bulldyke', 'clit', 'slutkiss', 'homoey', 'fukkin', 'assbite', 'vjayjay', 'smut', 'dummy', 'gays', 'raghead', 'strapon', 'jerk-off', 'clunge', 'motherfucks', 'dickwad', 'cornhole', 'shota', 'sadist', 'm0f0', 'figging', 'ritard', 'loins', 'junky', 'beeyotch', 'fisty', 'orgasims', 'camgirl', 'fenian', 'fuck-ass', 'cox', 'queaf', 'fuck you', 'screw', 'd1ck', 'heshe', 'gayass', '5h1t', 'sh!t', 'clitorus', 'motherfuckings', 'poontang', 'doggie style', 'pansy', 'gayfuckist', 'pussi', 'fuckwitt', 'jail bait', 'gangbangs', 'areole', 'horny', 'cracker', 'p0rn', 'shemale', 'fuckedup', 'massa', 'dick hole', 'sex', 'cockmongruel', 'mafugly', 'assbandit', 'breasts', 'kike', 'nigaboo', 'porchmonkey', 'slut', 'girl on', 'gayfuck', 'boner', 'sleaze', 'pee', 'piece of shit', 'faig', 'cunts', 'masterbating', 'niglet', 'incest', 'retarded', 'dumbass', 'pissflaps', 'foah', 'cocknose', 'd0ng', 'shitblimp', 'master-bate', 'weewee', 'shibari', 'bastards', 'spic', 'twat', 'tittyfucker', 'asshat', 'assh0le', 'old bag', 'kyke', 'bondage', 'fuckhole', 'ham flap', 'lardass', 'cocksucks', 'pubes', 'wtf', 'cus', 'scrote', 'jailbait', 'hand job', 'bullturds', 'dumbfuck', 'phuked', 'shaved beaver', 'dickface', 'fucknugget', 'h0m0', 't1tt1e5', 'cyberfuck', 'god damn', 'reetard', 'axwound', 'fcuking', 'fagged', 'ma5terb8', 'yobbo', 'knob', 'dick', 'thundercunt', 'dicktickler', 'twathead', 'bootie', 'prude', 'clits', 'fistfucking', 'baby batter', 'jism', 'muther', 'fuckwad', 'hooker', 'minger', 'double penetration', 'douchebag', 'negro', 'deepthroat', 'whoreface', 'pcp', 'yid', 'opiate', 'mothafuckas', 'coccydynia', 'tongue in a', 'ballsack', 'bdsm', 'phone sex', 'jackhole', 'a$$', 'd1ldo', 'fuker', 'child-fucker', 'wiseass', 'big breasts', 'phukked', 'undressing', 'cuntslut', 'smartass', 'missionary position', 'butt fuck', 'cocksmith', 'pinko', 'kraut', 'sumofabiatch', 'yeasty', 'scag', 'queers', 'willies', 'asswipes', 'frigga', 'lemon party', 'cahone', 'felcher', 'dingleberry', 's.h.i.t.', 'shitstain', 'herp', 'cocain', 'scrud', 'chocolate rosebuds', 'butt', 'a2m', 'm45terbate', 'mof0', 'git', 'dopey', 'bung', 'fuck hole', 'leper', 'baby juice', 'pissers', 'raped', 'ugly', 'lesbian', 'kunja', 'clitty litter', 'tight white', 'bullshitted', 'damnit', 'sandnigger', 'cocksniffer', 'pawn', 'munter', 'cocksuka', 'ejaculation', 'assnigger', 'fuks', 'japs', 'circlejerk', 'a55', 'gippo', 'dog-fucker', 'knobjocky', 'bum boy', 'dickweasel', 'erotic', 'boiolas', 'blow your load', 'cut rope', 'sh!+', 'autoerotic', 'babeland', 'chode', 'dickjuice', 'fuckings', 'bloody hell', 'genitals', 'essohbee', 'cipa', 'kinky', 'mound of venus', 'shitcunt', 'fuckwit', 'womb', 'ar5e', 'fuckass', 'dickslap', 'n1gga', 'dickmilk', 'asspirate', 'bitched', 'lech', 'shi+', 'boooobs', 'doggiestyle', 'lolita', 'yaoi', 'pubis', 'spik', 'assbang', 'screwing', 'crabs', 'extacy', 'maxi', 'masterb8', 'bong', 'threesome', 'dick head', 'len', 'orgasim', 'fagg', 'cunt', 'hardcore', 'ejaculatings', 'ho', 'anus', 'nimrod', 'schlong', 'buttplug', 's&m', 'fuckup', 'pusse', 'tits', 'c.u.n.t', 'frenchify', 'poon', 'ass hole', 'spread legs', 'transsexual', 'niggah', 'cockblock', 'anal leakage', 'ejaculating', 'deggo', 'bimbo', 'anal impaler', 'homey', 'fannyfucker', 'twatwaffle', 'hard core', 'retard', 'cockface', 'futanari', 'slutdumper', 'ovary', 'raunch', 'seaman', 'sambo', 'nutter', 'x-rated', 'muthafecker', 'rectum', 'anilingus', 'knobend', 'minge', 'guido', 'fucktwat', 'busty', 'assface', 'beardedclam', 'sexo', 'commie', 'boobies', 'kooch', 'iberian slap', 'bumblefuck', 'kums', 'cunillingus', 'kinkster', 'weenie', 'assshit', 'dickdipper', 'gangbang', 'teets', 'b1tch', 'eat a dick', 'strap on', 'pleasure chest', 'dagos', 'feist', 'pms', 'strappado', 'dirty pillows', 'asshead', 'orgasms', 'chick with a dick', 'leather straight jacket', 'fux0r', 'lez', 'nazi', 'hun', 'huge fat', 'shit', 'goddamnit', 'style doggy', 'fatass', 'assgoblin', 'slope', 'boink', 'pornos', 'phuq', 'goo girl', 'phuk', 'wanky', 'cocksucker', 'assmucus', 'xxx', 'cocksmoker', 'jerk0ff', 'fingerfucker', 'fuc', 'dickhead', 'jiggerboo', 'shitdick', 'bum', 'glans', 'assmunch', 'hotsex', 'pillowbiter', 'fukker', 'bender', 'wrinkled starfish', 'ass-pirate', 'shithole', 'kunilingus', 'auto erotic', 'choad', 'damned', 'assfukka', 'cunt hair', 'shiting', 'cameltoe', 'fagfucker', 'motherfucked', 'cumguzzler', 'fingerfucks', 'beaver cleaver', 'a$$hole', 'blow job', 'gae', 'cock snot', 'apeshit', 'organ', 'f_u_c_k', 'c-0-c-k', 'fxck', 'fux', 'gspot', 'spick', 'shitters', 'polesmoker', 'cripple', 'whiz', 'coprolagnia', 'nipples', 'stroke', 'teabagging', 'hoe', 'cocknugget', 's0b', 'muthafuckker', 'gang-bang', 'darkie', 'hom0', 'heeb', '4r5e', 'godsdamn', 'l3itch', 'ovum', 'pussy', 'dingle', 'vulgar', 'wanker', 'son of a whore', 'fuckwhit', 'taff', 'hard on', 'va-j-j', 'punta', 'hentai', 'buttfucka', 'fuck trophy', 'hooters', 'shitfull', 'alaskan pipeline', 'tities', 'lameass', 'big knockers', 'vodka', 'phallic', 'faggot', 'jackoff', 'goddamn', 'l3i+ch', 'fice', 'tosser', 'fucktoy', 'barely legal', 'reverse cowgirl', 'asses', 'eat hair pie', 'd0uch3', 'dingleberries', 'cawk', 'fagots', 'hoor', 'neonazi', 'queer', 'trumped', 'a_s_s', 'dp action', 'dickfucker', 'foot fetish', 'goldenshower', 'two fingers', 'playboy', 'booooooobs', 'c-u-n-t', 'donkey punch', 'tit wank', 'ganja', 'poop', 'inbred', 'ball sucking', 'ejaculate', 'shitcanned', 'niggas', 'hussy', 'zibbi', 'yiffy', 'erection', 'rubbish', 'hemp', 'wankjob', 'bint', 'wang', 'crap', 'muff puff', 'racy', 'nsfw images', 'faggotcock', 'niggers', 'lube', 'gaytard', 'gaybob', 'titwank', 'bunghole', 'doosh', 'anal', 'mutherfucker', 'assshole', 'b!tch', 'rtard', 'dildos', 'carpetmuncher', 'chinky', 'shitfaced', 'asshole', 'goatse', 'prostitute', 'fuk', 'nympho', 'cunnie', 'cuntlicker', 'niggle', 'dirty', 'fist fuck', 'puto', 's hit', 'soused', 'beef curtain', 'cummer', 'nob jokey', 'fukkers', 'jigaboo', 'golden shower', 'jaggi', 'giant cock', 'porch monkey', 'shitface', 'masturbate', 'cock', 'frotting', 'damn', 'jiggaboo', 'fuckingshitmotherfucker', 'fuck-bitch', 'reefer', 'fucked', 'cunnilingus', 'twatty', 'ninnyhammer', 'phuking', 'numbnuts', 'injun', 'cyberfuc', 'pedophiliac', 'sperm', 'buttmunch', 'n1gger', 'tart', 'ejaculates', 'goregasm', 'lust', 'fuck buttons', 'fudge packer', 'souse', 'pissoff', 'flamer', 'bescumber', 'dickripper', 'cervix', 'foreskin', 'dyke', 'flange', 'rape', 'lesbians', 'dick-sneeze', 'mcfagget', 'junkie', 'urinal', 'slave', 'barf', 'bellend', 'wench', 'ginger', 'tied up', 'alabama hot pocket', 'hitler', 'analprobe', 'stiffy', 'ball gag', 'footjob', 'horniest', 'dendrophilia', 'cuntass', 'son of a bitch', 'testicle', 'pantie', 'acrotomophilia', 'coochy', 'wedgie', 'shit fucker', 'titfuck', 'hobag', 'honkey', 'knobbing', 'drunk', 'dildo', 'diddle', 'muffdiving', 'poopuncher', 'intercourse', 'skullfuck', 'bitchers', 'jelly donut', 'fuckersucker', \"bang (one's) box\", 'tittiefucker', 'donkeyribber', 'smeg', 'assmonkey', 'menses', 'masturbation', 'wigger', 'fecal', 'cnut', 'erotism', 'pussy palace', 'wazoo', 'clitty', 'balls', 'chink', 'motherfucking', 'ghey', 'barenaked', 'gringo', 'shitted', 'mother fucker', 'fook', 'poop chute', 'fucknutt', 'uterus', 'fucktards', 'masterbate', 'taking the piss', 'buceta', 'clit licker', 'moolie', 'kikes', 'nappy', 'blowjob', 'octopussy', 'opium', 'cockfucker', 'pricks', 'god', 'smartasses', 'mong', 'coksucka', 'jerk off', 'sluts', 'fukwit', 'cockjockey', 'c-o-c-k', 'one cup two girls', 'rosy palm', 'fistfucks', 'motherfuck', 'cuntsicle', 'niggaz', 'ninny', 'batty boy', 'shitbagger', 'pollock', '2g1c', 'upskirt', 'dykes', 'jock', 'raping', 'klan', 'rum', 'perversion', 'ball gravy', 'sausage queen', 'pussypounder', 'fuck-tard', 'twats', 'sadism', 'son of a motherless goat', 'biatch', 'bi+ch', 'cummin', 'honky', 'fistfuck', 'sodom', 'rectal', 'felching', 'orgasmic', 'twunter', 'orally', 'peckerhead', 'dago', 'fuckers', 'bimbos', 'bitcher', 'xrated', 'cockshit', 'foobar', 'sanger', 'fuckstick', 'muffdiver', 'douchey', 'fart', 'masterbations', 'clitfuck', 'bitching', 'bastinado', 'ecchi', 'dominatrix', 'fingerbang', 'tushy', 'poonani', 'bullshit', 'fondle', 'cyalis', 'lusting', 'fuck', 'microphallus', 'shittings', 'whores', 'buttmuch', 'fuq', 'bumclat', 'buncombe', 'chota bags', 'shitfuck', 'f-u-c-k', 'gonad', 'shitass', 'assholes', 'v1gra', 'cums', 'gaydo', 'cunny', 'aryan', 'goddammit', 'bummer', 'rumprammer', 'd0uche', 'shitting', 'shittier', 's-o-b', 'doggin', 'voyeur', 'cockass', 'faggots', 'dickheads', 'whorehopper', 'bitchy', 'faigt', 'fellate', 'revue', 'nonce', 'prickteaser', 'pot', 'fcuker', 'queerbait', 'kafir', 'shitey', 'weirdo', 'cokmuncher', 'hore', 'testical', 'pissed off', 'gaylord', 's-h-1-t'}, {'indeed', 'actually', 'etc', 'recently', 'id', 'she', 'him', 'everywhere', 'wont', 'have', 'nd', 'anything', 'happens', 'none', 'www', 'come', 'ie', 'alone', 'usefully', 'value', 'their', 'therefore', 'mr', 'truly', 'some', 'eighty', 'apparently', 'himself', 'right', 'o', 'overall', 'latter', 'thereto', 'away', 'past', 'also', 'thereby', 'because', 'seemed', 'her', 'biol', 'eight', 'accordingly', 'e', 'mg', 'noted', 'owing', 'hither', 'ed', 'comes', 'of', 'therere', 'old', 'plus', 'specifically', 'ml', 'werent', 'use', 'mean', 'try', 'previously', 'ok', 'hed', 'approximately', 'besides', 'obviously', 'immediately', 'pp', 'else', 'knows', 'following', 'normally', 'from', 'itd', 'namely', 'aside', 'enough', 'please', 'usually', 'having', 'placed', 'somebody', 'line', 'too', 'whereafter', 'last', 'anyway', 'looking', 'necessary', 'gotten', 'specified', 'after', 'nobody', 'part', 'this', 'begins', 'makes', 'shes', 'give', 'useful', 'com', 'whereby', 'new', 'were', 'let', 'meanwhile', 'they', 'asking', 'every', 'hereafter', 'keeps', 'during', 'whomever', 'auth', 'which', 'g', 'somewhere', 'known', 'many', 'using', 'sorry', 'either', 'obtained', 'by', 'something', 'quite', 'noone', 'little', 'seven', 'nevertheless', 'where', 'and', 'found', 'various', 'substantially', 'usefulness', 'but', 'regarding', 'theres', 'never', 'ups', 'forth', 'research', 'means', 'anywhere', 'particular', 'wish', 'kg', 'there', 'two', 'made', 'successfully', 'except', 'nearly', 'one', 'qv', 'affecting', 'back', 'b', 'lately', 'obtain', 'otherwise', 'section', 'over', 'tip', 'taken', 'relatively', 'yours', 'oh', 'become', 'welcome', 'gave', 'go', 'added', 'amongst', 'thereof', \"we've\", 'non', 'seeming', 'them', 'sure', 'lest', 'least', 'regardless', 'able', 'got', 'saw', 'further', 'selves', 'formerly', 'importance', 'somethan', \"'ll\", 'whos', 'might', 'shows', 'around', 'ord', 'thered', 'gone', 'abst', 'his', 'particularly', 'the', 'uses', 'somewhat', 'he', 'wouldnt', 'whatever', 'meantime', 'about', 'within', 'here', 'important', \"i'll\", 'thereafter', 'upon', \"she'll\", 'such', 'came', 'wherever', 'seen', 'sometimes', 'thou', 'that', 'hers', 'ltd', 'nonetheless', 'yes', 'whereupon', 'near', 'all', 'different', 'self', 'w', 'look', 'ex', 'affects', 'tends', \"you'll\", 'stop', 'inc', 'need', 'respectively', 'results', 'wed', 'nothing', 'i', 'thousand', 'since', 'trying', 'lets', 'therein', 'former', 'few', 'once', 'date', 'herein', 'went', \"what'll\", 'someone', 'whoever', \"didn't\", 'ourselves', 'certainly', 'shall', 'must', 'awfully', 'most', 'each', 'my', 'nowhere', 'says', 'h', 'x', \"there've\", \"can't\", 'on', 'affected', 'should', \"that'll\", 'beforehand', 're', 'what', 'unlike', 'we', 'in', 'rather', 'mainly', 'miss', 'predominantly', 'for', 'mug', 'off', 'til', 'd', 'or', 'say', 'certain', 'immediate', 'above', 'even', 'accordance', 'ah', 'liked', 'present', 'everything', 'c', 'down', \"we'll\", 'gets', 'begin', 'regards', 'less', 'hundred', 'anymore', 't', 'possibly', 'follows', 'becomes', 'twice', 'want', 'l', 'thru', 'been', 'information', 'ca', 'y', 'moreover', 'sometime', \"they've\", 'inward', 'thanx', 'words', 'believe', \"that've\", 'ninety', 'very', 'hereupon', 'primarily', 'end', 'whereas', 'mrs', 'hid', 'whod', 'although', 'v', 'causes', 'howbeit', 'unto', 'always', 'et', 'any', 'hereby', 'related', 'almost', 'far', 'not', 'et-al', 'beginning', 'several', 'couldnt', 'really', \"i've\", 'anyways', 'four', 'unfortunately', 'already', 'brief', 'others', 'refs', 'poorly', 'nos', 'more', 'widely', 'next', 'keep', 'afterwards', 'again', 'than', 'thoughh', 'beyond', 'took', 'whenever', 'five', 'looks', 'until', 'became', 'beside', 'arise', 'nine', 'could', 'likely', 'home', 'theirs', 'downwards', 'onto', 'viz', 'showns', 'zero', 'who', 'between', 'myself', 'without', 'followed', 'know', 'among', 'j', 'ts', 'ask', 'aren', 'getting', 'significant', 'needs', 'into', 'ran', 'vol', 'often', 'done', 'wherein', 'pages', 'just', 'similarly', 'however', 'q', 'across', 'fifth', 'index', 'theyre', 'doing', 'p', 'n', 'hi', 'm', 'tell', 'thats', 'million', 'invention', 'theyd', 'necessarily', 'wasnt', 'co', 'cause', 'kept', 'did', 'towards', 'km', 'k', 'how', 'name', 'up', 'an', 'contains', 'yet', 'through', 'provides', 'instead', 'youre', 'somehow', 'yourselves', 'beginnings', 'had', 'wants', 'suggest', 'before', 'am', 'potentially', 'rd', 'as', 'ought', 'soon', 'sup', 'announce', 'though', 'other', 'it', 'heres', 'possible', 'thanks', 'briefly', 'strongly', 'against', 'whose', 'together', 'our', 'seem', 'saying', \"there'll\", 'cannot', 'omitted', 'ff', \"shouldn't\", 'sec', 'has', 'do', 'me', 'hes', 'per', 'shed', 'quickly', 'available', 'much', 'similar', 'your', 'elsewhere', \"they'll\", 'due', 'gives', 'why', 'fix', 'whether', 'slightly', 'out', 'ever', 'taking', \"who'll\", 'with', 'everyone', 'anyone', 'get', 'un', 'according', 'specifying', 'giving', 'resulting', 'vs', 'sufficiently', 'way', 'page', 'significantly', 'anybody', 'vols', 'another', 'thereupon', 'recent', 'seems', 'eg', 'used', 'may', 'maybe', 'below', 'behind', 'showed', \"'ve\", 'resulted', 'shown', 'said', 'throughout', 'so', 'everybody', 'think', 'outside', 'under', 'us', 'itself', 'edu', 'would', 'ones', 'r', 'does', 'six', 'whim', 'wheres', \"isn't\", 'specify', 'neither', 'then', 'whither', 'na', 'probably', 'only', \"hasn't\", 'at', 'act', 'furthermore', 'arent', 'themselves', 'is', 'unless', 'via', 'make', 'still', 'containing', 'merely', 'f', 'que', 'own', 'those', 'take', 'nor', 'whence', 'whole', 'yourself', 'mostly', 'can', \"don't\", 'thus', 'whom', 'both', 'okay', 'readily', 'toward', 'u', 'ours', 'herself', 'adj', 'later', 'was', 'thence', 'hence', \"it'll\", 'im', 'hardly', 'nay', 'seeing', 'sent', 'given', 'th', 'first', 'largely', 'youd', 'its', 'see', 'latterly', 'run', 'show', \"haven't\", 'whats', 'proud', 'tries', 'anyhow', 'while', \"doesn't\", 'perhaps', 'unlikely', 'contain', 'ending', 'these', 'you', 'world', 'when', 's', 'are', 'effect', 'ref', 'being', 'especially', 'like', 'thank', 'promptly', 'put', 'no', 'z', 'if', 'along', 'same', 'now', 'tried', 'willing', 'sub', 'a', 'be', 'becoming', \"you've\", 'to', 'throug', 'goes'}]\n"
     ]
    }
   ],
   "source": [
    "# https://blog.cambridgespark.com/50-free-machine-learning-datasets-sentiment-analysis-b9388f79c124\n",
    "\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/feature_extraction.html#stop-words\n",
    "# https://www.aclweb.org/anthology/W18-2502/\n",
    "def stopwords():\n",
    "    import os\n",
    "    root_path = f\"/home/halpdesk/CODE/reddit-parser\"\n",
    "    filename = f\"{root_path}/datasets/ranksnl_large.csv\"\n",
    "    stop_words = []\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            for word in line.split(\",\"):\n",
    "                stop_words.append(word.lower().strip())\n",
    "    return set(stop_words)\n",
    "\n",
    "# Not the same ignore tokens as in nb-features - because we want to check for smilies here\n",
    "def ignoretokens():\n",
    "    common_fractals = [\"1/2\", \"1/3\", \"1/4\"]\n",
    "    low_numbers = [str(int) for int in range(0,10)]\n",
    "    mid_numbers = [str(int) for int in range(10,100,10)]\n",
    "    high_numbers = [str(int) for int in range(100,100100,100)]\n",
    "    tokens = [',', '.', '\"', '``', \"''\", '`', '*', '_', \"&\", \"$\", \"!\", \"#\", \"%\", \"'\", \"”\", \"“\", \"’\", \"‘\", \"―\", \"—\", \"~\", \"–\", \"/\", \"  \", \"   \", \"    \", \"\\n\", \"\\t\", \"\\r\\n\", \"\\r\", \"\t\", \"?\"] + low_numbers + mid_numbers + high_numbers\n",
    "    return tokens\n",
    "\n",
    "def badwords():\n",
    "    import os\n",
    "    root_path = f\"/home/halpdesk/CODE/reddit-parser\"\n",
    "    filename = f\"{root_path}/datasets/fb-bad-words.csv\"\n",
    "    bad_words = []\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            for word in line.split(\",\"):\n",
    "                bad_words.append(word.lower().strip())\n",
    "    return set(bad_words)\n",
    "\n",
    "\n",
    "def commonwords():\n",
    "    import os\n",
    "    root_path = f\"/home/halpdesk/CODE/reddit-parser\"\n",
    "    filename = f\"{root_path}/datasets/common-words.csv\"\n",
    "    common_words = []\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            for word in line.split(\",\"):\n",
    "                common_words.append(word.lower().strip())\n",
    "    return set(common_words)\n",
    "\n",
    "def names():\n",
    "    import os\n",
    "    root_path = f\"/home/halpdesk/CODE/reddit-parser\"\n",
    "    filename = f\"{root_path}/datasets/first_names_all.csv\"\n",
    "    first_names = []\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            for word in line.split(\",\"):\n",
    "                first_names.append(word.strip())\n",
    "    filename = f\"{root_path}/datasets/last_names_all.csv\"\n",
    "    last_names = []\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            for word in line.split(\",\"):\n",
    "                last_names.append(word.strip())\n",
    "        \n",
    "    return set(first_names + last_names)\n",
    "\n",
    "# From https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0144296:\n",
    "# http://kt.ijs.si/data/Emoji_sentiment_ranking/\n",
    "# Other: https://research.utwente.nl/files/5482763/sac13-senticon.pdf\n",
    "# http://emojitracker.com/\n",
    "def emoticons():\n",
    "    import os\n",
    "    root_path = f\"/home/halpdesk/CODE/reddit-parser\"\n",
    "    filename = f\"{root_path}/datasets/emoticons.csv\"\n",
    "    emoticons = {\n",
    "        \"pos\": [\":)\", \":))\", \":D\", \":DD\", \"xD\", \"xDD\", \":d\" \"=)\", \"=))\", \":')\", \"=')\", \":}\", \":}}\", \":]\", \":]]\", \"(:\", \"C:\", \":P\"],\n",
    "        \"neu\": [\":|\", \":/\", \":\\\\\", \"\"],\n",
    "        \"neg\": [\":(\", \":((\", \":(((\", \":((((\", \":C\", \":CC\", \":'C\", \"=(\", \"=((\", \":'(\", \"='(\", \":[\", \":{\", \"):\"]\n",
    "    }\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            word = line.split(\",\")\n",
    "            smil = word[0]\n",
    "            #unicodesmil = chr(int(word[1], 0))\n",
    "            neg = float(word[3])\n",
    "            neu = float(word[4])\n",
    "            pos = float(word[5])\n",
    "            if neu > pos and neu > neg:\n",
    "                emoticons[\"neu\"].append(smil)\n",
    "            elif pos > neg:\n",
    "                emoticons[\"pos\"].append(smil)\n",
    "                #emoticons[\"pos\"].append(unicodesmil)\n",
    "            else:\n",
    "                emoticons[\"neg\"].append(smil)\n",
    "    return emoticons\n",
    "\n",
    "print([emoticons(), badwords(), stopwords()])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "                                           submission  \\\n0   The author Tolkiens son has released a new fan...   \n1   The author Tolkiens son has released a new fan...   \n2   The author Tolkiens son has released a new fan...   \n3   The author Tolkiens son has released a new fan...   \n4   The author Tolkiens son has released a new fan...   \n5   The author Tolkiens son has released a new fan...   \n6   The author Tolkiens son has released a new fan...   \n7   The author Tolkiens son has released a new fan...   \n8   The author Tolkiens son has released a new fan...   \n9   The author Tolkiens son has released a new fan...   \n10  The author Tolkiens son has released a new fan...   \n11  The author Tolkiens son has released a new fan...   \n12  The author Tolkiens son has released a new fan...   \n13  The author Tolkiens son has released a new fan...   \n14  The author Tolkiens son has released a new fan...   \n15  The author Tolkiens son has released a new fan...   \n16  The author Tolkiens son has released a new fan...   \n17  The author Tolkiens son has released a new fan...   \n18  The author Tolkiens son has released a new fan...   \n19  The author Tolkiens son has released a new fan...   \n20  The author Tolkiens son has released a new fan...   \n\n                                                 body  \n0   Hey Tommy the blue, this is a great book for y...  \n1   Awesome books! Awesome author! Where did you g...  \n2   I hate this author :C he is really bad he can ...  \n3   No way! The blue wizards finally got a book?? ...  \n4   Fantasy is boring :/. I'd rather dive into som...  \n5    :D:D YEY!! *amazing* werent they supposed to be?  \n6               this is an old bag with herpes book 👎  \n7                             this wook I like 💚💚💚 !!  \n8                              marry this book I will  \n9   if you are going to buy this book, make sure y...  \n10                             how much does it cost?  \n11                                      I can pay $$$  \n12                      BLUE. JUNO. 100% sh!t http://  \n13          loooooooooooooooooooooooooooool trolololo  \n14  The authors son has released a new fantasy ser...  \n15                                author, blue wizard  \n16  Andrei svensson Daniel, Laura Marx Engels, Jon...  \n17  marry fantasy blue Nietzsche awsome Tommy trol...  \n18                           https://asoftmurmur.com/  \n19  we'll use one word from each in the next comme...  \n20  Tommy, blue, awesome book, ugly, fantasy, amaz...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>submission</th>\n      <th>body</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>Hey Tommy the blue, this is a great book for y...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>Awesome books! Awesome author! Where did you g...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>I hate this author :C he is really bad he can ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>No way! The blue wizards finally got a book?? ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>Fantasy is boring :/. I'd rather dive into som...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>:D:D YEY!! *amazing* werent they supposed to be?</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>this is an old bag with herpes book 👎</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>this wook I like 💚💚💚 !!</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>marry this book I will</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>if you are going to buy this book, make sure y...</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>how much does it cost?</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>I can pay $$$</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>BLUE. JUNO. 100% sh!t http://</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>loooooooooooooooooooooooooooool trolololo</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>The authors son has released a new fantasy ser...</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>author, blue wizard</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>Andrei svensson Daniel, Laura Marx Engels, Jon...</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>marry fantasy blue Nietzsche awsome Tommy trol...</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>https://asoftmurmur.com/</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>we'll use one word from each in the next comme...</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>Tommy, blue, awesome book, ugly, fantasy, amaz...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "import pandas as pd\n",
    "submission = \"The author Tolkiens son has released a new fantasy series! The first book is about the Blue wizards! OMG YOU HAVE TO BUY :D:D\"\n",
    "sentences = [\n",
    "    \"Hey Tommy the blue, this is a great book for you 😁💚\",\n",
    "    \"Awesome books! Awesome author! Where did you get it? :)\",\n",
    "    \"I hate this author :C he is really bad he can go and ugly himself ugly\",\n",
    "    \"No way! The blue wizards finally got a book?? OMG I have to buy it xD 👻\",\n",
    "    \"Fantasy is boring :/. I'd rather dive into some Nietzsche and I think everyone should\",\n",
    "    \":D:D YEY!! *amazing* werent they supposed to be?\",\n",
    "    \"this is an old bag with herpes book 👎\",\n",
    "    \"this wook I like 💚💚💚 !!\",\n",
    "    \"marry this book I will\",\n",
    "    \"if you are going to buy this book, make sure you get it at towns hall because they have extra t-shirts!\",\n",
    "    \"how much does it cost?\",\n",
    "    \"I can pay $$$\",\n",
    "    \"BLUE. JUNO. 100% sh!t http://\",\n",
    "    \"loooooooooooooooooooooooooooool trolololo\",\n",
    "    \"The authors son has released a new fantasy series! The first book is about the Blue wizards! OMG YOU HAVE TO BUY :D:D\",\n",
    "    \"author, blue wizard\",\n",
    "    \"Andrei svensson Daniel, Laura Marx Engels, Jonas!, Carl Sagan, Charlie Chapplin$, Walt Disney%, Frank Sinatra\",\n",
    "    \"marry fantasy blue Nietzsche awsome Tommy trolololo\",\n",
    "    \"https://asoftmurmur.com/\",\n",
    "    \"we'll use one word from each in the next comment to check similarity for all other comments\",\n",
    "    \"Tommy, blue, awesome book, ugly, fantasy, amazing, author\"\n",
    "]\n",
    "df = pd.DataFrame({\"submission\": [submission for i in range(len(sentences))], \"body\": sentences})\n",
    "display(df)"
   ]
  },
  {
   "source": [
    "### Links \n",
    "- https://www.researchgate.net/publication/221397355_Complex_Linguistic_Features_for_Text_Classification_A_Comprehensive_Study"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "is -> be\n",
      "did -> do\n",
      "is -> be\n",
      "wizards -> wizard\n",
      "got -> get\n",
      "is -> be\n",
      "supposed -> suppose\n",
      "is -> be\n",
      "are -> be\n",
      "going -> go\n",
      "towns -> town\n",
      "does -> do\n",
      "authors -> author\n",
      "has -> have\n",
      "released -> release\n",
      "is -> be\n",
      "comments -> comment\n",
      "['!!', '$$$', '*amazing*', '100%', ':)', ':/.', ':c', ':d:d', 'a', 'about', 'all', 'amazing,', 'an', 'and', 'andrei', 'at', 'author', 'author!', 'author,', 'awesome', 'awsome', 'bad', 'bag', 'be', 'be?', 'because', 'blue', 'blue,', 'blue.', 'book', 'book,', 'book??', 'books!', 'boring', 'buy', 'can', 'carl', 'chapplin$,', 'charlie', 'check', 'comment', 'cost?', 'daniel,', 'disney%,', 'dive', 'do', 'each', 'engels,', 'everyone', 'extra', 'fantasy', 'fantasy,', 'finally', 'first', 'for', 'frant', 'from', 'get', 'go', 'great', 'hall', 'hate', 'have', 'he', 'herpes', 'hey', 'himself', 'how', 'http://', 'https://asoftmurmur.com/', 'i', \"i'd\", 'if', 'in', 'into', 'it', 'it?', 'jonas!,', 'juno.', 'laura', 'like', 'loooooooooooooooooooooooooooool', 'make', 'marry', 'marx', 'much', 'new', 'next', 'nietzsche', 'no', 'old', 'omg', 'one', 'other', 'pay', 'rather', 'really', 'release', 'sagan,', 'series!', 'sh!t', 'should', 'similarity', 'sinatra', 'some', 'son', 'suppose', 'sure', 'svensson', 't-shirts!', 'the', 'they', 'think', 'this', 'to', 'tommy', 'tommy,', 'town', 'trolololo', 'ugly', 'ugly,', 'use', 'walt', 'way!', \"we'll\", 'werent', 'where', 'will', 'with', 'wizard', 'wizards!', 'wook', 'word', 'xd', 'yey!!', 'you', '👎', '👻', '💚💚💚', '😁💚']\n",
      "wizards -> wizard\n",
      "supposed -> suppose\n",
      "towns -> town\n",
      "authors -> author\n",
      "released -> release\n",
      "comments -> comment\n",
      "blue, -> blue\n",
      "books! -> book\n",
      "author! -> author\n",
      "wizards -> wizard\n",
      "book?? -> book\n",
      ":/. -> :\n",
      "yey!! -> yey\n",
      "*amazing* -> amazing\n",
      "supposed -> suppose\n",
      "!! -> \n",
      "book, -> book\n",
      "towns -> town\n",
      "t-shirts! -> t-shirt\n",
      "cost? -> cost\n",
      "$$$ -> \n",
      "blue. -> blue\n",
      "juno. -> juno\n",
      "100% -> \n",
      "http:// -> http:\n",
      "authors -> author\n",
      "released -> release\n",
      "series! -> series\n",
      "wizards! -> wizard\n",
      "author, -> author\n",
      "daniel, -> daniel\n",
      "engels, -> engels\n",
      "jonas!, -> jonas\n",
      "sagan, -> sagan\n",
      "chapplin$, -> chapplin\n",
      "disney%, -> disney\n",
      "https://asoftmurmur.com/ -> https://asoftmurmur.com\n",
      "comments -> comment\n",
      "tommy, -> tommy\n",
      "blue, -> blue\n",
      "book, -> book\n",
      "fantasy, -> fantasy\n",
      "amazing, -> amazing\n",
      "is -> be\n",
      "did -> do\n",
      "is -> be\n",
      "wizards -> wizard\n",
      "got -> get\n",
      "is -> be\n",
      "supposed -> suppose\n",
      "is -> be\n",
      "are -> be\n",
      "going -> go\n",
      "towns -> town\n",
      "does -> do\n",
      "authors -> author\n",
      "has -> have\n",
      "released -> release\n",
      "is -> be\n",
      "comments -> comment\n",
      "is -> be\n",
      "did -> do\n",
      "is -> be\n",
      "wizards -> wizard\n",
      "got -> get\n",
      "is -> be\n",
      "supposed -> suppose\n",
      "is -> be\n",
      "are -> be\n",
      "going -> go\n",
      "towns -> town\n",
      "does -> do\n",
      "authors -> author\n",
      "has -> have\n",
      "released -> release\n",
      "is -> be\n",
      "comments -> comment\n",
      "is -> be\n",
      "did -> do\n",
      "is -> be\n",
      "wizards -> wizard\n",
      "got -> get\n",
      "is -> be\n",
      "supposed -> suppose\n",
      "is -> be\n",
      "are -> be\n",
      "going -> go\n",
      "towns -> town\n",
      "does -> do\n",
      "authors -> author\n",
      "has -> have\n",
      "released -> release\n",
      "is -> be\n",
      "comments -> comment\n",
      "released -> release\n",
      "wizards -> wizard\n",
      "supposed -> suppose\n",
      "towns -> town\n",
      "authors -> author\n",
      "released -> release\n",
      "comments -> comment\n",
      "boring -> bore\n",
      "wizards -> wizard\n",
      "supposed -> suppose\n",
      "towns -> town\n",
      "authors -> author\n",
      "released -> release\n",
      "comments -> comment\n",
      "wizards -> wizard\n",
      "supposed -> suppose\n",
      "towns -> town\n",
      "authors -> author\n",
      "released -> release\n",
      "comments -> comment\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                           submission  \\\n",
       "0   The author Tolkiens son has released a new fan...   \n",
       "1   The author Tolkiens son has released a new fan...   \n",
       "2   The author Tolkiens son has released a new fan...   \n",
       "3   The author Tolkiens son has released a new fan...   \n",
       "4   The author Tolkiens son has released a new fan...   \n",
       "5   The author Tolkiens son has released a new fan...   \n",
       "6   The author Tolkiens son has released a new fan...   \n",
       "7   The author Tolkiens son has released a new fan...   \n",
       "8   The author Tolkiens son has released a new fan...   \n",
       "9   The author Tolkiens son has released a new fan...   \n",
       "10  The author Tolkiens son has released a new fan...   \n",
       "11  The author Tolkiens son has released a new fan...   \n",
       "12  The author Tolkiens son has released a new fan...   \n",
       "13  The author Tolkiens son has released a new fan...   \n",
       "14  The author Tolkiens son has released a new fan...   \n",
       "15  The author Tolkiens son has released a new fan...   \n",
       "16  The author Tolkiens son has released a new fan...   \n",
       "17  The author Tolkiens son has released a new fan...   \n",
       "18  The author Tolkiens son has released a new fan...   \n",
       "19  The author Tolkiens son has released a new fan...   \n",
       "20  The author Tolkiens son has released a new fan...   \n",
       "\n",
       "                                                 body  lnk  top-cos-sim  \\\n",
       "0   Hey Tommy the blue, this is a great book for y...    0     0.082471   \n",
       "1   Awesome books! Awesome author! Where did you g...    0     0.000000   \n",
       "2   I hate this author :C he is really bad he can ...    0     0.114108   \n",
       "3   No way! The blue wizards finally got a book?? ...    0     0.153396   \n",
       "4   Fantasy is boring :/. I'd rather dive into som...    0     0.093090   \n",
       "5    :D:D YEY!! *amazing* werent they supposed to be?    0     0.118058   \n",
       "6               this is an old bag with herpes book 👎    0     0.117727   \n",
       "7                             this wook I like 💚💚💚 !!    0     0.000000   \n",
       "8                              marry this book I will    0     0.123411   \n",
       "9   if you are going to buy this book, make sure y...    0     0.093090   \n",
       "10                             how much does it cost?    0     0.000000   \n",
       "11                                      I can pay $$$    0     0.000000   \n",
       "12                      BLUE. JUNO. 100% sh!t http://    1     0.000000   \n",
       "13          loooooooooooooooooooooooooooool trolololo    0     0.000000   \n",
       "14  The authors son has released a new fantasy ser...    0     1.000000   \n",
       "15                                author, blue wizard    0     0.123411   \n",
       "16  Andrei svensson Daniel, Laura Marx Engels, Jon...    0     0.000000   \n",
       "17  marry fantasy blue Nietzsche awsome Tommy trol...    0     0.176606   \n",
       "18                           https://asoftmurmur.com/    1     0.000000   \n",
       "19  we'll use one word from each in the next comme...    0     0.000000   \n",
       "20  Tommy, blue, awesome book, ugly, fantasy, amaz...    0     0.081873   \n",
       "\n",
       "     cos-sim  tfidf-mean  nam  wc  sw  bw  smil+  smil-  smil&  \n",
       "0   0.254778    0.202954    1   6   6   0      2      0      0  \n",
       "1   0.232763    0.240204    0   5   4   0      1      0      0  \n",
       "2   0.213965    0.131112    0   4  10   2      0      1      0  \n",
       "3   0.296867    0.186447    0   7   8   1      1      0      1  \n",
       "4   0.208668    0.162718    1   6   9   0      0      0      1  \n",
       "5   0.244467    0.319057    0   4   3   0      2      0      0  \n",
       "6   0.179406    0.198450    0   3   5   2      0      1      0  \n",
       "7   0.192876    0.288675    0   3   3   0      3      0      0  \n",
       "8   0.173056    0.343660    0   3   2   0      0      0      0  \n",
       "9   0.261426    0.116227    0   6  15   0      0      0      0  \n",
       "10  0.111357    0.200000    0   1   4   0      0      0      0  \n",
       "11  0.157483    0.353553    0   2   2   0      0      0      0  \n",
       "12  0.222714    0.447214    0   4   0   1      0      0      1  \n",
       "13  0.150209    0.705688    0   2   0   0      0      0      0  \n",
       "14  0.316661    0.143155    1  10  12   1      2      0      0  \n",
       "15  0.173056    0.572766    0   3   0   0      0      0      0  \n",
       "16  0.431284    0.258199    5  15   0   0      0      0      0  \n",
       "17  0.262707    0.376452    2   7   0   0      0      0      0  \n",
       "18  0.111357    1.000000    0   1   0   0      0      0      1  \n",
       "19  0.210445    0.111166    0   5  12   0      0      0      0  \n",
       "20  0.297772    0.402757    0   7   0   0      0      0      0  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>submission</th>\n      <th>body</th>\n      <th>lnk</th>\n      <th>top-cos-sim</th>\n      <th>cos-sim</th>\n      <th>tfidf-mean</th>\n      <th>nam</th>\n      <th>wc</th>\n      <th>sw</th>\n      <th>bw</th>\n      <th>smil+</th>\n      <th>smil-</th>\n      <th>smil&amp;</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>Hey Tommy the blue, this is a great book for y...</td>\n      <td>0</td>\n      <td>0.082471</td>\n      <td>0.254778</td>\n      <td>0.202954</td>\n      <td>1</td>\n      <td>6</td>\n      <td>6</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>Awesome books! Awesome author! Where did you g...</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>0.232763</td>\n      <td>0.240204</td>\n      <td>0</td>\n      <td>5</td>\n      <td>4</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>I hate this author :C he is really bad he can ...</td>\n      <td>0</td>\n      <td>0.114108</td>\n      <td>0.213965</td>\n      <td>0.131112</td>\n      <td>0</td>\n      <td>4</td>\n      <td>10</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>No way! The blue wizards finally got a book?? ...</td>\n      <td>0</td>\n      <td>0.153396</td>\n      <td>0.296867</td>\n      <td>0.186447</td>\n      <td>0</td>\n      <td>7</td>\n      <td>8</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>Fantasy is boring :/. I'd rather dive into som...</td>\n      <td>0</td>\n      <td>0.093090</td>\n      <td>0.208668</td>\n      <td>0.162718</td>\n      <td>1</td>\n      <td>6</td>\n      <td>9</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>:D:D YEY!! *amazing* werent they supposed to be?</td>\n      <td>0</td>\n      <td>0.118058</td>\n      <td>0.244467</td>\n      <td>0.319057</td>\n      <td>0</td>\n      <td>4</td>\n      <td>3</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>this is an old bag with herpes book 👎</td>\n      <td>0</td>\n      <td>0.117727</td>\n      <td>0.179406</td>\n      <td>0.198450</td>\n      <td>0</td>\n      <td>3</td>\n      <td>5</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>this wook I like 💚💚💚 !!</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>0.192876</td>\n      <td>0.288675</td>\n      <td>0</td>\n      <td>3</td>\n      <td>3</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>marry this book I will</td>\n      <td>0</td>\n      <td>0.123411</td>\n      <td>0.173056</td>\n      <td>0.343660</td>\n      <td>0</td>\n      <td>3</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>if you are going to buy this book, make sure y...</td>\n      <td>0</td>\n      <td>0.093090</td>\n      <td>0.261426</td>\n      <td>0.116227</td>\n      <td>0</td>\n      <td>6</td>\n      <td>15</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>how much does it cost?</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>0.111357</td>\n      <td>0.200000</td>\n      <td>0</td>\n      <td>1</td>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>I can pay $$$</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>0.157483</td>\n      <td>0.353553</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>BLUE. JUNO. 100% sh!t http://</td>\n      <td>1</td>\n      <td>0.000000</td>\n      <td>0.222714</td>\n      <td>0.447214</td>\n      <td>0</td>\n      <td>4</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>loooooooooooooooooooooooooooool trolololo</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>0.150209</td>\n      <td>0.705688</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>The authors son has released a new fantasy ser...</td>\n      <td>0</td>\n      <td>1.000000</td>\n      <td>0.316661</td>\n      <td>0.143155</td>\n      <td>1</td>\n      <td>10</td>\n      <td>12</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>author, blue wizard</td>\n      <td>0</td>\n      <td>0.123411</td>\n      <td>0.173056</td>\n      <td>0.572766</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>Andrei svensson Daniel, Laura Marx Engels, Jon...</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>0.431284</td>\n      <td>0.258199</td>\n      <td>5</td>\n      <td>15</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>marry fantasy blue Nietzsche awsome Tommy trol...</td>\n      <td>0</td>\n      <td>0.176606</td>\n      <td>0.262707</td>\n      <td>0.376452</td>\n      <td>2</td>\n      <td>7</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>https://asoftmurmur.com/</td>\n      <td>1</td>\n      <td>0.000000</td>\n      <td>0.111357</td>\n      <td>1.000000</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>we'll use one word from each in the next comme...</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>0.210445</td>\n      <td>0.111166</td>\n      <td>0</td>\n      <td>5</td>\n      <td>12</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>Tommy, blue, awesome book, ugly, fantasy, amaz...</td>\n      <td>0</td>\n      <td>0.081873</td>\n      <td>0.297772</td>\n      <td>0.402757</td>\n      <td>0</td>\n      <td>7</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel, sigmoid_kernel, rbf_kernel, polynomial_kernel, laplacian_kernel, cosine_similarity\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "\n",
    "class LemmaTokenizer:\n",
    "    ignoretokens = []\n",
    "    stopwords = []\n",
    "    vocab = []\n",
    "    lcase = True\n",
    "    def __init__(self, stopwords=[], vocabulary=[], ignoretokens=[], lcase=True):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "        self.stopwords = stopwords\n",
    "        self.vocab = vocabulary\n",
    "        self.ignoretokens = ignoretokens\n",
    "        self.lcase = lcase\n",
    "    def __call__(self, document):\n",
    "        lemmas = []\n",
    "        tokenized_words = document.split(\" \")\n",
    "        for word, tag in pos_tag(tokenized_words):\n",
    "            lower_cased_tag = tag[0].lower()\n",
    "            stripped_word = word.strip(''.join(self.ignoretokens))\n",
    "            if (word != stripped_word):\n",
    "                #print(f\"{word} -> {stripped_word}\")\n",
    "                pass\n",
    "            wn_tag = lower_cased_tag if lower_cased_tag in ['a', 'r', 'n', 'v'] else None\n",
    "            if not wn_tag:\n",
    "                lemma = stripped_word\n",
    "            else:\n",
    "                lemma = self.wnl.lemmatize(stripped_word, wn_tag)\n",
    "            if lemma not in list(self.stopwords): # and word in self.vocab:\n",
    "                lemmas.append(lemma.lower() if self.lcase else lemma)\n",
    "                if word != lemma:\n",
    "                    print(f\"{word} -> {lemma}\")\n",
    "        return lemmas\n",
    "\n",
    "swords = stopwords()\n",
    "bwords = badwords()\n",
    "cwords = commonwords()\n",
    "ignore = ignoretokens()\n",
    "smil = emoticons()\n",
    "nam = names()\n",
    "\n",
    "#print(cwords)\n",
    "\n",
    "# All vocabs\n",
    "cva = CountVectorizer(lowercase=True, tokenizer=LemmaTokenizer())\n",
    "asdf = cva.fit_transform(df.get(\"body\"))\n",
    "all_vocabs = cva.get_feature_names()\n",
    "print(all_vocabs)\n",
    "\n",
    "# Filtered vocabs\n",
    "cvf = CountVectorizer(lowercase=True, tokenizer=LemmaTokenizer(stopwords=list(swords)+list(bwords)))\n",
    "asdf = cvf.fit_transform(df.get(\"body\"))\n",
    "filtered_vocab = cvf.get_feature_names()\n",
    "\n",
    "# Significant words count\n",
    "cv = CountVectorizer(lowercase=True, tokenizer=LemmaTokenizer(stopwords=list(swords)+list(bwords), ignoretokens=ignore))\n",
    "wc_data = cv.fit_transform(df.get(\"body\"))\n",
    "wc_df = pd.DataFrame(wc_data.sum(axis=1))\n",
    "wc_df.columns = [\"wc\"]\n",
    "\n",
    "# Bad words count\n",
    "cv = CountVectorizer(vocabulary=bwords, stop_words=None, lowercase=True, tokenizer=LemmaTokenizer(), ngram_range=(1,2)) # finds \"old bag\"\n",
    "bw_data = cv.fit_transform(df.get(\"body\"))\n",
    "bw_df = pd.DataFrame(bw_data.sum(axis=1))\n",
    "bw_df.columns = [\"bw\"]\n",
    "\n",
    "# Stop words count\n",
    "cv = CountVectorizer(vocabulary=swords, stop_words=None, lowercase=True, tokenizer=LemmaTokenizer())\n",
    "sw_data = cv.fit_transform(df.get(\"body\"))\n",
    "sw_df = pd.DataFrame(sw_data.sum(axis=1))\n",
    "sw_df.columns = [\"sw\"]\n",
    "\n",
    "# Names count\n",
    "cv = CountVectorizer(vocabulary=nam, stop_words=None, lowercase=False, tokenizer=LemmaTokenizer(lcase=False))\n",
    "nam_data = cv.fit_transform(df.get(\"body\"))\n",
    "nam_df = pd.DataFrame(nam_data.sum(axis=1))\n",
    "nam_df.columns = [\"nam\"]\n",
    "\n",
    "# Positive smilies\n",
    "cv = CountVectorizer(vocabulary=smil[\"pos\"], analyzer=\"char\", ngram_range=(1,2), stop_words=None, lowercase=False) # char + 2 ngram = \":D\"\n",
    "smilp_data = cv.fit_transform(df.get(\"body\"))\n",
    "smilp_df = pd.DataFrame(smilp_data.sum(axis=1))\n",
    "smilp_df.columns = [\"smil+\"]\n",
    "\n",
    "# Negative smilies count\n",
    "cv = CountVectorizer(vocabulary=smil[\"neg\"], analyzer=\"char\", ngram_range=(1,2), stop_words=None, lowercase=False) # char + 2 ngram = \":D\"\n",
    "sniln_data = cv.fit_transform(df.get(\"body\"))\n",
    "smiln_df = pd.DataFrame(sniln_data.sum(axis=1))\n",
    "smiln_df.columns = [\"smil-\"]\n",
    "\n",
    "# Neutral smilies count\n",
    "cv = CountVectorizer(vocabulary=smil[\"neu\"], analyzer=\"char\", ngram_range=(1,2), stop_words=None, lowercase=False) # char + 2 ngram = \":D\"\n",
    "smile_data = cv.fit_transform(df.get(\"body\"))\n",
    "smile_df = pd.DataFrame(smile_data.sum(axis=1))\n",
    "smile_df.columns = [\"smil&\"]\n",
    "\n",
    "# TF-IDF cosine similarity toawrd topic\n",
    "submission_with_comments = [submission] + list(df.get(\"body\").array)\n",
    "tfidfv = TfidfVectorizer(vocabulary=filtered_vocab, lowercase=True, ngram_range=(1,1), smooth_idf=True, tokenizer=LemmaTokenizer(stopwords=list(swords)))\n",
    "tfidf_data = tfidfv.fit_transform(submission_with_comments)\n",
    "cosine_similarities = cosine_similarity(tfidf_data[0:1], tfidf_data[1:]).flatten()\n",
    "top_simi_df = pd.DataFrame(cosine_similarities)\n",
    "top_simi_df.columns = [\"top-cos-sim\"]\n",
    "\n",
    "# TF-IDF cosine similarity towards all documents\n",
    "# (possible to do just as toward topic by changing first element from submission to \" \".join(all_vocabs))\n",
    "submission_with_comments = [\" \".join(all_vocabs)] + list(df.get(\"body\").array)\n",
    "tfidfv = TfidfVectorizer(vocabulary=filtered_vocab, lowercase=True, ngram_range=(1,1), smooth_idf=True, tokenizer=LemmaTokenizer(stopwords=list(swords)))\n",
    "tfidf_data = tfidfv.fit_transform(submission_with_comments)\n",
    "cosine_similarities = cosine_similarity(tfidf_data[0:1], tfidf_data[1:]).flatten()\n",
    "all_simi_df = pd.DataFrame(cosine_similarities)\n",
    "all_simi_df.columns = [\"cos-sim\"]\n",
    "\n",
    "#tfidfv = TfidfVectorizer(vocabulary=all_vocabs, lowercase=True, ngram_range=(1,1), smooth_idf=True)\n",
    "#tfidf_data = tfidfv.fit_transform(df.get(\"body\"))\n",
    "#cosine_similarities = cosine_similarity(tfidf_data, tfidf_data)\n",
    "#cosine_similarities[cosine_similarities == 0] = np.nan\n",
    "#cosine_similarities = np.nanmean(cosine_similarities, axis=1)\n",
    "#all_simi_df = pd.DataFrame(cosine_similarities)\n",
    "#all_simi_df.columns = [\"cos-sim\"]\n",
    "\n",
    "# TF-IDF mean value (checks across all documents)\n",
    "tfidfv = TfidfVectorizer(vocabulary=all_vocabs, lowercase=True, ngram_range=(1,1), smooth_idf=True, tokenizer=LemmaTokenizer(stopwords=list(swords)))\n",
    "tfidf_data = tfidfv.fit_transform(df.get(\"body\")).todense()\n",
    "means = [0]*tfidf_data.shape[0]\n",
    "#means = np.nanmean(tfidf_data, axis=1)\n",
    "for i in range(0, tfidf_data.shape[0]):\n",
    "    word_count = wc_df.get(\"wc\")[i] + bw_df.get(\"bw\")[i] + sw_df.get(\"sw\")[i]\n",
    "    means[i] = tfidf_data[i].sum()/word_count if word_count > 0 else 0\n",
    "    #tfidf_data[i][tfidf_data[i] == 0] = np.nan\n",
    "tfidf_df = pd.DataFrame(means)\n",
    "tfidf_df.columns = [\"tfidf-mean\"]\n",
    "\n",
    "# Has link\n",
    "df['lnk'] = [1 if \"http\" in row[['body']].to_string() else 0 for i,row in df.iterrows()]\n",
    "cv_dataframe = pd.concat([df, top_simi_df, all_simi_df, tfidf_df, nam_df, wc_df, sw_df, bw_df, smilp_df, smiln_df, smile_df], axis=1)\n",
    "cv_dataframe"
   ]
  }
 ]
}