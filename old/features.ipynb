{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('python_modules': venv)",
   "metadata": {
    "interpreter": {
     "hash": "9634d4b890a726f71d8044046bc71cdc391c406dc663847f104c7db04bcb4cdc"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[{'pos': [':)', ':))', ':D', ':DD', 'xD', 'xDD', ':d=)', '=))', \":')\", \"=')\", ':}', ':}}', ':]', ':]]', '(:', 'C:', ':P', '😂', '❤', '♥', '😍', '😘', '😊', '👌', '💕', '👏', '😁', '☺', '♡', '👍', '🙏', '✌', '😏', '😉', '🙌', '🙈', '💪', '😄', '💃', '💖', '😃', '😱', '🎉', '😜', '🌸', '💜', '💙', '😳', '💗', '☀', '😎', '😢', '💋', '😋', '🙊', '🎶', '💞', '😌', '💯', '💛', '💁', '💚', '😆', '😝', '😅', '👊', '😀', '😚', '😻', '💘', '👋', '✋', '🎊', '🍕', '❄', '😥', '😈', '🔝', '⚽', '👑', '😹', '🍃', '🎁', '🐧', '🎈', '✊', '💤', '💓', '💦', '🙋', '🎄', '🎵', '😛', '😬', '👯', '💎', '🎂', '👫', '🏆', '☝', '😙', '⛄', '👅', '♪', '🍂', '💏', '🌴', '👈', '🌹', '🙆', '🍻', '🌞', '🍁', '⭐', '🎀', '🙉', '🌺', '💅', '🐶', '🌚', '🎤', '👭', '🎧', '👆', '🍸', '🍉', '😇', '🏃', '🍺', '🎸', '🍹', '💫', '📚', '🌷', '💝', '💨', '🏈', '💍', '☔', '👸', '🇪', '🍩', '☁', '🌻', '😵', '↿', '🐯', '👼', '🍔', '😸', '👶', '↾', '💐', '🌊', '🍦', '🍓', '💆', '🍴', '🇸', '😮', '😽', '🌈', '🙀', '🎮', '🍆', '🍰', '🙇', '🍟', '🍌', '💑', '🐣', '🎃', '😟', '🐾', '🎓', '🏊', '🍫', '📷', '👄', '🌼', '🐱', '🇺', '🚬', '📖', '🐒', '🌍', '┊', '🐥', '💄', '💸', '⛔', '🏀', '💉', '💟', '😯', '♦', '🌙', '🐟', '👣', '🗿', '🍝', '🍭', '❌', '🐰', '💊', '🚨', '🍪', '🎆', '🎎', '🇩', '✅', '🍑', '🔊', '🌌', '🍎', '🐻', '💇', '🍊', '🍒', '🐭', '👟', '🌎', '🍍', '🐮', '📲', '🌅', '🇷', '👠', '🌽', '🍬', '😺', '🚀', '¦', '🍧', '🍜', '🐏', '👧', '🏄', '🍋', '🆗', '📺', '🍅', '⛅', '👙', '🏡', '🌾', '✏', '🐬', '🇹', '♣', '🇮', '🐍', '♔', '🍳', '🔵', '🌕', '🐨', '🔐', '💿', '🌳', '👰', '⚓', '🚴', '👗', '➕', '💬', '🔜', '🍨', '🍙', '🍗', '🍲', '😼', '🐙', '👨', '🍚', '🍖', '♨', '▃', '🚘', '👩', '🐠', '🚹', '💵', '✰', '👛', '🌱', '🌏', '🌲', '👴', '🏠', '🍇', '🍘', '🍛', '🐇', '👵', '🌵', '🎇', '🐎', '🐤', '🛀', '🌑', '🚲', '🏁', '🎾', '🐵', '◕', '🗼', '🍵', '🍯', '⇨', '🌓', '🔒', '👳', '♩', '💌', '🌜', '🚿', '🔆', '🌛', '🏩', '🇫', '📢', '🐦', '♻', '🌘', '🍐', '🌔', '╥', '👖', '😗', '🐄', '⬇', '🚼', '🌗', '🌖', '🔅', '👜', '🐌', '💼', '🐹', '🌠', '⚫', '♧', '🎢', '🎷', '🌇', '⏰', '◠', '🎿', '🆔', '🌒', '🐪', '╝', '👔', '🐋', '▽', '🐛', '👕', '💳', '🏧', '💡', '⬅', '🇱', '📹', '👞', '🚑', '🆘', '👚', '🚍', '🚣', '🏉', '🗻', '⛺', '🏂', '👡', '📻', '🌰', '🎒', '⌒', '📴', '🚢', '🔔', '◢', '🏥', '🃏', '💒', '🐐', '🔚', '🔓', '🎽', '📅', '🎺', '✉', '◤', '○', '🍼', '📣', '🐗', '⛳', '┛', '┃', '💺', '☻', '📞', '🌉', '✎', '📃', '💷', '🚄', '▲', '⛵', '⌛', '🚜', '👒', '❕', '🔛', '🇲', '❅', '👝', '✞', '🎋', '👥', '◆', '🔭', '🐜', '♌', '👷', '📄', '🚐', '🌋', '📡', '🚳', '✘', '🅰', '🇼', '┓', '┣', 'Ⓛ', 'Ⓔ', '👤', '🎠', '📗', '🔩', '👢', '📰', 'Ⓜ'], 'neu': [':|', ':/', ':\\\\', '', '☯', '✨', '★', '█', '🔥', '♫', '�', '©', '👀', '🐓', '☕', '💥', '►', '✈', '👉', '☆', '🍀', '🎅', '✔', '⚡', '➡', '🌿', '🌟', '🔮', '❗', '✖', '🔪', '➜', '👻', '💰', '▪', '━', '☷', '🐷', '👽', '🍷', '®', '☑', '│', '💣', '▶', '░', '👾', '📒', '👇', '▓', '⚠', '╯', '✓', '▬', '🚶', '║', '🐸', '✿', '🌀', '🐼', '🎥', '●', '🚗', '📝', '═', '💭', '☞', '🌃', '╭', '✧', '╮', '👹', '📱', '🎼', '─', '╰', '♬', '♚', '🔴', '☼', '❓', '🐴', '💢', '🎬', '🐘', '⠀', '➤', '⬆', '⚪', '🐢', '◉', '🍤', '🐝', '🌝', '❁', '❀', '▀', '▒', '💲', '⛽', '▸', '♛', '🎹', '♕', '🍏', '👦', '🇬', '🇧', '☠', '╠', '🚙', '💻', '▄', '👓', '◄', '🔞', '◀', '🔙', '🐽', '➔', '💶', '╩', '🐑', '🍞', '╚', '🈹', '🐳', '✪', '▐', '♠', '🐚', '👂', '🗽', '🆒', '🐺', '➨', '╬', '🌂', '🚌', '🍡', '❥', '🎡', '🐩', '⌚', '🐖', '🐔', '🐲', '❊', '🚺', '◟', '🍢', '🎨', '⛲', '▁', '🇴', '🚕', '🐈', '⇧', '☎', '🌁', '🏰', '🚵', '🎐', '╗', '╱', '⇩', '🚂', '✦', '⛪', '╔', '🔱', '🆓', '▂', '🚋', '🌆', '🔹', '🐫', '🏪', '۩', '🐂', '✳', '🐀', '╦', '🐕', '✒', '🏢', '🚚', '🐉', '❒', '🐊', '🚖', '▼', '☛', '✩', '🚤', '🎻', '🔷', '🚦', '✯', '╣', '📀', '🚛', '📓', '☉', '💴', '┼', '🐃', '🍄', '📕', '🚓', '↪', '👱', 'Ⓐ', '🏨', '♎', '🔸', '🐆', '♢', '◡', '📵', '🐡', '🏯', '☂', '🎪', '↳', '🔈', '📍', '🚔', '⏩', '۞', '☾', '📥', '🔦', '🚁', '🐁', '♂', '◞', '📯', '◂', '📶', '🚥', '🌄', '🗾', '🔶', '🏤', '🎩', '🐅', '♮', '🔄', '☄', '☨'], 'neg': [':(', ':((', ':(((', ':((((', ':C', ':CC', \":'C\", '=(', '=((', \":'(\", \"='(\", ':[', ':{', '):', '😭', '😩', '😒', '😔', '😡', '😴', '🔫', '😞', '😪', '😫', '💀', '😕', '💔', '😤', '😰', '😑', '😠', '😓', '😣', '😐', '😨', '😖', '👎', '😷', '💩', '🙅', '😿', '😲', '😶', '😧', '🚫', '👐', '👬', '￼', '👿', '✂', '👪', '😦', '🍣', '🙍', '🍱', '💧', '🔋', '😾', '🍥', '⚾', '🍮', '👮', '☹', '┳', '👺', '💂', '🙎', '🔨', '🎭', '┈', '🍠', '□', '🏫', '❔', '▌', '■', '🍈', '➰', '🔌', '┻', '⏳', '🏇', '🚩', '📌', '☐', '┐', '☮', '🔧', '🅾']}, {'daterape', 'genitals', 'shitey', 'cockmaster', 'hotsex', 'bootee', 'cuntslut', 'screw', 'fuck puppet', 'pisser', 'vibrator', 'seaman', 'assholes', 'wench', 'wanky', 'fingering', 'two girls one cup', 'feltcher', 'ganja', 'scrud', 'klan', 'douchebag', 'niggas', 'breasts', 'wrapping men', 'assclown', 'blow job', 'knob end', 'asspirate', 'taig', 'ginger', 'heroin', 'menses', 'bloody', 'hoor', 'he11', 'fxck', 'asswipes', 'faggitt', 'sodomy', 'tw4t', 'div', 'boners', 'gaybob', 'tramp', 'raping', 'orgasims', 'fvck', 'cooter', 'scag', 'dookie', 'mafugly', 'asshopper', 'womb', 'psycho', 'cleveland steamer', 'deggo', 'hobag', 'ejaculatings', 'hell', 'penis', 'kondums', 'fuckface', 'damn', 'dickbeaters', 'jungle bunny', 'ma5terb8', 'hoer', 'knobead', 'humping', 'bint', 'schizo', 'prude', 'ballsack', 'handjob', 'gokkun', 'motherfucks', 'pms', 'tushy', 'cunt', 'rump', 'frotting', 'son-of-a-bitch', 'upskirt', '4r5e', 'suicide girls', 'gender bender', 'dickhead', 'bong', 'clitorus', 'pussy palace', 'bondage', 'prick', 'cornhole', 'fistfuck', 'c.u.n.t', 'topless', 'queaf', 'poontang', 'asshead', 'raghead', 'dickfucker', 'toots', 'shite', 'jailbait', 'wazoo', 'boobies', 'chincs', 'fingerfucked', 'cum', 'hussy', 'azz', 'shittiest', 'ejakulate', 'slutbag', 'son of a whore', 'undies', 'skag', 'fanyy', 'cervix', 'pimpis', 'booooobs', 'whorehopper', 'cocksmoker', 'clit', 'seamen', 'ball licking', 'pawn', 'wh0reface', 'hentai', 'whorealicious', 'fudge-packer', 'buttfucker', 'beef curtains', 'raper', 'master-bate', 'rimming', 'dickhole', 'pleasure chest', 'motherfuckers', 'pimp', 'cumguzzler', 'assmunch', 'ar5e', 'stoned', 'carpet muncher', 'n1gga', 'pron', 'shi+', 'dimwit', 'shrimping', 'bung hole', 'camel toe', 'kums', 'dumass', 'cawk', 'fisting', 'mothafuckings', 'zibbi', 'dry hump', 'munging', 'jack off', 'assbag', 'baby juice', 'phuq', 'fuckingshitmotherfucker', 'lube', 'lezza/lesbo', 'boong', 'dickzipper', 'mick', 'faggs', 'corpulent', 'pussypounder', 'cumstain', 'bimbos', 'footjob', 'c.0.c.k', 'erotic', 'crotte', 'buttmunch', 'polesmoker', 'tub girl', 'naked', 'f4nny', 's_h_i_t', 'goatcx', 'dumb ass', 'pornos', 'pcp', 'fuks', 'acrotomophilia', 'fuckheads', 'beaver cleaver', 'masterbations', 'stupid', 'smartasses', 'tits', 'junkie', 'octopussy', 'hore', 'juggs', 'jerk0ff', 'testes', 'sleazy', 'turd', 'boink', 'wankjob', 'freex', 'lesbos', 'blumpkin', 'rosy palm and her 5 sisters', 'fuckbag', '2 girls 1 cup', 'tard', 'gaytard', 'cyalis', 'cuntface', 'fuckup', 'fubar', 'fudge packer', 'whoar', 'scantily', 'fecker', 'ahole', 'suck', 'boiolas', 'booty', 'penisbanger', 'god damn', 'faggit', 'twatwaffle', 'valium', 'jackass', 'motherfucking', 'cus', 'h0mo', 'sucked', 'aryan', 'shit fucker', 'titties', 'doggin', 'booty call', 'phone sex', 'big breasts', 'minge', 'son of a motherless goat', 'dick-sneeze', 'female squirting', 'diligaf', 'neonazi', 'quim', 'hump', 'extacy', 'flog the log', 'gippo', 'eat a dick', 'eunuch', 'wet dream', 'cox', 'dickface', 'kyke', 'cockhead', 'crabs', 'ritard', 'ponyplay', 'horny', 'shaved beaver', 'sandnigger', 'perversion', 'blonde action', 'asslicker', 'tittiefucker', 'taff', 'cipa', 'w00se', 'figging', 'need the dick', 'dicksucking', 'x-rated', 'tribadism', 'nazi', 'twink', 'skullfuck', 'wanker', 'cockmongruel', 'gangbangs', 'teste', 'hircismus', 'unclefucker', 'glans', 'pthc', 'fellatio', 'homo', 'cut rope', 'queef', 'cocain', 'strip', 'a55', 'fcuker', 'girls gone wild', 'ejaculated', 'shiznit', 'dogging', 'fucked', 'urine', 'cunilingus', 'gaysex', 'playboy', 'threesome', 'ghey', 'asswipe', 'booooooobs', 'jock', 'tittyfucker', 'cum guzzler', 'punta', 'porchmonkey', 'poonani', '2g1c', 'shit ass', 'bookie', 'honky', 'boner', 'cocksmoke', 'fistfuckings', 'goddammit', 'm0fo', 'knob', 'bollox', 'swastika', 'bampot', 'rimjaw', 'hiv', 'cock', 'scat', 'gaydo', 'pecker', 'phuck', 'masterbating', 'dykes', 'bitch', 'mong', 'dagos', 'pigfucker', 'bastard', 'lusting', 'pubic', 'whoring', 'clusterfuck', 'incest', 'midget', 'sexual', 'fags', 'fuk', 'coital', 's-h-i-t', 'junglebunny', 'bunghole', 'hun', 'orgy', 'alabama hot pocket', 'peepee', 'fuckwit', 'kunt', 'raunch', 'faigt', 'shittier', 'flaps', 'pedophile', 'sand nigger', 'nonce', 'nappy', 'nigga', 'm0f0', 'wiseass', 'bod', 'pussi', 'kunilingus', 'bosom', 'assshit', 'poop chute', 'buceta', 'thrust', 'asses', 'pot', 'nigg3r', 'ass-pirate', 'intercourse', 'ghay', 'cocksuka', 'coprophilia', 'screwing', 'shaved pussy', 'group sex', 'bitcher', 'masterbation', 'erect', 'douchewaffle', 'butthole', 'hom0', 'nutter', 'ovums', 'seks', 'douche', 'kwif', 'beef curtain', 'dog style', 'mound of venus', 'chinc', 'tart', 'cum freak', 'kooches', 'donkey punch', 'dick-ish', 'assmaster', 'stfu', 'gfy', 'ass-hat', 'labia', 'retard', 'assnigger', 'slanteye', 'dog-fucker', 'pube', 'cockmongler', 'cumbubble', 'jerk-off', 'yeasty', 'assfaces', 'assmonkey', 'beeyotch', 'fuckin', 'piss', 'jack-off', 'fuker', 'mr hands', 'shag', 'ball sucking', 'sadism', 'motherfuck', 'penetrate', 'crack', 'homey', 'twathead', 'd1ck', 'kock', 'tranny', 'slutdumper', 'fistfucker', 'weewee', 'd0ng', 'mothafucker', 'corp whore', 'bescumber', 'menstruate', 'willies', 'dolcett', 'nobjocky', 'pisspig', 'hardcore', 'honkey', 'dummy', 'titi', 'hoar', 'dickfuck', 'bootie', 'fuckers', 'jail bait', 'c0ck', 's0b', 'shaggin', 'cumdumpster', 'stiffy', 'analprobe', 'poopchute', 'goodpoop', 'nsfw images', 'penispuffer', 'molest', 'whorebag', 'bellend', 'hookah', 'snuff', 'transsexual', 'fuc', 'fistfucks', 'punkass', 'rubbish', 'bull shit', 'big black', 'peckerhead', 'sluts', 'shitblimp', 'jism', 'asswhole', 'venus mound', 'scrote', 'guido', 'fagg', 'crap', 'n1gger', 'nutsack', 'coccydynia', 'heshe', 'titt', 'masterbat3', 'teabagging', 'pissing', 'brunette action', 'whoreface', 'booby', 'iap', 'ugly', 'pantie', 'hot chick', 'fuck yo mama', 'ass hole', 'shitcanned', 'fucka', 'phuks', 'bust a load', 'pegging', 'fuck-tard', 'shitcunt', 'frigg', 'sumofabiatch', 'hooter', 'semen', 'tittyfuck', 'douch3', 'fatass', 'doggy-style', 'tongue in a', 'orgasim', 'b17ch', 'meth', 'fingerfucking', 'spiks', 'dicktickler', 'orgies', 'donkeyribber', 'assfukka', 'hand job', 'hoe', 'dumbfuck', 'queer', 'buttcheeks', 'quicky', 'bung', 'bumblefuck', 'ass-fucker', 'loin', 'queerhole', 'a_s_s', 'shagging', 'deepthroat', 't1t', 'dickweed', 'motherfuckin', 'goddamnit', 'lez', 'foreskin', 'camwhore', 'jackhole', 'assbanged', 'piss-off', 'dirsa', 'l3itch', 'clitty', 'fukkers', 'niggaz', 'ejaculates', 'cyberfuc', 'smegma', 'shamedame', 'godamn', 'cocknugget', 'splooge moose', 'bollok', 'assho1e', 'bitches', 'fucking', 'moo moo foo foo', 'kill', 'tit', 'phuk', 'lovemaking', 'shitbag', 'hemp', 'masterbat*', 'fisted', 'jagoff', 'moolie', 'chocolate rosebuds', 'fucks', 'ejaculation', 'erotism', 'lezzie', 'soused', 'beaver', 'trashy', 'piece of shit', 'massa', 'motherfuckka', 'smartass', 'ninny', 'wh0re', 'porch monkey', 'babeland', 'yellow showers', 'viagra', 'mams', 'blowjobs', 'nobhead', 'c-o-c-k', 'twunter', 'jizz', 'assfucker', 'vjayjay', 'fuck hole', 'fuckersucker', 'dp action', 'knobbing', 'deep throat', 'coffin dodger', 'whoralicious', 'pricks', 'assbanger', 'nazism', 'bdsm', 'cummer', 'fuckstick', 'poof', 'cyberfuckers', 'futanari', 'shitbagger', 'douchey', 's-h-1-t', 'dickwod', 'barenaked', 'cocklump', 'asshat', 'big knockers', 'fagots', 'pissed', 'guro', 'kawk', 'vixen', 'window licker', 'hard core', 'bawdy', 'creampie', 'whore', 'fagging', 'shemale', 'blow me', 'fuck trophy', 'white power', 'dominatrix', 'shitfaced', 'feist', 'flamer', 'girl on top', 'gai', 'dickwhipper', 'c.o.c.k.', 'cocksucking', 'rimjob', 'lolita', 'titwank', 'panooch', 'dike', 'fuckme', 'ass', 'gringo', 'gtfo', 'prostitute', 'yobbo', 'dick', 'missionary position', 'xx', 'git', 'knobed', 'make me come', 'testee', 'bastards', 'slut bucket', 'tea bagging', 'ball sack', 'boooobs', 'scum', 'femdom', 'bitching', 'how to kill', 'strappado', 'puss', 'fcuk', 'leather straight jacket', 'shitbrains', 'cocknose', 'sucking', 'rosy palm', 'cocksniffer', 'nig nog', 'pussylicking', 'boozer', 'sandbar', 'teets', 'camgirl', 'jackoff', 'cockshit', 'nut butter', 'schlong', 'renob', 'smut', 'dickjuice', 'fudgepacker', 'testical', 'buttfucka', 'wiseasses', 'retarded', 'weed', 'bullshitted', 'cacafuego', 'fag', 'darn', 'rum', 'fisty', 'd0uche', 'phallic', 'doggiestyle', 'scissoring', 'shibari', 'wog', 'sausage queen', 'gooks', 'phukking', 'ruski', 'camslut', 'kondum', 'wigger', 'panty', 'mothafucka', 'gaywad', 'gay', 'butt plug', 'tities', 'ejaculating', 'chinky', 'whorehouse', 'fagged', 'homoey', 'fuck buttons', 'boob', 'twunt', 'fucktard', 'spook', 'm45terbate', 'jiz', 'queero', 'thundercunt', 'fcuking', 'pissers', 'pissoff', 'pust', 'cock snot', 'lesbians', 'rapey', 'clover clamps', 'raped', 'ho', 'one cup two girls', 'jerk', 'niggle', 'he-she', 'rectum', 'girl on', 'bitchin', 'goddamned', 'frigga', 'anal impaler', 'knobend', 'coon', 'bitched', 'gigolo', 'fukwit', 'niglet', 'dickslap', 'gay sex', 'punani', 'fagbag', 'sperm', 'reverse cowgirl', 'muff', 'coksucka', 'puto', 'ballbag', 'buttplug', 'cocksuck', 'cuntlicker', 'rectal', 'scrog', 'fagtard', 'felch', 'lust', 'cuntsicle', 'zoophilia', 'dyke', 'butt fuck', 'cockeye', 'dickbag', 'ass fuck', 'cumdump', 'areola', 'dildo', 'dick hole', 'goldenshower', 'climax', 'p.u.s.s.y.', 'choc ice', 'lmfao', 'asslick', 'cuntlick', 'sleaze', 'sadist', 'seduce', 'gangbanged', 'bukkake', 'fuck off', 'chode', 'bullshit', 'uzi', 'racy', 'fuckedup', 'heeb', 'shiz', 'orgasmic', 'vag', 'big tits', 'whiz', 'phukked', 'male squirting', 'skeet', 'd1ldo', 'masturbating', 'sanger', 'leper', 'cok', 'shiteater', 'cunt hair', 'dickmilk', 'damned', 'mothafuckers', 'clit licker', 'darkie', 'herpes', 'spic', 'dick shy', 'gonad', 'muff puff', 'wank', 'foad', 'caca', 'fuckwhit', 'nipple', 'yaoi', 'kum', 'spread legs', 'blow your load', 'pusse', 'paki', 'poopuncher', 'opiate', 'damnit', 'cocksuckers', 'violet wand', 'beardedclam', 'cuntrag', 'shitt', 'fleshflute', 'extasy', 'ninnyhammer', 'anus', 'crackwhore', 'dickwad', 'cl1t', 'homoerotic', 'how to murder', 'nambla', 'pole smoker', 'slag', 'len', 'bollock', 'dumbcunt', 'balls', 'swinger', 'ham flap', 'mutherfucker', 'shittings', 'cum dumpster', 'urinal', 'carpetmuncher', 'pissflaps', 'va-j-j', 'nimrod', 'buncombe', 'twats', 'cockbite', 'phonesex', 'a$$', 'dumbass', 'ecchi', 'asscock', 'yiffy', 'organ', 'potty', 'ball gag', 'booze', 'mothafucked', 'sh!+', 'fukker', 'paedophile', 'bugger', 'coons', '5h1t', 'masochist', 'kike', 'bitchy', 'black cock', 'clitoris', 'whored', 'weenie', 'wad', 'coochy', 'grope', 'cums', 'nawashi', 'double penetration', 'teez', 'd1ld0', 'fucker', 'dickish', 'bangbros', 'blue waffle', 'mothafuck', 'cop some wood', 'doochbag', 'fingerfuck', 'brown showers', 'spick', 'motherfucked', 'assbandit', 'mof0', 'chota bags', 'chick with a dick', 'bullet vibe', 'circlejerk', 'faggot', 'assbang', 'rape', 'scrotum', 'nigger', 'cyberfuck', 'muthafuckker', 'boozy', 'murder', 'gays', 'undressing', 'fucktwat', 'strip club', 'spooge', 'screwed', 'vomit', 'cyberfucker', 'cunthunter', 'cocaine', 'sniper', 'gonads', 'cockass', 'jiggaboo', 'yid', 'nude', 'asscracker', 'japs', 'cumtart', 'tainted love', 'bbw', 'fuckbutter', 'hooters', 'masturbate', 'butt', 'dumbasses', 'jap', 'sodom', 'dickheads', 's hit', 'douche-fag', 'v14gra', 'fingerfuckers', 'asssucker', 'faggots', 'fuckmeat', 'fartknocker', 'essohbee', 'nobjokey', 'weirdo', 'donkeypunch', 'xrated', 'vorarephilia', 'arrse', 'ball gravy', 'fingerfucks', 'muther', 'pedophilia', 'bastinado', 'fuckboy', 'dirty pillows', 'gayfuck', 'nig-nog', 'fannyfucker', 'bastardo', 'mutha', 'twatlips', 'smutty', 'assface', 'fondle', 'hymen', 'nimphomania', 'shota', 'taste my', 'penetration', 'prig', 'cocks', 'dink', 'fuckhead', 'fuck-bitch', 'f_u_c_k', 'shitheads', 'bestial', 'crikey', 'pussy', 'rumprammer', 'wop', 'dumbshit', 'cockface', 'facial', 'nipples', 'testis', 'beaver lips', 'dingleberries', 'lmao', 'bodily', 'gassy ass', 'gangbang', 'jizm', 'uterus', 'ovum', 'vulgar', 'bloodclaat', 'nob jokey', 'gayass', 'c0cksucker', 'nut sack', 'knobjocky', 'm-fucking', 'dirty', 'cumslut', 'cnut', 'shitass', 'fagfucker', 'clitty litter', 'kinkster', 'taking the piss', 'mothafuckaz', 'cunts', 'cunnie', 'muffdiving', 'fistfuckers', 'raging boner', 'hot carl', 'milf', 'goddam', 'knobhead', 'frenchify', 'panties', 'cock-sucker', 'fart', 'penial', 'fingerbang', 'bulldyke', 'shitter', 'herpy', 'fuckhole', 'pubes', 'godamnit', 'sissy', 'cockmunch', 'cock sucker', 'pedobear', 'twat', 'looney', 'eat hair pie', 'giant cock', 'sex', 'foot fetish', 'whores', 'b00bs', 'sexy', 'assmuncher', 'motherfuckings', 'areole', 'r-tard', 'axwound', 'shits', 'boobs', 'wetback', 'hootch', 'middle finger', 'condom', 'gooch', 'vagina', 'fice', 'muff diver', 'son of a bitch', 'cuntass', 's&m', 'mcfagget', 'niggah', 'junky', 'dickdipper', 'arsehole', 'b1tch', 'beastiality', 'fistfucked', 'jerkass', 'cunillingus', 'cunny', 'pornography', 'assjacker', 'reetard', 'rtard', 'assbite', 'barf', 'beaner', 'g-spot', 'fuck you', 'rusty trombone', 'urethra play', 'fook', 'fuckwad', 'fuckbutt', 'weiner', 'lemon party', 'sodomize', 'numbnuts', 'anal', 'god', 't1tt1e5', 'fannybandit', 'gayfuckist', 'pansy', 'bigtits', 'coprolagnia', 'child-fucker', 'sambo', 'choade', 'assmucus', 'punky', 'wang', 'two fingers with tongue', 'goregasm', 'fack', 'nigaboo', 'dammit', 'strap on', 'ball kicking', 'pussys', 'huge fat', 'blow mud', 'dumshit', 'coochie', 'sultry women', 'knobjokey', 'douchebags', 'flange', 'gae', 'fooker', 'fucknugget', 'bitchass', 'titty', 'lech', 'pollock', 'strapon', 'loins', 'dildos', 'dipshit', 'opium', 'lameass', 'shited', 'testicle', 'microphallus', 'niggers', 'tampon', 'coonnass', 'prickteaser', 'pee', 'unwed', 'how to murdep', 'sh!t', 'cockknocker', 'doofus', 'titfuck', 'penisfucker', 'diddle', 'auto erotic', 'lusty', 'cum chugger', 'punanny', 'fukwhit', 'dong', 'dinks', 'goo girl', 'beastial', 'cameltoe', 'sod off', 'kummer', 'bi+ch', 'chodes', 'ovary', 'chink', 'minger', 'pussy fart', 'corksucker', 'voyeur', 'bender', 'fenian', 'style doggy', 'dicksipper', 'tawdry', 'orgasms', 'old bag', 'poonany', 'queers', 'shithouse', 'mothafucks', 'shitspitter', 's.h.i.t.', 'slave', 'punany', 'xxx', 'dommes', 'shithole', 'escort', 'suckass', 'rectus', 'commie', 'feck', 'holy shit', 'tubgirl', 'kafir', 'goddamn', 'gaylord', 'omg', 'doggie style', 'bitchers', 'sexo', \"bang (one's) box\", 'foobar', 'cyberfucking', 'shitbreath', 'cracker', 'humped', 'bestiality', 'kumming', 'nymphomania', 'tight white', 'polack', 'cretin', 'piss off', 'cokmuncher', 'fistfucking', 'shitface', 'shirt lifter', 'blowjob', 'double dong', 'fingerfucker', 'dvda', 'h0m0', 'choad', 'dicksucker', 'bosomy', 'shitting', 'beotch', 'nudity', 'inbred', 'shithead', 'gash', 'jigaboo', 'bummer', 'vulva', 'cockblock', 'goatse', 'd0uch3', 'phuking', 'bunny fucker', 's-o-b', 'boned', 'asswad', 'muthafecker', 'azazel', 'shitters', 'nigg4h', 'jerked', 'pissin', 'souse', 'santorum', 'sh1t', 'a2m', 'slut', 'fellate', 'clitfuck', 'anal leakage', 'crappy', 'cunthole', 'dirty sanchez', 'bitchtits', 'booger', 'motherfucka', 'dickmonger', 'shiting', 'bitch tit', 'maxi', 'f.u.c.k', 'hooker', 'nob', 'cockfucker', 'cockjockey', 'virgin', 'butt-pirate', 'mofo', 'shitted', 'scrot', 'pikey', 'cripple', 'pissed off', 'jerkoff', 'doosh', 'snowballing', 'menage a trois', 'orally', 'leather restraint', 'masturbation', 'godsdamn', 'jelly donut', 'dlck', 'dickweasel', 'pubis', 'fannyflaps', 'fagot', 'anilingus', 'horniest', 'motherfucker', 'clitface', 'dawgie-style', 'kootch', 'piss pig', 'masterbate', 'ma5terbate', 'napalm', 'herp', 'moron', 'pedophiliac', 'batty boy', 'alaskan pipeline', 'hebe', 'thug', 'fist fuck', 'hooch', 'cumshots', 'willy', 'barely legal', 'fuckings', 'negro', 'cocksmith', 'god-damned', 'trumped', 'foah', 'cunt-struck', 'fuck', 'throating', 'twatty', 'fuckoff', 'cockmuncher', 'assshole', 'breeder', 'blonde on blonde action', 'bloody hell', 'baby batter', 'arian', 'masterb8', 'kikes', 'dingleberry', 'v1gra', 'spac', 'injun', 'dago', 'a54', 'slutkiss', 'rapist', 'veqtable', 'c-u-n-t', 'cuntbag', 'jiggerboo', 'twinkie', 'hoare', 'ass-jabber', 'sandler', 'tied up', 'cocksucked', 'assbangs', 'bollocks', 'kooch', 'vodka', 'gook', 'kraut', 'reefer', 'cumming', 'shitdick', 'a$$hole', 'cunnilingus', 'prod', 'aeolus', 'cocksucks', 'fuckwitt', 'pinko', 'domination', 'cockmonkey', 'queerbait', 'reich', 'urophilia', 'golden shower', 'jaggi', 'c-0-c-k', 'buttmuch', 'fux', 'gey', 'dick head', 'wrinkled starfish', 'shitty', 'felcher', 'homodumbshit', 'feltch', 'mo-fo', 'fucktards', 'dopey', 'beatch', 'mother fucker', 'shitings', 'chi-chi man', 'menstruation', 'cumjockey', 'fukkin', 'clits', 'sucks', 'mothafuckas', 'date rape', 'iberian slap', 'shitfull', 'a55hole', 'dendrophilia', 'clunge', 'penile', 'poop', 'porn', 'god-dam', 'tit wank', 'tinkle', 'jackasses', 'floozy', 'spik', 'skank', 'snatch', 'chesticle', 'prince albert piercing', 'hardcoresex', 'bumclat', 'mothafuckin', 'eat my ass', 'teat', 'lesbo', 'cockholster', 'fuq', 'fuck-ass', 'assgoblin', 'b!tch', 'shagger', 'cocksukka', 'bimbo', 'ejaculate', 'fanny', 'jizzed', 'pussies', 'wtf', 'hard on', 'dipship', 'smeg', 'duche', 'apeshit', 'gang-bang', 'bareback', 'biatch', 'fux0r', 'bum boy', 'jerk off', 'hitler', 's.o.b.', 'toke', 'doublelift', 'poon', 'drunk', 'tittywank', 'lesbian', 'revue', 'fecal', 'slope', 'fuckbrain', 'spade', 'booobs', 'gspot', 'dingle', 'scroat', 'pillowbiter', 'pisses', 'birdlock', 'towelhead', 'asshole', 'dillweed', 'phuked', 'tittie5', 'tosser', 'busty', 'cockwaffle', 'autoerotic', 'cummin', 'p0rn', 'f-u-c-k', 'cockburger', 'beaners', 'shitstain', 'kinky', 'kunja', 'two fingers', 'golliwog', 'cocksucker', 't1tties', 'brotherfucker', 'tush', 'cockknoker', 'cyberfucked', 'wedgie', 'kinbaku', 'stroke', 'gang bang', 'buttfuck', 'mothafucking', 'dickripper', 'felching', 'nympho', 'dickflipper', 'pedo', 'faig', 'bullshits', 'assfuck', 'dicks', 'cumshot', 'shit', 'cahone', 'zubb', 'fucknutt', 'porno', 'cock pocket', 'assh0le', 'faggotcock', 'erection', 'doggie-style', 'fucktoy', 'arse', '5hit', 'bum', 'fuckass', 'vajayjay', 'munter', 'lardass', 'muffdiver', 'doggy style', 'bullturds', 'doggystyle', 'orgasm', 'splooge', 'shitfuck', 'fucknut', 'fucktart', 'omorashi', 'spunk', 'cuntlicking', 'one guy one jar', 'l3i+ch'}, {'containing', 'seem', 'then', 're', 'merely', 'here', 'theyd', 'others', 'want', 'especially', 'old', 'unless', 'few', 'become', 'tip', 'date', 'predominantly', 'latter', 'refs', 'when', 'after', 'a', 'tried', 'accordingly', 'aside', 'new', 'yourselves', 'forth', 'looking', 'either', 'ah', \"they've\", 'towards', 'somethan', 'makes', 'use', 'ask', 'says', 'doing', 'among', 'beginnings', 'both', 'recent', 'hers', 'try', 'l', 'herself', 'wont', 'but', 'anyhow', 'information', 'specifically', 'asking', 'comes', 'next', 'she', 'needs', 'tends', 'amongst', 'once', 'welcome', 'o', 'off', 'how', 'k', 'can', 'later', 'necessary', 'knows', 'throug', \"it'll\", 'although', 'kg', \"hasn't\", 'were', 'ourselves', \"we'll\", 'other', 'nothing', 'alone', 'specify', 'via', 'home', 'too', 'present', 'relatively', 'less', 'taken', 'thank', 'upon', 'often', 'moreover', 'approximately', 'regards', 'namely', 'ts', 'necessarily', 'through', 'toward', 'somewhat', 'anything', 'un', 'his', 'arise', 'affected', 'along', 'whereas', 'without', 'whomever', 'given', 'cause', 'did', 'please', 'ones', \"can't\", 'those', 'respectively', 'et-al', 'using', 'r', 'got', 'seven', 'trying', 'on', 'results', 'had', 'non', 'ltd', 'shes', 'words', 'which', 'five', 'maybe', 'whose', 'youre', 'becoming', 'and', 'beginning', 'brief', 'sometime', 'particular', 'successfully', 'adj', 'miss', 'h', 'thou', 'com', 'therefore', 'am', 'f', 'put', 'own', 'poorly', 'however', 'all', \"you'll\", 'go', 'immediately', 'seemed', 'part', 'any', 'gone', 'certainly', 'vols', 'further', 'awfully', 'recently', \"didn't\", 'co', 'hereupon', 'every', 'come', 'whereupon', 'having', 'thus', 'n', 'seeming', 'hence', \"don't\", 'below', 'hither', 'selves', 'hereafter', 'everybody', 'over', 'went', 'someone', 'anymore', 'line', 'wasnt', 'about', 'does', 'inward', 'until', 'they', 'm', 'take', 'pages', 'around', 'never', 'whence', 'okay', 'whoever', 'each', \"you've\", \"that'll\", 'because', 'shows', 'sub', 'except', 'their', 'being', 'let', 'nine', 'is', 'what', 'just', 'may', 'yet', 'no', 'or', 'happens', \"who'll\", 'inc', 'with', 'even', 'sorry', 'ff', 'saw', 'where', 'yourself', 'from', 'edu', 'whatever', 'meanwhile', 'th', 'thereupon', 'lest', 'anybody', 'beside', 'instead', 'tell', 'usefulness', 'whod', 'contain', 'between', 'nobody', 'provides', 'somebody', 'became', 'out', 'though', 'at', 'something', 'anyways', 'due', 'lately', 'like', 'since', 'sometimes', 'na', 'keep', 'its', 'past', 'possible', 'that', 'related', 'added', 'liked', 'ok', 'you', 'as', 'lets', 'used', 'furthermore', 'almost', 'done', 'found', 'my', 'thereto', 'research', 'whereby', 'viz', 'sec', 'across', 'near', 'likely', 'anywhere', 'overall', 'world', 'mainly', 'affects', 'must', 'heres', 'hereby', 'need', 'slightly', 'thru', 'gave', 'ran', 'him', 'primarily', 'your', 'www', 'down', 'kept', 'former', 'took', 'unlikely', 'thereof', 'omitted', 'hid', 'announce', 'during', 'gotten', 'usually', 'hed', 'ed', 'beyond', 'this', \"we've\", 'obtained', 'readily', \"haven't\", 'thence', 'sure', 'ups', 'run', 'fifth', 'not', 'plus', 'means', 'important', 'noted', 'affecting', 'self', 'importance', 'giving', 'seeing', \"isn't\", 'by', 'otherwise', 'another', 'ought', 'similar', 'nevertheless', 'whom', 'g', 'getting', 'whither', 'seen', 'p', 'obtain', 'abst', 'owing', 'say', 'd', 'end', 'unfortunately', 'itself', 'sufficiently', 'anyway', 'else', 'so', 'thoughh', 'v', 'again', 'up', 'unlike', 'in', 'act', 'themselves', 'several', 'nowhere', 'it', 't', 'usefully', 'thousand', 'meantime', 'id', 'willing', 'nd', 'much', 'regardless', 'keeps', 'til', 'wed', 'u', 'if', 'noone', 'possibly', 'causes', 'wherein', 'proud', 'look', 'first', 'himself', 'throughout', 'index', 'wouldnt', 'enough', 'he', 'our', 'ours', 'showed', 'thanx', 'away', 'indeed', 'see', \"doesn't\", 'suggest', 'beforehand', 'why', 'everything', 'similarly', 'particularly', 'wherever', 'ml', 'within', 'nor', 'seems', 'regarding', 'such', 'whos', 'above', 'km', 'resulting', 'only', 'significantly', 'et', 'herein', 'saying', 'perhaps', 'therere', 'specified', 'placed', 'against', \"'ve\", 'these', 'ref', 'certain', 'e', 'of', 'get', 'elsewhere', \"i'll\", 'ex', 'last', 'followed', 'etc', 'normally', 'made', 'aren', 'invention', 'tries', 'j', 'y', 'somehow', 'unto', 'outside', 'theres', 'begins', 'pp', 'truly', 'have', 'wheres', 'ending', 'mg', 'thereby', 'shall', 'while', 'thats', 'six', 'b', 'nay', 'ord', 'specifying', 'has', 'very', 'her', \"she'll\", 'also', 'werent', 'back', 'i', 'nearly', 'per', 'hes', 'following', 'whereafter', 'known', 'far', 'thanks', 'widely', 'thered', 'eg', 'us', 'significant', 'already', 'w', 'neither', 'same', 'myself', 'thereafter', 'came', 'wish', 'said', 'c', 'way', 's', 'more', 'million', 'than', 'whats', 'value', 'besides', 'goes', 'sent', 'before', 'make', 'oh', 'the', 'accordance', 'immediate', 'zero', 'yours', 'able', 'que', 'ie', 'different', 'couldnt', 'do', 'potentially', 'believe', 'into', \"there'll\", 'we', 'substantially', 'youd', 'mug', 'howbeit', 'none', 'section', \"shouldn't\", 'should', 'soon', 'quite', 'shown', 'whim', 'think', 'nonetheless', 'together', 'whole', 'would', \"there've\", 'probably', 'been', 'now', 'auth', 'some', 'mrs', 'still', 'fix', 'nos', 'many', 'know', 'hi', 'for', 'vs', 'mean', 'quickly', 'promptly', 'yes', 'always', 'hundred', 'shed', 'behind', 'apparently', \"'ll\", 'rather', 'stop', 'vol', 'begin', 'wants', 'least', 'showns', 'available', 'right', 'eighty', 'uses', 'page', 'name', \"they'll\", 'theyre', 'formerly', 'various', 'afterwards', 'whenever', 'qv', 'are', 'effect', 'mr', 'most', 'becomes', 'little', 'resulted', 'strongly', 'briefly', 'me', 'to', 'whether', 'taking', 'onto', 'could', 'largely', 'im', 'arent', 'looks', 'eight', 'biol', 'useful', 'rd', 'was', 'there', 'might', 'everyone', 'mostly', 'two', 'be', 'ever', 'gives', \"i've\", 'them', 'gets', 'under', 'anyone', 'everywhere', 'downwards', 'obviously', 'itd', 'give', 'who', 'one', 'twice', 'theirs', 'an', 'show', 'previously', 'really', 'latterly', 'contains', 'according', 'follows', 'four', 'therein', 'hardly', 'ninety', 'z', 'somewhere', 'sup', 'cannot', \"what'll\", 'ca', 'q', 'x', 'actually', \"that've\"}]\n"
     ]
    }
   ],
   "source": [
    "# https://blog.cambridgespark.com/50-free-machine-learning-datasets-sentiment-analysis-b9388f79c124\n",
    "\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/feature_extraction.html#stop-words\n",
    "# https://www.aclweb.org/anthology/W18-2502/\n",
    "def stopwords():\n",
    "    import os\n",
    "    root_path = f\"/home/halpdesk/CODE/reddit-parser\"\n",
    "    filename = f\"{root_path}/datasets/ranksnl_large.csv\"\n",
    "    stop_words = []\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            for word in line.split(\",\"):\n",
    "                stop_words.append(word.lower().strip())\n",
    "    return set(stop_words)\n",
    "\n",
    "# Not the same ignore tokens as in nb-features - because we want to check for smilies here\n",
    "def ignoretokens():\n",
    "    common_fractals = [\"1/2\", \"1/3\", \"1/4\"]\n",
    "    low_numbers = [str(int) for int in range(0,10)]\n",
    "    mid_numbers = [str(int) for int in range(10,100,10)]\n",
    "    high_numbers = [str(int) for int in range(100,100100,100)]\n",
    "    tokens = [',', '.', '\"', '``', \"''\", '`', '*', '_', \"&\", \"$\", \"!\", \"#\", \"%\", \"'\", \"”\", \"“\", \"’\", \"‘\", \"―\", \"—\", \"~\", \"–\", \"/\", \"  \", \"   \", \"    \", \"\\n\", \"\\t\", \"\\r\\n\", \"\\r\", \"\t\", \"?\"] + low_numbers + mid_numbers + high_numbers\n",
    "    return tokens\n",
    "\n",
    "def badwords():\n",
    "    import os\n",
    "    root_path = f\"/home/halpdesk/CODE/reddit-parser\"\n",
    "    filename = f\"{root_path}/datasets/fb-bad-words.csv\"\n",
    "    bad_words = []\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            for word in line.split(\",\"):\n",
    "                bad_words.append(word.lower().strip())\n",
    "    return set(bad_words)\n",
    "\n",
    "\n",
    "def commonwords():\n",
    "    import os\n",
    "    root_path = f\"/home/halpdesk/CODE/reddit-parser\"\n",
    "    filename = f\"{root_path}/datasets/common-words.csv\"\n",
    "    common_words = []\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            for word in line.split(\",\"):\n",
    "                common_words.append(word.lower().strip())\n",
    "    return set(common_words)\n",
    "\n",
    "def names():\n",
    "    import os\n",
    "    root_path = f\"/home/halpdesk/CODE/reddit-parser\"\n",
    "    filename = f\"{root_path}/datasets/first_names_all.csv\"\n",
    "    first_names = []\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            for word in line.split(\",\"):\n",
    "                first_names.append(word.strip())\n",
    "    filename = f\"{root_path}/datasets/last_names_all.csv\"\n",
    "    last_names = []\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            for word in line.split(\",\"):\n",
    "                last_names.append(word.strip())\n",
    "        \n",
    "    return set(first_names + last_names)\n",
    "\n",
    "# From https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0144296:\n",
    "# http://kt.ijs.si/data/Emoji_sentiment_ranking/\n",
    "# Other: https://research.utwente.nl/files/5482763/sac13-senticon.pdf\n",
    "# http://emojitracker.com/\n",
    "def emoticons():\n",
    "    import os\n",
    "    root_path = f\"/home/halpdesk/CODE/reddit-parser\"\n",
    "    filename = f\"{root_path}/datasets/emoticons.csv\"\n",
    "    emoticons = {\n",
    "        \"pos\": [\":)\", \":))\", \":D\", \":DD\", \"xD\", \"xDD\", \":d\" \"=)\", \"=))\", \":')\", \"=')\", \":}\", \":}}\", \":]\", \":]]\", \"(:\", \"C:\", \":P\"],\n",
    "        \"neu\": [\":|\", \":/\", \":\\\\\", \"\"],\n",
    "        \"neg\": [\":(\", \":((\", \":(((\", \":((((\", \":C\", \":CC\", \":'C\", \"=(\", \"=((\", \":'(\", \"='(\", \":[\", \":{\", \"):\"]\n",
    "    }\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            word = line.split(\",\")\n",
    "            smil = word[0]\n",
    "            #unicodesmil = chr(int(word[1], 0))\n",
    "            neg = float(word[3])\n",
    "            neu = float(word[4])\n",
    "            pos = float(word[5])\n",
    "            if neu > pos and neu > neg:\n",
    "                emoticons[\"neu\"].append(smil)\n",
    "            elif pos > neg:\n",
    "                emoticons[\"pos\"].append(smil)\n",
    "                #emoticons[\"pos\"].append(unicodesmil)\n",
    "            else:\n",
    "                emoticons[\"neg\"].append(smil)\n",
    "    return emoticons\n",
    "\n",
    "print([emoticons(), badwords(), stopwords()])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "                                           submission  \\\n0   The author Tolkiens son has released a new fan...   \n1   The author Tolkiens son has released a new fan...   \n2   The author Tolkiens son has released a new fan...   \n3   The author Tolkiens son has released a new fan...   \n4   The author Tolkiens son has released a new fan...   \n5   The author Tolkiens son has released a new fan...   \n6   The author Tolkiens son has released a new fan...   \n7   The author Tolkiens son has released a new fan...   \n8   The author Tolkiens son has released a new fan...   \n9   The author Tolkiens son has released a new fan...   \n10  The author Tolkiens son has released a new fan...   \n11  The author Tolkiens son has released a new fan...   \n12  The author Tolkiens son has released a new fan...   \n13  The author Tolkiens son has released a new fan...   \n14  The author Tolkiens son has released a new fan...   \n15  The author Tolkiens son has released a new fan...   \n16  The author Tolkiens son has released a new fan...   \n17  The author Tolkiens son has released a new fan...   \n18  The author Tolkiens son has released a new fan...   \n\n                                                 body  \n0   Hey Tommy the blue, this is a great book for y...  \n1   Awesome books! Awesome author! Where did you g...  \n2   I hate this author :C he is really bad he can ...  \n3   No way! The blue wizards finally got a book?? ...  \n4   Fantasy is boring :/. I'd rather dive into som...  \n5    :D:D YEY!! *amazing* werent they supposed to be?  \n6               this is an old bag with herpes book 👎  \n7                             this wook I like 💚💚💚 !!  \n8                              marry this book I will  \n9   if you are going to buy this book, make sure y...  \n10                             how much does it cost?  \n11                                      I can pay $$$  \n12                      BLUE. JUNO. 100% sh!t http://  \n13          loooooooooooooooooooooooooooool trolololo  \n14  The authors son has released a new fantasy ser...  \n15                                author, blue wizard  \n16  Andrei svensson Daniel, Laura Marx Engels, Jon...  \n17  marry fantasy blue Nietzsche awsome Tommy trol...  \n18                           https://asoftmurmur.com/  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>submission</th>\n      <th>body</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>Hey Tommy the blue, this is a great book for y...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>Awesome books! Awesome author! Where did you g...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>I hate this author :C he is really bad he can ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>No way! The blue wizards finally got a book?? ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>Fantasy is boring :/. I'd rather dive into som...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>:D:D YEY!! *amazing* werent they supposed to be?</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>this is an old bag with herpes book 👎</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>this wook I like 💚💚💚 !!</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>marry this book I will</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>if you are going to buy this book, make sure y...</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>how much does it cost?</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>I can pay $$$</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>BLUE. JUNO. 100% sh!t http://</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>loooooooooooooooooooooooooooool trolololo</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>The authors son has released a new fantasy ser...</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>author, blue wizard</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>Andrei svensson Daniel, Laura Marx Engels, Jon...</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>marry fantasy blue Nietzsche awsome Tommy trol...</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>https://asoftmurmur.com/</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "import pandas as pd\n",
    "submission = \"The author Tolkiens son has released a new fantasy series! The first book is about the Blue wizards! OMG YOU HAVE TO BUY :D:D\"\n",
    "sentences = [\n",
    "    \"Hey Tommy the blue, this is a great book for you 😁💚\",\n",
    "    \"Awesome books! Awesome author! Where did you get it? :)\",\n",
    "    \"I hate this author :C he is really bad he can go and ugly himself ugly\",\n",
    "    \"No way! The blue wizards finally got a book?? OMG I have to buy it xD 👻\",\n",
    "    \"Fantasy is boring :/. I'd rather dive into some Nietzsche and I think everyone should\",\n",
    "    \":D:D YEY!! *amazing* werent they supposed to be?\",\n",
    "    \"this is an old bag with herpes book 👎\",\n",
    "    \"this wook I like 💚💚💚 !!\",\n",
    "    \"marry this book I will\",\n",
    "    \"if you are going to buy this book, make sure you get it at towns hall because they have extra t-shirts!\",\n",
    "    \"how much does it cost?\",\n",
    "    \"I can pay $$$\",\n",
    "    \"BLUE. JUNO. 100% sh!t http://\",\n",
    "    \"loooooooooooooooooooooooooooool trolololo\",\n",
    "    \"The authors son has released a new fantasy series! The first book is about the Blue wizards! OMG YOU HAVE TO BUY :D:D\",\n",
    "    \"author, blue wizard\",\n",
    "    \"Andrei svensson Daniel, Laura Marx Engels, Jonas!, Carl Sagan, Charlie Chapplin$, Walt Disney%, Frant Sinatra\",\n",
    "    \"marry fantasy blue Nietzsche awsome Tommy trolololo\",\n",
    "    \"https://asoftmurmur.com/\",\n",
    "]\n",
    "df = pd.DataFrame({\"submission\": [submission for i in range(len(sentences))], \"body\": sentences})\n",
    "display(df)"
   ]
  },
  {
   "source": [
    "### Links \n",
    "- https://www.researchgate.net/publication/221397355_Complex_Linguistic_Features_for_Text_Classification_A_Comprehensive_Study"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['!!', '$$$', '*amazing*', '100%', ':)', ':/.', ':c', ':d:d', 'a', 'about', 'an', 'and', 'andrei', 'at', 'author', 'author!', 'author,', 'awesome', 'awsome', 'bad', 'bag', 'be', 'be?', 'because', 'blue', 'blue,', 'blue.', 'book', 'book,', 'book??', 'books!', 'boring', 'buy', 'can', 'carl', 'chapplin$,', 'charlie', 'cost?', 'daniel,', 'disney%,', 'dive', 'do', 'engels,', 'everyone', 'extra', 'fantasy', 'finally', 'first', 'for', 'frant', 'get', 'go', 'great', 'hall', 'hate', 'have', 'he', 'herpes', 'hey', 'himself', 'how', 'http://', 'https://asoftmurmur.com/', 'i', \"i'd\", 'if', 'into', 'it', 'it?', 'jonas!,', 'juno.', 'laura', 'like', 'loooooooooooooooooooooooooooool', 'make', 'marry', 'marx', 'much', 'new', 'nietzsche', 'no', 'old', 'omg', 'pay', 'rather', 'really', 'release', 'sagan,', 'series!', 'sh!t', 'should', 'sinatra', 'some', 'son', 'suppose', 'sure', 'svensson', 't-shirts!', 'the', 'they', 'think', 'this', 'to', 'tommy', 'town', 'trolololo', 'ugly', 'walt', 'way!', 'werent', 'where', 'will', 'with', 'wizard', 'wizards!', 'wook', 'xd', 'yey!!', 'you', '👎', '👻', '💚💚💚', '😁💚']\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                           submission  \\\n",
       "0   The author Tolkiens son has released a new fan...   \n",
       "1   The author Tolkiens son has released a new fan...   \n",
       "2   The author Tolkiens son has released a new fan...   \n",
       "3   The author Tolkiens son has released a new fan...   \n",
       "4   The author Tolkiens son has released a new fan...   \n",
       "5   The author Tolkiens son has released a new fan...   \n",
       "6   The author Tolkiens son has released a new fan...   \n",
       "7   The author Tolkiens son has released a new fan...   \n",
       "8   The author Tolkiens son has released a new fan...   \n",
       "9   The author Tolkiens son has released a new fan...   \n",
       "10  The author Tolkiens son has released a new fan...   \n",
       "11  The author Tolkiens son has released a new fan...   \n",
       "12  The author Tolkiens son has released a new fan...   \n",
       "13  The author Tolkiens son has released a new fan...   \n",
       "14  The author Tolkiens son has released a new fan...   \n",
       "15  The author Tolkiens son has released a new fan...   \n",
       "16  The author Tolkiens son has released a new fan...   \n",
       "17  The author Tolkiens son has released a new fan...   \n",
       "18  The author Tolkiens son has released a new fan...   \n",
       "\n",
       "                                                 body  lnk  top-cos-sim  \\\n",
       "0   Hey Tommy the blue, this is a great book for y...    0     0.078705   \n",
       "1   Awesome books! Awesome author! Where did you g...    0     0.000000   \n",
       "2   I hate this author :C he is really bad he can ...    0     0.131485   \n",
       "3   No way! The blue wizards finally got a book?? ...    0     0.150119   \n",
       "4   Fantasy is boring :/. I'd rather dive into som...    0     0.091297   \n",
       "5    :D:D YEY!! *amazing* werent they supposed to be?    0     0.116420   \n",
       "6               this is an old bag with herpes book 👎    0     0.114888   \n",
       "7                             this wook I like 💚💚💚 !!    0     0.000000   \n",
       "8                              marry this book I will    0     0.120616   \n",
       "9   if you are going to buy this book, make sure y...    0     0.089400   \n",
       "10                             how much does it cost?    0     0.000000   \n",
       "11                                      I can pay $$$    0     0.000000   \n",
       "12                      BLUE. JUNO. 100% sh!t http://    1     0.000000   \n",
       "13          loooooooooooooooooooooooooooool trolololo    0     0.000000   \n",
       "14  The authors son has released a new fantasy ser...    0     1.000000   \n",
       "15                                author, blue wizard    0     0.120616   \n",
       "16  Andrei svensson Daniel, Laura Marx Engels, Jon...    0     0.000000   \n",
       "17  marry fantasy blue Nietzsche awsome Tommy trol...    0     0.173237   \n",
       "18                           https://asoftmurmur.com/    1     0.000000   \n",
       "\n",
       "     cos-sim  tfidf-mean  nam  wc  sw  bw  smil+  smil-  smil&  \n",
       "0   0.271516    0.202860    1   6   6   0      2      0      0  \n",
       "1   0.247882    0.235702    0   5   4   0      1      0      0  \n",
       "2   0.228214    0.131782    0   4  10   2      0      1      0  \n",
       "3   0.310975    0.186381    0   7   8   1      1      0      1  \n",
       "4   0.218376    0.162682    1   6   9   0      0      0      1  \n",
       "5   0.256391    0.319034    0   4   3   0      2      0      0  \n",
       "6   0.187867    0.198354    0   3   5   2      0      1      0  \n",
       "7   0.202395    0.288675    0   3   3   0      3      0      0  \n",
       "8   0.180990    0.343476    0   3   2   0      0      0      0  \n",
       "9   0.278572    0.116261    0   6  15   0      0      0      0  \n",
       "10  0.116853    0.200000    0   1   4   0      0      0      0  \n",
       "11  0.165255    0.353553    0   2   2   0      0      0      0  \n",
       "12  0.233705    0.447214    0   4   0   1      0      0      1  \n",
       "13  0.157393    0.705599    0   2   0   0      0      0      0  \n",
       "14  0.333891    0.143164    1  10  12   1      2      0      0  \n",
       "15  0.180990    0.572460    0   3   0   0      0      0      0  \n",
       "16  0.452568    0.258199    5  15   0   0      0      0      0  \n",
       "17  0.274665    0.376349    2   7   0   0      0      0      0  \n",
       "18  0.116853    1.000000    0   1   0   0      0      0      1  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>submission</th>\n      <th>body</th>\n      <th>lnk</th>\n      <th>top-cos-sim</th>\n      <th>cos-sim</th>\n      <th>tfidf-mean</th>\n      <th>nam</th>\n      <th>wc</th>\n      <th>sw</th>\n      <th>bw</th>\n      <th>smil+</th>\n      <th>smil-</th>\n      <th>smil&amp;</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>Hey Tommy the blue, this is a great book for y...</td>\n      <td>0</td>\n      <td>0.078705</td>\n      <td>0.271516</td>\n      <td>0.202860</td>\n      <td>1</td>\n      <td>6</td>\n      <td>6</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>Awesome books! Awesome author! Where did you g...</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>0.247882</td>\n      <td>0.235702</td>\n      <td>0</td>\n      <td>5</td>\n      <td>4</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>I hate this author :C he is really bad he can ...</td>\n      <td>0</td>\n      <td>0.131485</td>\n      <td>0.228214</td>\n      <td>0.131782</td>\n      <td>0</td>\n      <td>4</td>\n      <td>10</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>No way! The blue wizards finally got a book?? ...</td>\n      <td>0</td>\n      <td>0.150119</td>\n      <td>0.310975</td>\n      <td>0.186381</td>\n      <td>0</td>\n      <td>7</td>\n      <td>8</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>Fantasy is boring :/. I'd rather dive into som...</td>\n      <td>0</td>\n      <td>0.091297</td>\n      <td>0.218376</td>\n      <td>0.162682</td>\n      <td>1</td>\n      <td>6</td>\n      <td>9</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>:D:D YEY!! *amazing* werent they supposed to be?</td>\n      <td>0</td>\n      <td>0.116420</td>\n      <td>0.256391</td>\n      <td>0.319034</td>\n      <td>0</td>\n      <td>4</td>\n      <td>3</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>this is an old bag with herpes book 👎</td>\n      <td>0</td>\n      <td>0.114888</td>\n      <td>0.187867</td>\n      <td>0.198354</td>\n      <td>0</td>\n      <td>3</td>\n      <td>5</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>this wook I like 💚💚💚 !!</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>0.202395</td>\n      <td>0.288675</td>\n      <td>0</td>\n      <td>3</td>\n      <td>3</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>marry this book I will</td>\n      <td>0</td>\n      <td>0.120616</td>\n      <td>0.180990</td>\n      <td>0.343476</td>\n      <td>0</td>\n      <td>3</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>if you are going to buy this book, make sure y...</td>\n      <td>0</td>\n      <td>0.089400</td>\n      <td>0.278572</td>\n      <td>0.116261</td>\n      <td>0</td>\n      <td>6</td>\n      <td>15</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>how much does it cost?</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>0.116853</td>\n      <td>0.200000</td>\n      <td>0</td>\n      <td>1</td>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>I can pay $$$</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>0.165255</td>\n      <td>0.353553</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>BLUE. JUNO. 100% sh!t http://</td>\n      <td>1</td>\n      <td>0.000000</td>\n      <td>0.233705</td>\n      <td>0.447214</td>\n      <td>0</td>\n      <td>4</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>loooooooooooooooooooooooooooool trolololo</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>0.157393</td>\n      <td>0.705599</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>The authors son has released a new fantasy ser...</td>\n      <td>0</td>\n      <td>1.000000</td>\n      <td>0.333891</td>\n      <td>0.143164</td>\n      <td>1</td>\n      <td>10</td>\n      <td>12</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>author, blue wizard</td>\n      <td>0</td>\n      <td>0.120616</td>\n      <td>0.180990</td>\n      <td>0.572460</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>Andrei svensson Daniel, Laura Marx Engels, Jon...</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>0.452568</td>\n      <td>0.258199</td>\n      <td>5</td>\n      <td>15</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>marry fantasy blue Nietzsche awsome Tommy trol...</td>\n      <td>0</td>\n      <td>0.173237</td>\n      <td>0.274665</td>\n      <td>0.376349</td>\n      <td>2</td>\n      <td>7</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>https://asoftmurmur.com/</td>\n      <td>1</td>\n      <td>0.000000</td>\n      <td>0.116853</td>\n      <td>1.000000</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel, sigmoid_kernel, rbf_kernel, polynomial_kernel, laplacian_kernel, cosine_similarity\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "\n",
    "class LemmaTokenizer:\n",
    "    ignoretokens = []\n",
    "    stopwords = []\n",
    "    vocab = []\n",
    "    lcase = True\n",
    "    def __init__(self, stopwords=[], vocabulary=[], ignoretokens=[], lcase=True):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "        self.stopwords = stopwords\n",
    "        self.vocab = vocabulary\n",
    "        self.ignoretokens = ignoretokens\n",
    "        self.lcase = lcase\n",
    "    def __call__(self, document):\n",
    "        lemmas = []\n",
    "        tokenized_words = document.split(\" \")\n",
    "        for word, tag in pos_tag(tokenized_words):\n",
    "            lower_cased_tag = tag[0].lower()\n",
    "            stripped_word = word.strip(''.join(self.ignoretokens))\n",
    "            if (word != stripped_word):\n",
    "                #print(f\"{word} -> {stripped_word}\")\n",
    "                pass\n",
    "            wn_tag = lower_cased_tag if lower_cased_tag in ['a', 'r', 'n', 'v'] else None\n",
    "            if not wn_tag:\n",
    "                lemma = stripped_word\n",
    "            else:\n",
    "                lemma = self.wnl.lemmatize(stripped_word, wn_tag)\n",
    "            if lemma not in list(self.stopwords): # and word in self.vocab:\n",
    "                lemmas.append(lemma.lower() if self.lcase else lemma)\n",
    "        return lemmas\n",
    "\n",
    "swords = stopwords()\n",
    "bwords = badwords()\n",
    "cwords = commonwords()\n",
    "ignore = ignoretokens()\n",
    "smil = emoticons()\n",
    "nam = names()\n",
    "\n",
    "#print(cwords)\n",
    "\n",
    "# All vocabs\n",
    "cva = CountVectorizer(lowercase=True, tokenizer=LemmaTokenizer())\n",
    "asdf = cva.fit_transform(df.get(\"body\"))\n",
    "all_vocabs = cva.get_feature_names()\n",
    "print(all_vocabs)\n",
    "\n",
    "# Filtered vocabs\n",
    "cvf = CountVectorizer(lowercase=True, tokenizer=LemmaTokenizer(stopwords=list(swords)+list(bwords)))\n",
    "asdf = cvf.fit_transform(df.get(\"body\"))\n",
    "filtered_vocab = cvf.get_feature_names()\n",
    "\n",
    "# Significant words count\n",
    "cv = CountVectorizer(lowercase=True, tokenizer=LemmaTokenizer(stopwords=list(swords)+list(bwords), ignoretokens=ignore))\n",
    "wc_data = cv.fit_transform(df.get(\"body\"))\n",
    "wc_df = pd.DataFrame(wc_data.sum(axis=1))\n",
    "wc_df.columns = [\"wc\"]\n",
    "\n",
    "# Bad words count\n",
    "cv = CountVectorizer(vocabulary=bwords, stop_words=None, lowercase=True, tokenizer=LemmaTokenizer(), ngram_range=(1,2)) # finds \"old bag\"\n",
    "bw_data = cv.fit_transform(df.get(\"body\"))\n",
    "bw_df = pd.DataFrame(bw_data.sum(axis=1))\n",
    "bw_df.columns = [\"bw\"]\n",
    "\n",
    "# Stop words count\n",
    "cv = CountVectorizer(vocabulary=swords, stop_words=None, lowercase=True, tokenizer=LemmaTokenizer())\n",
    "sw_data = cv.fit_transform(df.get(\"body\"))\n",
    "sw_df = pd.DataFrame(sw_data.sum(axis=1))\n",
    "sw_df.columns = [\"sw\"]\n",
    "\n",
    "# Names count\n",
    "cv = CountVectorizer(vocabulary=nam, stop_words=None, lowercase=False, tokenizer=LemmaTokenizer(lcase=False))\n",
    "nam_data = cv.fit_transform(df.get(\"body\"))\n",
    "nam_df = pd.DataFrame(nam_data.sum(axis=1))\n",
    "nam_df.columns = [\"nam\"]\n",
    "\n",
    "# Positive smilies\n",
    "cv = CountVectorizer(vocabulary=smil[\"pos\"], analyzer=\"char\", ngram_range=(1,2), stop_words=None, lowercase=False) # char + 2 ngram = \":D\"\n",
    "smilp_data = cv.fit_transform(df.get(\"body\"))\n",
    "smilp_df = pd.DataFrame(smilp_data.sum(axis=1))\n",
    "smilp_df.columns = [\"smil+\"]\n",
    "\n",
    "# Negative smilies count\n",
    "cv = CountVectorizer(vocabulary=smil[\"neg\"], analyzer=\"char\", ngram_range=(1,2), stop_words=None, lowercase=False) # char + 2 ngram = \":D\"\n",
    "sniln_data = cv.fit_transform(df.get(\"body\"))\n",
    "smiln_df = pd.DataFrame(sniln_data.sum(axis=1))\n",
    "smiln_df.columns = [\"smil-\"]\n",
    "\n",
    "# Neutral smilies count\n",
    "cv = CountVectorizer(vocabulary=smil[\"neu\"], analyzer=\"char\", ngram_range=(1,2), stop_words=None, lowercase=False) # char + 2 ngram = \":D\"\n",
    "smile_data = cv.fit_transform(df.get(\"body\"))\n",
    "smile_df = pd.DataFrame(smile_data.sum(axis=1))\n",
    "smile_df.columns = [\"smil&\"]\n",
    "\n",
    "# TF-IDF cosine similarity toawrd topic\n",
    "submission_with_comments = [submission] + list(df.get(\"body\").array)\n",
    "tfidfv = TfidfVectorizer(vocabulary=filtered_vocab, lowercase=True, ngram_range=(1,1), smooth_idf=True, tokenizer=LemmaTokenizer(stopwords=list(swords)))\n",
    "tfidf_data = tfidfv.fit_transform(submission_with_comments)\n",
    "cosine_similarities = cosine_similarity(tfidf_data[0:1], tfidf_data[1:]).flatten()\n",
    "top_simi_df = pd.DataFrame(cosine_similarities)\n",
    "top_simi_df.columns = [\"top-cos-sim\"]\n",
    "\n",
    "# TF-IDF cosine similarity towards all documents\n",
    "# (possible to do just as toward topic by changing first element from submission to \" \".join(all_vocabs))\n",
    "submission_with_comments = [\" \".join(all_vocabs)] + list(df.get(\"body\").array)\n",
    "tfidfv = TfidfVectorizer(vocabulary=filtered_vocab, lowercase=True, ngram_range=(1,1), smooth_idf=True, tokenizer=LemmaTokenizer(stopwords=list(swords)))\n",
    "tfidf_data = tfidfv.fit_transform(submission_with_comments)\n",
    "cosine_similarities = cosine_similarity(tfidf_data[0:1], tfidf_data[1:]).flatten()\n",
    "all_simi_df = pd.DataFrame(cosine_similarities)\n",
    "all_simi_df.columns = [\"cos-sim\"]\n",
    "\n",
    "#tfidfv = TfidfVectorizer(vocabulary=all_vocabs, lowercase=True, ngram_range=(1,1), smooth_idf=True)\n",
    "#tfidf_data = tfidfv.fit_transform(df.get(\"body\"))\n",
    "#cosine_similarities = cosine_similarity(tfidf_data, tfidf_data)\n",
    "#cosine_similarities[cosine_similarities == 0] = np.nan\n",
    "#cosine_similarities = np.nanmean(cosine_similarities, axis=1)\n",
    "#all_simi_df = pd.DataFrame(cosine_similarities)\n",
    "#all_simi_df.columns = [\"cos-sim\"]\n",
    "\n",
    "# TF-IDF mean value (checks across all documents)\n",
    "tfidfv = TfidfVectorizer(vocabulary=all_vocabs, lowercase=True, ngram_range=(1,1), smooth_idf=True, tokenizer=LemmaTokenizer(stopwords=list(swords)))\n",
    "tfidf_data = tfidfv.fit_transform(df.get(\"body\")).todense()\n",
    "means = [0]*tfidf_data.shape[0]\n",
    "#means = np.nanmean(tfidf_data, axis=1)\n",
    "for i in range(0, tfidf_data.shape[0]):\n",
    "    word_count = wc_df.get(\"wc\")[i] + bw_df.get(\"bw\")[i] + sw_df.get(\"sw\")[i]\n",
    "    means[i] = tfidf_data[i].sum()/word_count if word_count > 0 else 0\n",
    "    #tfidf_data[i][tfidf_data[i] == 0] = np.nan\n",
    "tfidf_df = pd.DataFrame(means)\n",
    "tfidf_df.columns = [\"tfidf-mean\"]\n",
    "\n",
    "# Has link\n",
    "df['lnk'] = [1 if \"http\" in row[['body']].to_string() else 0 for i,row in df.iterrows()]\n",
    "cv_dataframe = pd.concat([df, top_simi_df, all_simi_df, tfidf_df, nam_df, wc_df, sw_df, bw_df, smilp_df, smiln_df, smile_df], axis=1)\n",
    "cv_dataframe"
   ]
  }
 ]
}