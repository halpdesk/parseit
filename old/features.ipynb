{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('python_modules')",
   "metadata": {
    "interpreter": {
     "hash": "9634d4b890a726f71d8044046bc71cdc391c406dc663847f104c7db04bcb4cdc"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[{'pos': [':)', ':))', ':D', ':DD', 'xD', 'xDD', ':d=)', '=))', \":')\", \"=')\", ':}', ':}}', ':]', ':]]', '(:', 'C:', ':P', '😂', '❤', '♥', '😍', '😘', '😊', '👌', '💕', '👏', '😁', '☺', '♡', '👍', '🙏', '✌', '😏', '😉', '🙌', '🙈', '💪', '😄', '💃', '💖', '😃', '😱', '🎉', '😜', '🌸', '💜', '💙', '😳', '💗', '☀', '😎', '😢', '💋', '😋', '🙊', '🎶', '💞', '😌', '💯', '💛', '💁', '💚', '😆', '😝', '😅', '👊', '😀', '😚', '😻', '💘', '👋', '✋', '🎊', '🍕', '❄', '😥', '😈', '🔝', '⚽', '👑', '😹', '🍃', '🎁', '🐧', '🎈', '✊', '💤', '💓', '💦', '🙋', '🎄', '🎵', '😛', '😬', '👯', '💎', '🎂', '👫', '🏆', '☝', '😙', '⛄', '👅', '♪', '🍂', '💏', '🌴', '👈', '🌹', '🙆', '🍻', '🌞', '🍁', '⭐', '🎀', '🙉', '🌺', '💅', '🐶', '🌚', '🎤', '👭', '🎧', '👆', '🍸', '🍉', '😇', '🏃', '🍺', '🎸', '🍹', '💫', '📚', '🌷', '💝', '💨', '🏈', '💍', '☔', '👸', '🇪', '🍩', '☁', '🌻', '😵', '↿', '🐯', '👼', '🍔', '😸', '👶', '↾', '💐', '🌊', '🍦', '🍓', '💆', '🍴', '🇸', '😮', '😽', '🌈', '🙀', '🎮', '🍆', '🍰', '🙇', '🍟', '🍌', '💑', '🐣', '🎃', '😟', '🐾', '🎓', '🏊', '🍫', '📷', '👄', '🌼', '🐱', '🇺', '🚬', '📖', '🐒', '🌍', '┊', '🐥', '💄', '💸', '⛔', '🏀', '💉', '💟', '😯', '♦', '🌙', '🐟', '👣', '🗿', '🍝', '🍭', '❌', '🐰', '💊', '🚨', '🍪', '🎆', '🎎', '🇩', '✅', '🍑', '🔊', '🌌', '🍎', '🐻', '💇', '🍊', '🍒', '🐭', '👟', '🌎', '🍍', '🐮', '📲', '🌅', '🇷', '👠', '🌽', '🍬', '😺', '🚀', '¦', '🍧', '🍜', '🐏', '👧', '🏄', '🍋', '🆗', '📺', '🍅', '⛅', '👙', '🏡', '🌾', '✏', '🐬', '🇹', '♣', '🇮', '🐍', '♔', '🍳', '🔵', '🌕', '🐨', '🔐', '💿', '🌳', '👰', '⚓', '🚴', '👗', '➕', '💬', '🔜', '🍨', '🍙', '🍗', '🍲', '😼', '🐙', '👨', '🍚', '🍖', '♨', '▃', '🚘', '👩', '🐠', '🚹', '💵', '✰', '👛', '🌱', '🌏', '🌲', '👴', '🏠', '🍇', '🍘', '🍛', '🐇', '👵', '🌵', '🎇', '🐎', '🐤', '🛀', '🌑', '🚲', '🏁', '🎾', '🐵', '◕', '🗼', '🍵', '🍯', '⇨', '🌓', '🔒', '👳', '♩', '💌', '🌜', '🚿', '🔆', '🌛', '🏩', '🇫', '📢', '🐦', '♻', '🌘', '🍐', '🌔', '╥', '👖', '😗', '🐄', '⬇', '🚼', '🌗', '🌖', '🔅', '👜', '🐌', '💼', '🐹', '🌠', '⚫', '♧', '🎢', '🎷', '🌇', '⏰', '◠', '🎿', '🆔', '🌒', '🐪', '╝', '👔', '🐋', '▽', '🐛', '👕', '💳', '🏧', '💡', '⬅', '🇱', '📹', '👞', '🚑', '🆘', '👚', '🚍', '🚣', '🏉', '🗻', '⛺', '🏂', '👡', '📻', '🌰', '🎒', '⌒', '📴', '🚢', '🔔', '◢', '🏥', '🃏', '💒', '🐐', '🔚', '🔓', '🎽', '📅', '🎺', '✉', '◤', '○', '🍼', '📣', '🐗', '⛳', '┛', '┃', '💺', '☻', '📞', '🌉', '✎', '📃', '💷', '🚄', '▲', '⛵', '⌛', '🚜', '👒', '❕', '🔛', '🇲', '❅', '👝', '✞', '🎋', '👥', '◆', '🔭', '🐜', '♌', '👷', '📄', '🚐', '🌋', '📡', '🚳', '✘', '🅰', '🇼', '┓', '┣', 'Ⓛ', 'Ⓔ', '👤', '🎠', '📗', '🔩', '👢', '📰', 'Ⓜ'], 'neu': [':|', ':/', ':\\\\', '', '☯', '✨', '★', '█', '🔥', '♫', '�', '©', '👀', '🐓', '☕', '💥', '►', '✈', '👉', '☆', '🍀', '🎅', '✔', '⚡', '➡', '🌿', '🌟', '🔮', '❗', '✖', '🔪', '➜', '👻', '💰', '▪', '━', '☷', '🐷', '👽', '🍷', '®', '☑', '│', '💣', '▶', '░', '👾', '📒', '👇', '▓', '⚠', '╯', '✓', '▬', '🚶', '║', '🐸', '✿', '🌀', '🐼', '🎥', '●', '🚗', '📝', '═', '💭', '☞', '🌃', '╭', '✧', '╮', '👹', '📱', '🎼', '─', '╰', '♬', '♚', '🔴', '☼', '❓', '🐴', '💢', '🎬', '🐘', '⠀', '➤', '⬆', '⚪', '🐢', '◉', '🍤', '🐝', '🌝', '❁', '❀', '▀', '▒', '💲', '⛽', '▸', '♛', '🎹', '♕', '🍏', '👦', '🇬', '🇧', '☠', '╠', '🚙', '💻', '▄', '👓', '◄', '🔞', '◀', '🔙', '🐽', '➔', '💶', '╩', '🐑', '🍞', '╚', '🈹', '🐳', '✪', '▐', '♠', '🐚', '👂', '🗽', '🆒', '🐺', '➨', '╬', '🌂', '🚌', '🍡', '❥', '🎡', '🐩', '⌚', '🐖', '🐔', '🐲', '❊', '🚺', '◟', '🍢', '🎨', '⛲', '▁', '🇴', '🚕', '🐈', '⇧', '☎', '🌁', '🏰', '🚵', '🎐', '╗', '╱', '⇩', '🚂', '✦', '⛪', '╔', '🔱', '🆓', '▂', '🚋', '🌆', '🔹', '🐫', '🏪', '۩', '🐂', '✳', '🐀', '╦', '🐕', '✒', '🏢', '🚚', '🐉', '❒', '🐊', '🚖', '▼', '☛', '✩', '🚤', '🎻', '🔷', '🚦', '✯', '╣', '📀', '🚛', '📓', '☉', '💴', '┼', '🐃', '🍄', '📕', '🚓', '↪', '👱', 'Ⓐ', '🏨', '♎', '🔸', '🐆', '♢', '◡', '📵', '🐡', '🏯', '☂', '🎪', '↳', '🔈', '📍', '🚔', '⏩', '۞', '☾', '📥', '🔦', '🚁', '🐁', '♂', '◞', '📯', '◂', '📶', '🚥', '🌄', '🗾', '🔶', '🏤', '🎩', '🐅', '♮', '🔄', '☄', '☨'], 'neg': [':(', ':((', ':(((', ':((((', ':C', ':CC', \":'C\", '=(', '=((', \":'(\", \"='(\", ':[', ':{', '):', '😭', '😩', '😒', '😔', '😡', '😴', '🔫', '😞', '😪', '😫', '💀', '😕', '💔', '😤', '😰', '😑', '😠', '😓', '😣', '😐', '😨', '😖', '👎', '😷', '💩', '🙅', '😿', '😲', '😶', '😧', '🚫', '👐', '👬', '￼', '👿', '✂', '👪', '😦', '🍣', '🙍', '🍱', '💧', '🔋', '😾', '🍥', '⚾', '🍮', '👮', '☹', '┳', '👺', '💂', '🙎', '🔨', '🎭', '┈', '🍠', '□', '🏫', '❔', '▌', '■', '🍈', '➰', '🔌', '┻', '⏳', '🏇', '🚩', '📌', '☐', '┐', '☮', '🔧', '🅾']}, {'bescumber', 'cretin', 'ham flap', 'goo girl', 'stoned', 'looney', 'slut bucket', 'kock', 'camslut', 'spread legs', 'cockmongruel', 'negro', 'floozy', 'hell', 'clit licker', 'mick', 'pissoff', 'toke', 'dickweed', 'rtard', 'h0m0', 'stiffy', 'dagos', 'f4nny', 'shiteater', 'auto erotic', 'gringo', 'bumclat', 'cum guzzler', 'mams', 'fucknutt', 'fux', 'shemale', 'cyalis', 'clunge', 'bitch tit', 'bollock', 'sumofabiatch', 'bootie', 'zibbi', 'gai', 'panty', 'dirty', 'sandler', 'sniper', 'g-spot', 'gay sex', 'scrot', 'fuckbutter', 'futanari', 'cock pocket', 'sod off', 'azz', 'vulgar', 'sausage queen', 'gang-bang', 'masterbat*', 'menstruation', 'window licker', 'murder', 'f-u-c-k', 'rapist', 'big knockers', 'scrud', 'assbang', 'pussypounder', 'milf', 'fucktoy', 'queerbait', 'shitings', 'fubar', 'reich', 'pole smoker', 'l3itch', 'dumbcunt', 's hit', 'dopey', 's_h_i_t', 'xxx', 'bunghole', 'toots', 'puss', 'screwed', 'circlejerk', 'fart', 'fuckwhit', 'incest', 'lezza/lesbo', 'humping', 'bampot', 'dirsa', 'cockwaffle', 's&m', 'fisted', 'pansy', 'div', 'fuck yo mama', 'len', 'ecchi', 'shagger', 'flaps', 'fecal', 'mothafucked', 'alabama hot pocket', 'fagg', 'schlong', 'gays', 'horny', 'queaf', 'bitchin', 'assbag', 'cock sucker', 'assbite', 'fistfuck', 'wiseass', 'aeolus', 'faggots', 'doggiestyle', 'fingerfuckers', 'cockholster', 'uterus', 'strip', 'poof', 'stupid', 'whored', 'muthafuckker', 'fagged', 'rimming', 'two girls one cup', 'b00bs', 'smeg', 'middle finger', 'pot', 'lust', 'jail bait', 'v1gra', 'fukker', 'wad', 'bullshitted', 'fuckmeat', 'deepthroat', 'phuking', 'dlck', 'guro', 'rump', 'menses', 'b17ch', 'hoe', 'clitface', 'fuck-tard', 't1t', 'pecker', 'rapey', 'chincs', 'hobag', 'eunuch', 'fuckoff', 'dickfucker', 'fxck', 'jungle bunny', 'god-damned', 'hom0', 'shitbagger', 'feltcher', 'barf', 'dumb ass', 'assmaster', 'scantily', 'herp', 'asshole', 'shitfull', 'chi-chi man', 'hoare', 'pube', 'spooge', 'voyeur', 'beaner', 'buttmunch', 'iberian slap', 'paki', 'hot chick', 'beatch', 'mothafucker', 'assface', 'boners', 'poopchute', 'panties', 'b!tch', 'penetrate', 'flange', 'slag', 'testis', '2 girls 1 cup', 'bareback', 'jack off', 'boiolas', 'pussy fart', 'd0uche', 'nympho', 'dawgie-style', 'punta', 'bollok', 'cus', 'm-fucking', 'style doggy', 'kafir', 'tush', 'genitals', 'knobhead', 'motherfucked', 'douchey', 'rectus', 'slanteye', 'dumshit', 'twinkie', 'mr hands', 'shithead', 'assfucker', 'horniest', 'bloody hell', 'ass-fucker', 'fuks', 'cockeye', 'turd', 'bitches', 'piss off', 'jerk', 'numbnuts', 'asscracker', 'blumpkin', 'nipples', 'chinky', 'shittings', 'how to murder', 'titty', 'lameass', 'yid', 'boozy', 'doggy style', 'leather straight jacket', 'piss-off', 'fuckup', 'hardcoresex', 'sanger', 'fucks', 'whoring', 'bimbos', 'gangbanged', 'lardass', 'ball sucking', 'ballsack', 'masterbations', 'wigger', 'tittiefucker', 'shitcanned', 'dyke', 'cyberfuckers', 'godamnit', 'goregasm', 'shibari', 'dildo', 'fuck-bitch', 'sh1t', 'sultry women', 'tubgirl', 'pissers', 'virgin', 'fuckbrain', 'climax', 'nonce', 'kikes', 'thrust', 'hard core', 'fack', 'sleazy', 'gook', 'cameltoe', 'rimjaw', 'mothafuckas', 'corksucker', 'fagtard', 'tosser', 'whoar', 'fook', 'cumbubble', 'niggle', 'doggin', 'hotsex', 'beastial', 'fuckedup', 'camel toe', 'p0rn', 'dykes', 'masterbate', 'felcher', 'pollock', 'prince albert piercing', 'nawashi', 'tea bagging', 'kummer', 'motherfucking', 'fuck buttons', 'hootch', 'kawk', 'niggas', 'group sex', 'violet wand', 'tramp', 'lez', 'queers', 'pron', 'spook', 'bloody', 'dickbeaters', 'splooge moose', 'beeyotch', 'twathead', 'ejaculatings', 'fucking', 'crack', 'blowjobs', 'shited', 'coon', 'poontang', 'polack', 'prick', 'douche-fag', 'shit', 'phuk', 'shitty', 'masterbation', 'girl on top', 'herpes', 'baby juice', 'dvda', 'foad', 'fistfucks', 'muthafecker', 'scissoring', 'vjayjay', 'veqtable', 'clusterfuck', 'ball licking', 'fvck', 'gippo', 'ass fuck', 'cunnie', 'pussylicking', 'tittywank', 'brown showers', 'fuck you', 'twatty', 'knobbing', 'douchewaffle', 'kum', 'bloodclaat', 'munter', 'cocksmoke', 'chinc', 'cipa', 'cumslut', 'bull shit', 'mutha', 'godamn', 'breasts', 'booty call', 'shamedame', 'octopussy', 'whiz', 'cumstain', 'fukwit', 'master-bate', 'stfu', 'buttfucker', 'grope', 'unclefucker', 'pissed', 'titwank', 'cocksucking', 'undies', 'rectal', 'testicle', 'cockmunch', 'extasy', 'shitter', 'fagots', 'dumbasses', 'ovum', 'd1ldo', 'whoreface', 'trumped', 'urophilia', 'retard', 'goldenshower', 'figging', 'ballbag', 'girls gone wild', 'tities', 'corpulent', 'porchmonkey', 'blowjob', 'areole', 'tight white', 'fuckhole', 'lech', 'motherfuckin', 'homoerotic', 'cumtart', 'nudity', 'titi', 'erection', 'dickhead', 'fagfucker', 'son-of-a-bitch', 'need the dick', 'slut', 'handjob', 'ass-hat', 'pantie', 'penial', 'racy', 'jerkoff', 'yellow showers', 'flamer', 'p.u.s.s.y.', 'hussy', 'masturbating', 'raped', 'cuntass', 'slave', 'tittie5', 'axwound', 'boobs', 'lesbian', 'dink', 'cunt', 'cocksucks', 'cockfucker', 'fuckass', 'suicide girls', 'fannyflaps', 'mofo', 'cocain', 'taste my', 'shit ass', 'fudge packer', 'kunja', 'wetback', 'fudge-packer', 'taking the piss', 'dickweasel', 'wrapping men', 'ho', 'arian', 'cokmuncher', 'butt', 'pussy palace', 'cacafuego', 'carpetmuncher', 'how to kill', 'jiggerboo', 'mutherfucker', 'cut rope', 'fellatio', 'shit fucker', 'doosh', 'fucktart', 'fuc', 'heroin', 'f.u.c.k', 'kinbaku', 'cocknose', 'ma5terbate', 'ejaculate', 'niggah', 'shitblimp', 'goddammit', 'fecker', 'phukking', 'jiz', 'thug', 'dickwod', 'reetard', 'assbandit', 'cop some wood', 'c0cksucker', 'jelly donut', 'cocks', 'seks', 'homoey', 'beastiality', 'cuntface', 'raping', 'fags', 'm45terbate', 'smegma', 'fingering', 'dickish', '5h1t', 'prig', 'asses', 'upskirt', 'fuck trophy', 'ugly', 'dipship', 'urinal', 'nob', 'shitcunt', 'tainted love', 'carpet muncher', 'shitbreath', 'rum', 'dingleberry', 'cockhead', 'vixen', 'phuked', 'microphallus', 'phone sex', 'd1ck', 'dinks', 'jizzed', 'shota', 'nipple', 'bullturds', 'pedo', 'hebe', 'skank', 'femdom', 'shirt lifter', 'dicktickler', 'old bag', 'nymphomania', 'aryan', 'damnit', 'lusting', 'shitfuck', 'organ', 'goddam', 'hymen', 'prude', 'fuk', 'cahone', 'buttfucka', 'intercourse', 'nobjokey', 'shite', 'smartass', 'goodpoop', 'uzi', 'cyberfucker', 'raghead', 'mothafucking', 'hand job', 'hard on', 'cocksuck', 'cyberfuc', 'blow mud', 'psycho', 'birdlock', 'dommes', 'alaskan pipeline', 'hore', 'meth', 'a$$hole', 'japs', '2g1c', 'shag', 'penile', 'heeb', 'hot carl', 'nobjocky', 'mong', 'booty', 'cockbite', 'playboy', 'lemon party', 'semen', 'giant cock', 'choc ice', 'bitchers', 'lovemaking', 'dry hump', 'cox', 'gaysex', 'buttcheeks', 'boozer', 'motherfucka', 'shaggin', 'donkey punch', 'iap', 'cockknoker', 'dookie', 'bum', 'ball gag', 'dick shy', 'wiseasses', 'cockblock', 'buttmuch', 'cuntbag', 'niglet', 'ejaculating', 'mothafucks', 'cock snot', 'sodomize', 'breeder', 'motherfucks', 'massa', 'clit', 'kill', 'clitty litter', 'cocksucker', 'fatass', 'panooch', 's-h-i-t', 'knobead', 'c.o.c.k.', 'sandnigger', 'yeasty', 'fingerfucking', 'wtf', 'd1ld0', 'pinko', 'masterbat3', 'blonde action', 'doggie style', 'bollocks', 'nimphomania', 'weed', 'asssucker', 'pedophiliac', 'shitstain', 'one guy one jar', 'kumming', 'raging boner', 'jizm', 'goddamned', 'fucker', 'creampie', 'ahole', 'bangbros', 'bi+ch', 'fuker', 'spunk', 'sadist', 'dog style', 'cocksniffer', 'napalm', 'bawdy', 'big tits', 'booger', 'dogging', 'tart', 'rosy palm', 'sexy', 'fuckwitt', 'facial', 'spade', 'douche', 'raper', 'cooter', 'pisser', 'deggo', 'cum dumpster', 'naked', 'fuckheads', 'gaytard', 'gooch', 'mafugly', 'cripple', 'dick-sneeze', 'fukkers', 'thundercunt', 'rosy palm and her 5 sisters', 'dickmilk', 'mothafuckin', 'condom', 'crikey', 'smutty', 'dolcett', 'girl on', 'dick-ish', 'spik', 'domination', 'molest', 'shitey', 'twink', 'beaver lips', 'cum freak', 'missionary position', 'gaywad', 'dumass', 'dicksucking', 'dickmonger', 'kyke', 'pubes', 'whore', 'motherfuckers', 'hump', 'baby batter', 'fingerfucked', 'phuks', 'twat', 'penispuffer', 'bosomy', 'ejaculated', 'suck', 'wanker', 'butt fuck', 'sucked', 'anal', 'poop', 'faggit', 'fuckme', 'date rape', 'golden shower', 'fondle', 'crackwhore', 'fucked', 'cocklump', 'dog-fucker', 'masterb8', 'sodomy', 'dirty sanchez', 'batty boy', 'mothafucka', 'tribadism', 'jizz', 'm0fo', 'muther', 'shi+', 'pornography', 'shittiest', 'pikey', 'fleshflute', 'a$$', 'cracker', 'fucknut', 'jerk0ff', 'leper', 'fisting', 'fistfucker', 'bullet vibe', 'ghay', 'neonazi', 'beaners', 'pimpis', 'sh!+', 'faggitt', 'fcuking', 'urethra play', 'dickbag', 'vajayjay', 'brunette action', 'fooker', 'wog', 'escort', 'ma5terb8', 'wang', 'clitfuck', 'soused', 'omorashi', 'zubb', 'bitching', 'faigt', 'jaggi', 'raunch', 'kike', 'dickdipper', 'asslicker', 'ball gravy', 'fellate', 'skullfuck', 'bestial', 'mothafuckers', 'eat a dick', 'orgies', 'ponyplay', 'shitdick', 'jap', 'spac', 'phonesex', 'two fingers with tongue', 'w00se', 'double penetration', 'gender bender', 'teets', 'coprolagnia', 'tranny', 'fuq', 'sucks', 'fuckstick', '4r5e', 'scum', 'pissin', 'wh0re', 'midget', 'gangbangs', 'cyberfuck', 'pawn', 'punky', 'gae', 'jiggaboo', 'bulldyke', 'gayfuck', 'willy', 'schizo', 'drunk', 'nigger', 'unwed', 'weiner', 'apeshit', 'bitchass', 'motherfuckings', 'poonani', 'fuck hole', 'darkie', 'ass-jabber', 'queerhole', 'cumshot', 'l3i+ch', 'splooge', 'c.u.n.t', 'dick head', 'knob', 'balls', 'pegging', 'pcp', 'eat my ass', 'undressing', 'honkey', 'feck', 'fudgepacker', 'whorehouse', 'assfukka', 'ritard', 'faggs', 'ruski', 'bookie', 'fucktards', 'knobjokey', 'coffin dodger', 'dumbfuck', 'bbw', 'cornhole', 'souse', 'weewee', 'tittyfucker', 'cunthunter', 'fingerfucks', 'vodka', 'asshat', 'nappy', 'dammit', 'booooooobs', 'wedgie', 'dp action', 'fucktwat', 'pedobear', 'bitchtits', 'a55', 'douchebag', 'poonany', 'lmao', 'kinky', 'gay', \"bang (one's) box\", 'cocksuckers', 'dicks', 'beardedclam', 'labia', 'maxi', 'sambo', 'anal impaler', 'camwhore', 'rimjob', 'god-dam', 'cumdumpster', 'chick with a dick', 'wh0reface', 'chota bags', 'queer', 'teat', 'pubic', 'swinger', 'tit wank', 'clitorus', 'puto', 'menage a trois', 'butt plug', 'c0ck', 'knob end', 'yaoi', 'cuntlick', 'erotic', 'coonnass', 'stroke', 'zoophilia', 'faggotcock', '5hit', 'f_u_c_k', 'diligaf', 'anal leakage', 'knobend', 'fice', 'eat hair pie', 'chodes', 'a_s_s', 'gash', 'shagging', 'snowballing', 'rectum', 'twats', 'mound of venus', 'sex', 'masterbating', 'orgy', 'shrimping', 'kwif', 'valium', 'deep throat', 'boink', 'nut sack', 'teez', 'whoralicious', 'scrog', 'h0mo', 'duche', 'fucka', 'huge fat', 'autoerotic', 'pissflaps', 'gangbang', 'sh!t', 'fuckbutt', 'fuckboy', 'slutbag', 'asslick', 'assgoblin', 'cockburger', 'asswipes', 'd0ng', 'prod', 'twatlips', 'kunt', 'cockmongler', 'bodily', 'gaydo', 'sissy', 'lusty', 'blow job', 'lmfao', 'coksucka', 'titties', 'n1gga', 'opiate', 'jock', 'renob', 'faig', 'foreskin', 'cummer', 'ninnyhammer', 'pedophile', 'anus', 'cleveland steamer', 'nobhead', 'reverse cowgirl', 'viagra', 'boobies', 'teste', 'orgasmic', 'buttplug', 'cok', 'holy shit', 'dummy', 'shitheads', 'smartasses', 'cumdump', 'booze', 'pedophilia', 'buceta', 'fuckers', 'omg', 'shitass', 'lesbos', 'cocksucked', 'bestiality', 'ass hole', 'nutsack', 'va-j-j', 'shiznit', 'shitfaced', 'child-fucker', 'boong', 'clits', 'choad', 'pussi', 'cunilingus', 'dipshit', 'm0f0', 'bullshits', 'blow your load', 'chink', 'cunt-struck', 'arse', 'mothafuckings', 'fukwhit', 'scroat', 'pillowbiter', 'ninny', 'assshit', 'hitler', 'white power', 'ovary', 'ass', 'slope', 'darn', 'fannybandit', 'fuckwit', 'muff puff', 'queero', 'wank', 'kraut', 'boooobs', 'coochie', 'snuff', 'arrse', 'lesbo', 'lube', 'juggs', 'hircismus', 'crap', 'topless', 'pussies', 's.h.i.t.', 'cnut', 'herpy', 'jigaboo', 'orgasm', 'bitched', 'jerk-off', 'punkass', 'tongue in a', 'cunthole', 's.o.b.', 'bung', 't1tties', 'wench', 'assh0le', 'cuntsicle', 'ejakulate', 'wanky', 'gaybob', 'b1tch', 'cumjockey', 'hemp', 'godsdamn', 'whorebag', 'pimp', 'ball sack', 'shitface', 'chode', 'anilingus', 'brotherfucker', 'bod', 'bong', 'orgasms', 'male squirting', 'penis', 'god', 'tits', 'bastinado', 's-o-b', 'gokkun', 'gaylord', 'coochy', 'bummer', 'dominatrix', 'goatse', 'mother fucker', 'wet dream', 'busty', 'cocksmith', 'shitspitter', 'cockmaster', 'coccydynia', 'niggaz', 'dillweed', 'tinkle', 'c-0-c-k', 'tub girl', 'gang bang', 'clover clamps', 'pissing', 'hoor', 'damn', 'fagbag', 'goatcx', 'bitchy', 'bastards', 'commie', 'sand nigger', 'strappado', 'foah', 'yiffy', 'fuck puppet', 'bumblefuck', 'twunter', 'female squirting', 'frenchify', 'dong', 'prostitute', 'asscock', 'mothafuck', 'cockass', 'diddle', 'inbred', 'doggie-style', 'snatch', 'poopuncher', 'gooks', 'sadism', 'threesome', 'dickzipper', 'frigg', 'fag', 'fingerfuck', 'footjob', 'gonad', 'cunnilingus', 'hiv', 'tard', 'fuckings', 'pusse', 'ganja', 'moron', 'bastardo', 'fuckingshitmotherfucker', 'muffdiving', 'cumguzzler', 'xrated', 'nazi', 'titt', 'wankjob', 'doggystyle', 'guido', 'pleasure chest', 'phallic', 'strip club', 'venus mound', 'dingle', 'dingleberries', 'god damn', 'nutter', 'asswhole', 'azazel', 'throating', 'x-rated', 'dick', 'ovums', 'shaved beaver', 'ejaculation', 'assmunch', 'bukkake', 'v14gra', 'hoer', 'nambla', 'hooters', 'fartknocker', 'cocksukka', 'fucktard', 'taig', 'dick hole', 'tw4t', 'fistfuckers', 'shittier', 'kondums', 'orgasims', 'beaver', 'womb', 'tawdry', 'gfy', 'jackhole', 'crappy', 'injun', 'retarded', 'bastard', 'nude', 'd0uch3', 'daterape', 'queef', 'tittyfuck', 'bondage', 'nigg3r', 'bimbo', 'moolie', 'kooch', 'cuntlicker', 'phuq', 'how to murdep', 'fuck', 'he-she', 'spiks', 'he11', 'cockmonkey', 'erect', 'loin', 'son of a bitch', 'twunt', 'cyberfucking', 'doublelift', 'bung hole', 'muff', 'gigolo', 'fenian', 'bootee', 'coital', 'seduce', 'rusty trombone', 'fux0r', 'slutdumper', 'vomit', 'poon', 'booooobs', 'dumbass', 'shitbag', 'hoar', 'perversion', 'cl1t', 'swastika', 'sodom', 'boner', 'munging', 'xx', 'seamen', 'doochbag', 'frotting', 'fannyfucker', 'buncombe', 'mothafuckaz', 'fcuker', 'hardcore', 'trashy', 'caca', 'barely legal', 'muff diver', 'beaver cleaver', 'blow me', 's-h-1-t', 'fuckhead', 'skag', 'ejaculates', 'weirdo', 'cuntlicking', 'punany', 'jackoff', 'dickwad', 'ar5e', 'kums', 'scat', 'assholes', 'nig nog', 'wazoo', 'bitcher', 'loins', 'gayfuckist', 'gtfo', 'dendrophilia', 'homo', 'paedophile', 'choade', 'wrinkled starfish', 'pussy', 'gassy ass', 'dike', 'babeland', 'nimrod', 'gey', 'fagging', 'mcfagget', 'sperm', 'willies', 'lolita', 'skeet', 'gspot', 'crotte', 'pussys', 'pubis', 'strapon', 'rape', 'orgasim', 'dickhole', 'beef curtain', 'cums', 'assmuncher', 'assnigger', 'fcuk', 'quicky', 'jerked', 'fistfucking', 'jism', 'cunillingus', 'whorealicious', 'booobs', 'bust a load', 'moo moo foo foo', 'cunts', 'analprobe', 'tit', 'cervix', 'hookah', 'slutkiss', 'cockmuncher', 'nut butter', 'pissed off', 'towelhead', 'vagina', 'strap on', 'menstruate', 'gayass', 'cumshots', 'essohbee', 'homey', 'ghey', 'dumbshit', 'bellend', 'jerkass', 'big black', 'fuckface', 'fingerbang', 'rubbish', 'punani', 'yobbo', 'crabs', 'a2m', 'fagot', 'douchebags', 'fuckbag', 'tushy', 'nigga', 'fucknugget', 'shitting', 'doggy-style', 'assshole', 'heshe', 'vorarephilia', 'reefer', 'peepee', 'dickheads', 'dickflipper', 'bugger', 'testee', 'ass-pirate', 'dickjuice', 'cock-sucker', 'poop chute', 'c-u-n-t', 'nob jokey', 'jagoff', 'porn', 'polesmoker', 'goddamn', 'felching', 'assfaces', 'knobjocky', 'git', 'glans', 'bollox', 'cockshit', 'screw', 'make me come', 'dimwit', 'assjacker', 'clitty', 'faggot', 'arsehole', 'pricks', 'teabagging', 'c.0.c.k', 'dickface', 'motherfuckka', 'biatch', 'booby', 'jerk off', 'knobed', 'sandbar', 'fuck off', 'whorehopper', 'boned', 'feist', 'sleaze', 'golliwog', 'felch', 'chocolate rosebuds', 'cumming', 'masochist', 'butt-pirate', 'a55hole', 'rumprammer', 'peckerhead', 'prickteaser', 'acrotomophilia', 'buttfuck', 'sluts', 'coons', 'fuck-ass', 'weenie', 'bullshit', 'asshead', 'masturbate', 'kondum', 'urine', 'freex', 'jailbait', 't1tt1e5', 'assclown', 'nsfw images', 'homodumbshit', 'kootch', 'tied up', 'scrote', 'wop', 'dicksipper', 'son of a motherless goat', 'cum', 'fingerfucker', 'hooch', 'penetration', 'taff', 'dickripper', 'c-o-c-k', 'fistfuckings', 'shitters', 'a54', 'fuckwad', 'bunny fucker', 'motherfuck', 'bint', 'assmucus', 'shiz', 'cockknocker', 'foot fetish', 'cockface', 'pisses', 'kunilingus', 'transsexual', 'donkeypunch', 'douch3', 'mof0', 'pisspig', 'damned', 'porch monkey', 'goddamnit', 'shitt', 'honky', 'klan', 'kooches', 'bigtits', 'piss', 'screwing', 'opium', 'scag', 'blue waffle', 'frigga', 'pee', 'cunny', 'niggers', 'orally', 'piece of shit', 'boob', 'piss pig', 'pust', 'bum boy', 'sucking', 'n1gger', 'assbanged', 'cuntslut', 'pms', 'r-tard', 'asspirate', 'twatwaffle', 'asshopper', 'cock', 'cocksuka', 'penisfucker', 'motherfucker', 'fuckersucker', 'gonads', 'fukkin', 'fisty', 's0b', 'spic', 'shaved pussy', 'dildos', 'beef curtains', 'jack-off', 'nig-nog', 'cawk', 'lesbians', 'shiting', 'assbanger', 'vulva', 'asswad', 'cocknugget', 'dicksucker', 'assbangs', 'testical', 'lezzie', 'two fingers', 'beotch', 'coprophilia', 'blonde on blonde action', 'shitted', 'santorum', 'barenaked', 'cum chugger', 'cocksmoker', 'penisbanger', 'spick', 'suckass', 'bdsm', 'shits', 'masturbation', 'bender', 'hun', 'seaman', 'punanny', 'junglebunny', 'son of a whore', 'tampon', 'dago', 'shithouse', 'feltch', 'dickslap', 'junkie', 'shithole', 'titfuck', 'double dong', 'kinkster', 'camgirl', 'cunt hair', 'fanyy', 'vag', 'foobar', 'smut', 'dickfuck', 'nazism', 'assmonkey', 'sexo', 'black cock', 'cyberfucked', 'fuckin', 'fanny', 'whores', 'sexual', 'bosom', 'hentai', 'humped', 'bitch', 'chesticle', 'corp whore', 'quim', 'scrotum', 'nigaboo', 'one cup two girls', 'butthole', 'dirty pillows', 'extacy', 'minge', 'fist fuck', 'porno', 'areola', 'testes', 'cuntrag', 'mo-fo', 'asswipe', 'assho1e', 'erotism', 'leather restraint', 'cockjockey', 'phukked', 'hooker', 'hooter', 'jackasses', 'muffdiver', 'junky', 'assfuck', 'shitbrains', 'flog the log', 'jackass', 'doofus', 'revue', 'clitoris', 'big breasts', 'dickwhipper', 'minger', 'fistfucked', 'potty', 'pornos', 'ball kicking', 'donkeyribber', 'phuck', 'cocaine', 'vibrator', 'ginger', 'nigg4h', 'pigfucker', 'cummin', 'pthc'}, {'it', 'had', 'thank', 'new', 'largely', 'take', 'specified', 'show', \"you've\", 'here', 'willing', 'eighty', 'arise', 'did', 'similar', 'regards', 'ie', 'thereafter', \"hasn't\", 'never', 'b', 'moreover', 'although', 'as', 'says', 'wants', 'mr', 'namely', 's', 'become', 'inc', 'how', 'like', 'kg', 'means', 'except', 'unto', 'whoever', 'more', 'anyways', 'asking', 'between', 'also', 'taking', 'thanx', 'itself', 'hereby', 'were', 'herein', 'plus', 'somewhere', 'becoming', 'adj', 'on', 'thru', 'c', 'beside', 'others', 'approximately', 'accordance', 'got', 'lately', 'saying', 'begins', 'believe', 'alone', 'index', 'most', 'predominantly', 'slightly', 'and', 'those', 'throug', 'everyone', 'towards', 'whats', 'usefulness', 'down', 'edu', 'certain', 'ord', 'ours', 'previously', 'seem', 'see', 'ex', 'since', 'ourselves', 'some', 'not', 'i', 'normally', 'an', 'from', 'begin', 'such', 'far', 'vs', 'wherein', 't', 'sometimes', 'whereupon', 'mean', 'be', 'know', 'www', 'youd', 'neither', 'several', 'ok', 'she', 'vol', 'somethan', 'related', 'whatever', 'whenever', 'hers', 'cannot', \"haven't\", 'whomever', 'affecting', 'q', 'primarily', 'mostly', 'instead', 'refs', 'vols', 'least', 'r', 'thousand', 'id', 'further', 'took', 'being', 'above', 'seven', 'arent', 'usefully', 'contains', 'beginning', 'youre', 'anyway', 'hes', 'him', 'poorly', 'six', 'wherever', 'else', 'somewhat', 'came', 'looking', 'ltd', 'awfully', 'he', 'briefly', 'thereby', 'affected', 'anymore', 'million', 'ask', 'g', 'think', 'himself', 'zero', 'significantly', 'across', \"you'll\", 'qv', 'get', 'results', 'hundred', 'nor', 'only', 'obtained', 'latter', 'about', 'gave', 'auth', \"'ve\", 'therein', 'added', 'thoughh', 'herself', 'resulted', 'look', 'every', 'ca', 'until', 'due', 'therere', 'whereby', 'beforehand', 'around', 'widely', 'followed', 'rather', 'whod', 'specifying', 'four', 'throughout', 'whence', 'co', 'que', 'you', 'anyone', \"can't\", \"i'll\", 'fifth', 'ones', 'nd', 'another', 'noone', 'e', 'contain', 'whereafter', 'giving', 'could', \"she'll\", 'brief', 'respectively', 'accordingly', 'everywhere', \"there've\", 'that', 'whole', 'x', 'aside', 'theyd', 'heres', 'somehow', 'act', 'non', 'owing', 'provides', 'couldnt', 'ups', 'seeming', 'different', 'us', 'besides', 'at', 'shall', 'give', 'thou', 'less', 'keep', 'together', 'howbeit', 'ran', 'meanwhile', 'im', 'n', 'onto', 'apparently', 'cause', 'un', 'wasnt', 'wed', 'ending', 'doing', 'ought', 'furthermore', 'm', 'taken', 'ever', 'whereas', 'regarding', 'went', 'where', \"they'll\", 'wheres', 'any', 'viz', 'particularly', 'her', 'promptly', 'often', 'comes', 'tip', 'yourselves', \"doesn't\", 'mrs', 'getting', 'proud', \"don't\", 'own', 'anything', 'wouldnt', 'which', 'nothing', 'whos', \"there'll\", 'formerly', 'hence', 'biol', 'nevertheless', 'particular', 'oh', 'wont', 'thereto', 'other', 'am', 'became', 'saw', 'regardless', 'theres', \"we'll\", 'yourself', 'part', 'said', 'nay', 'overall', 'already', 'wish', 'up', 'section', 'been', 'lets', 'many', 'showns', 'whom', 'make', 'soon', 'against', 'all', 'either', 'a', 'must', 'uses', 'sorry', 'hither', 'immediately', 'home', 'obtain', 'everybody', 'probably', 'twice', 'causes', 'say', 'words', 'y', 'sent', 'rd', 'according', 'are', 'becomes', 'inward', 'u', 'selves', 'gives', 'known', 'end', 'what', 'yet', 'name', 'put', 'come', 'significant', 'made', 'mainly', 'hi', 'substantially', 'if', 'little', 'before', 'shes', 'welcome', 'fix', \"shouldn't\", 'would', 'same', 'nobody', 'sure', 'likely', 'line', 'hereafter', 'follows', 'available', 'does', 'w', 'ref', 'specify', 'let', 'want', 'both', 'over', 'usually', 'may', 'thus', 'quite', 'five', 'mg', 'hed', 'theyre', 'each', 'away', 'gone', 'when', 'under', 'once', 'til', 'right', 'than', 'k', 'present', 'miss', 'yes', 'information', 'very', 'for', 'nos', 'then', 'with', 'o', 'someone', 'happens', 'na', \"we've\", \"that've\", 'et-al', 'while', 'f', 'shown', 'do', 'its', 'unlike', 'ts', 'they', 'who', 'recent', 'announce', 'specifically', 'has', 'sec', 'self', 'per', \"they've\", 'shed', 'thereof', 'quickly', 'itd', 'nearly', 'have', 'should', 'okay', 'me', 'gets', 'et', 'found', 'recently', 'd', 'the', 'done', 'etc', 'toward', \"i've\", 'km', 'next', 'there', 'into', 'along', 'former', 'much', 'unfortunately', 'following', 'still', 'our', 'again', 'keeps', 'whether', 'goes', 'your', 'back', 'useful', 'abst', 'among', 'past', 'containing', 'them', 'during', 'whose', 'thats', 'world', 'something', 'seemed', 'to', 'possible', 'yours', 'run', 'upon', 'but', 'merely', 'because', 'ed', \"that'll\", 'of', 'always', \"what'll\", 'important', 'can', 'showed', 'page', 'go', 'hid', 'eg', 'nonetheless', 'no', 'unless', 'having', 'given', 'otherwise', 'afterwards', 'none', 'meantime', \"didn't\", 'possibly', \"'ll\", 'sometime', 'need', 'everything', 'perhaps', 'near', 'downwards', 'l', 'try', 'makes', 'behind', 'invention', 'too', 'stop', 'readily', 'therefore', 'latterly', 'h', 'kept', 'pages', 'why', 'by', 'date', 'tried', 'anyhow', 'thanks', 'anybody', 'anywhere', 'actually', 'was', 'looks', 'werent', 'placed', 'effect', 'ah', 'noted', 'omitted', 'tell', 'whither', 'aren', 'without', 'suggest', 'com', 'especially', 'their', 'these', 'importance', 'p', 'forth', 're', 'lest', 'nine', 'z', 'gotten', 'through', 'seems', 'research', 'we', 'ml', 'v', 'thence', 'though', 'after', 'unlikely', 'pp', 'resulting', 'immediate', 'necessarily', 'strongly', 'thered', 'however', 'one', 'knows', 'tends', 'out', 'sup', \"isn't\", 'needs', 'please', 'tries', 'last', \"it'll\", 'themselves', 'value', 'via', 'eight', 'few', 'below', 'my', 'sub', 'is', 'indeed', 'myself', 'ninety', 'using', 'almost', 'nowhere', 'successfully', 'within', 'able', 'similarly', 'in', 'used', 'whim', 'truly', 'two', 'somebody', 'off', 'th', 'hereupon', 'use', 'potentially', 'beginnings', 'now', 'this', 'liked', 'might', 'amongst', 'certainly', 'maybe', 'later', 'old', 'shows', 'relatively', 'his', 'necessary', 'affects', 'elsewhere', 'really', 'beyond', 'sufficiently', 'theirs', 'trying', 'so', 'enough', 'seen', 'j', 'thereupon', 'seeing', 'hardly', 'outside', \"who'll\", 'even', 'mug', 'obviously', 'just', 'first', 'or', 'various', 'way', 'ff'}]\n"
     ]
    }
   ],
   "source": [
    "# https://blog.cambridgespark.com/50-free-machine-learning-datasets-sentiment-analysis-b9388f79c124\n",
    "\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/feature_extraction.html#stop-words\n",
    "# https://www.aclweb.org/anthology/W18-2502/\n",
    "def stopwords():\n",
    "    import os\n",
    "    root_path = f\"/home/halpdesk/CODE/reddit-parser\"\n",
    "    filename = f\"{root_path}/datasets/ranksnl_large.csv\"\n",
    "    stop_words = []\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            for word in line.split(\",\"):\n",
    "                stop_words.append(word.lower().strip())\n",
    "    return set(stop_words)\n",
    "\n",
    "# Not the same ignore tokens as in nb-features - because we want to check for smilies here\n",
    "def ignoretokens():\n",
    "    common_fractals = [\"1/2\", \"1/3\", \"1/4\"]\n",
    "    low_numbers = [str(int) for int in range(0,10)]\n",
    "    mid_numbers = [str(int) for int in range(10,100,10)]\n",
    "    high_numbers = [str(int) for int in range(100,100100,100)]\n",
    "    tokens = [',', '.', '\"', '``', \"''\", '`', '*', '_', \"&\", \"$\", \"!\", \"#\", \"%\", \"'\", \"”\", \"“\", \"’\", \"‘\", \"―\", \"—\", \"~\", \"–\", \"/\", \"  \", \"   \", \"    \", \"\\n\", \"\\t\", \"\\r\\n\", \"\\r\", \"\t\", \"?\"] + low_numbers + mid_numbers + high_numbers\n",
    "    return tokens\n",
    "\n",
    "def badwords():\n",
    "    import os\n",
    "    root_path = f\"/home/halpdesk/CODE/reddit-parser\"\n",
    "    filename = f\"{root_path}/datasets/fb-bad-words.csv\"\n",
    "    bad_words = []\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            for word in line.split(\",\"):\n",
    "                bad_words.append(word.lower().strip())\n",
    "    return set(bad_words)\n",
    "\n",
    "\n",
    "def commonwords():\n",
    "    import os\n",
    "    root_path = f\"/home/halpdesk/CODE/reddit-parser\"\n",
    "    filename = f\"{root_path}/datasets/common-words.csv\"\n",
    "    common_words = []\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            for word in line.split(\",\"):\n",
    "                common_words.append(word.lower().strip())\n",
    "    return set(common_words)\n",
    "\n",
    "def names():\n",
    "    import os\n",
    "    root_path = f\"/home/halpdesk/CODE/reddit-parser\"\n",
    "    filename = f\"{root_path}/datasets/first_names_all.csv\"\n",
    "    first_names = []\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            for word in line.split(\",\"):\n",
    "                first_names.append(word.strip())\n",
    "    filename = f\"{root_path}/datasets/last_names_all.csv\"\n",
    "    last_names = []\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            for word in line.split(\",\"):\n",
    "                last_names.append(word.strip())\n",
    "        \n",
    "    return set(first_names + last_names)\n",
    "\n",
    "# From https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0144296:\n",
    "# http://kt.ijs.si/data/Emoji_sentiment_ranking/\n",
    "# Other: https://research.utwente.nl/files/5482763/sac13-senticon.pdf\n",
    "# http://emojitracker.com/\n",
    "def emoticons():\n",
    "    import os\n",
    "    root_path = f\"/home/halpdesk/CODE/reddit-parser\"\n",
    "    filename = f\"{root_path}/datasets/emoticons.csv\"\n",
    "    emoticons = {\n",
    "        \"pos\": [\":)\", \":))\", \":D\", \":DD\", \"xD\", \"xDD\", \":d\" \"=)\", \"=))\", \":')\", \"=')\", \":}\", \":}}\", \":]\", \":]]\", \"(:\", \"C:\", \":P\"],\n",
    "        \"neu\": [\":|\", \":/\", \":\\\\\", \"\"],\n",
    "        \"neg\": [\":(\", \":((\", \":(((\", \":((((\", \":C\", \":CC\", \":'C\", \"=(\", \"=((\", \":'(\", \"='(\", \":[\", \":{\", \"):\"]\n",
    "    }\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            word = line.split(\",\")\n",
    "            smil = word[0]\n",
    "            #unicodesmil = chr(int(word[1], 0))\n",
    "            neg = float(word[3])\n",
    "            neu = float(word[4])\n",
    "            pos = float(word[5])\n",
    "            if neu > pos and neu > neg:\n",
    "                emoticons[\"neu\"].append(smil)\n",
    "            elif pos > neg:\n",
    "                emoticons[\"pos\"].append(smil)\n",
    "                #emoticons[\"pos\"].append(unicodesmil)\n",
    "            else:\n",
    "                emoticons[\"neg\"].append(smil)\n",
    "    return emoticons\n",
    "\n",
    "print([emoticons(), badwords(), stopwords()])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "                                           submission  \\\n0   The author Tolkiens son has released a new fan...   \n1   The author Tolkiens son has released a new fan...   \n2   The author Tolkiens son has released a new fan...   \n3   The author Tolkiens son has released a new fan...   \n4   The author Tolkiens son has released a new fan...   \n5   The author Tolkiens son has released a new fan...   \n6   The author Tolkiens son has released a new fan...   \n7   The author Tolkiens son has released a new fan...   \n8   The author Tolkiens son has released a new fan...   \n9   The author Tolkiens son has released a new fan...   \n10  The author Tolkiens son has released a new fan...   \n11  The author Tolkiens son has released a new fan...   \n12  The author Tolkiens son has released a new fan...   \n13  The author Tolkiens son has released a new fan...   \n14  The author Tolkiens son has released a new fan...   \n15  The author Tolkiens son has released a new fan...   \n16  The author Tolkiens son has released a new fan...   \n17  The author Tolkiens son has released a new fan...   \n18  The author Tolkiens son has released a new fan...   \n19  The author Tolkiens son has released a new fan...   \n20  The author Tolkiens son has released a new fan...   \n\n                                                 body  \n0   Hey Tommy the blue, this is a great book for y...  \n1   Awesome books! Awesome author! Where did you g...  \n2   I hate this author :C he is really bad he can ...  \n3   No way! The blue wizards finally got a book?? ...  \n4   Fantasy is boring :/. I'd rather dive into som...  \n5    :D:D YEY!! *amazing* werent they supposed to be?  \n6               this is an old bag with herpes book 👎  \n7                             this wook I like 💚💚💚 !!  \n8                              marry this book I will  \n9   if you are going to buy this book, make sure y...  \n10                             how much does it cost?  \n11                                      I can pay $$$  \n12                      BLUE. JUNO. 100% sh!t http://  \n13          loooooooooooooooooooooooooooool trolololo  \n14  The authors son has released a new fantasy ser...  \n15                                author, blue wizard  \n16  Andrei svensson Daniel, Laura Marx Engels, Jon...  \n17  marry fantasy blue Nietzsche awsome Tommy trol...  \n18                           https://asoftmurmur.com/  \n19  we'll use one word from each in the next comme...  \n20  Tommy, blue, awesome book, ugly, fantasy, amaz...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>submission</th>\n      <th>body</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>Hey Tommy the blue, this is a great book for y...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>Awesome books! Awesome author! Where did you g...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>I hate this author :C he is really bad he can ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>No way! The blue wizards finally got a book?? ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>Fantasy is boring :/. I'd rather dive into som...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>:D:D YEY!! *amazing* werent they supposed to be?</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>this is an old bag with herpes book 👎</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>this wook I like 💚💚💚 !!</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>marry this book I will</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>if you are going to buy this book, make sure y...</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>how much does it cost?</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>I can pay $$$</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>BLUE. JUNO. 100% sh!t http://</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>loooooooooooooooooooooooooooool trolololo</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>The authors son has released a new fantasy ser...</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>author, blue wizard</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>Andrei svensson Daniel, Laura Marx Engels, Jon...</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>marry fantasy blue Nietzsche awsome Tommy trol...</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>https://asoftmurmur.com/</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>we'll use one word from each in the next comme...</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>Tommy, blue, awesome book, ugly, fantasy, amaz...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "import pandas as pd\n",
    "submission = \"The author Tolkiens son has released a new fantasy series! The first book is about the Blue wizards! OMG YOU HAVE TO BUY :D:D\"\n",
    "sentences = [\n",
    "    \"Hey Tommy the blue, this is a great book for you 😁💚\",\n",
    "    \"Awesome books! Awesome author! Where did you get it? :)\",\n",
    "    \"I hate this author :C he is really bad he can go and ugly himself ugly\",\n",
    "    \"No way! The blue wizards finally got a book?? OMG I have to buy it xD 👻\",\n",
    "    \"Fantasy is boring :/. I'd rather dive into some Nietzsche and I think everyone should\",\n",
    "    \":D:D YEY!! *amazing* werent they supposed to be?\",\n",
    "    \"this is an old bag with herpes book 👎\",\n",
    "    \"this wook I like 💚💚💚 !!\",\n",
    "    \"marry this book I will\",\n",
    "    \"if you are going to buy this book, make sure you get it at towns hall because they have extra t-shirts!\",\n",
    "    \"how much does it cost?\",\n",
    "    \"I can pay $$$\",\n",
    "    \"BLUE. JUNO. 100% sh!t http://\",\n",
    "    \"loooooooooooooooooooooooooooool trolololo\",\n",
    "    \"The authors son has released a new fantasy series! The first book is about the Blue wizards! OMG YOU HAVE TO BUY :D:D\",\n",
    "    \"author, blue wizard\",\n",
    "    \"Andrei svensson Daniel, Laura Marx Engels, Jonas!, Carl Sagan, Charlie Chapplin$, Walt Disney%, Frank Sinatra\",\n",
    "    \"marry fantasy blue Nietzsche awsome Tommy trolololo\",\n",
    "    \"https://asoftmurmur.com/\",\n",
    "    \"we'll use one word from each in the next comment to check similarity for all other comments\",\n",
    "    \"Tommy, blue, awesome book, ugly, fantasy, amazing, author\"\n",
    "]\n",
    "df = pd.DataFrame({\"submission\": [submission for i in range(len(sentences))], \"body\": sentences})\n",
    "display(df)"
   ]
  },
  {
   "source": [
    "### Links \n",
    "- https://www.researchgate.net/publication/221397355_Complex_Linguistic_Features_for_Text_Classification_A_Comprehensive_Study"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "is -> be\n",
      "did -> do\n",
      "is -> be\n",
      "wizards -> wizard\n",
      "got -> get\n",
      "is -> be\n",
      "supposed -> suppose\n",
      "is -> be\n",
      "are -> be\n",
      "going -> go\n",
      "towns -> town\n",
      "does -> do\n",
      "authors -> author\n",
      "has -> have\n",
      "released -> release\n",
      "is -> be\n",
      "comments -> comment\n",
      "['!!', '$$$', '*amazing*', '100%', ':)', ':/.', ':c', ':d:d', 'a', 'about', 'all', 'amazing,', 'an', 'and', 'andrei', 'at', 'author', 'author!', 'author,', 'awesome', 'awsome', 'bad', 'bag', 'be', 'be?', 'because', 'blue', 'blue,', 'blue.', 'book', 'book,', 'book??', 'books!', 'boring', 'buy', 'can', 'carl', 'chapplin$,', 'charlie', 'check', 'comment', 'cost?', 'daniel,', 'disney%,', 'dive', 'do', 'each', 'engels,', 'everyone', 'extra', 'fantasy', 'fantasy,', 'finally', 'first', 'for', 'frank', 'from', 'get', 'go', 'great', 'hall', 'hate', 'have', 'he', 'herpes', 'hey', 'himself', 'how', 'http://', 'https://asoftmurmur.com/', 'i', \"i'd\", 'if', 'in', 'into', 'it', 'it?', 'jonas!,', 'juno.', 'laura', 'like', 'loooooooooooooooooooooooooooool', 'make', 'marry', 'marx', 'much', 'new', 'next', 'nietzsche', 'no', 'old', 'omg', 'one', 'other', 'pay', 'rather', 'really', 'release', 'sagan,', 'series!', 'sh!t', 'should', 'similarity', 'sinatra', 'some', 'son', 'suppose', 'sure', 'svensson', 't-shirts!', 'the', 'they', 'think', 'this', 'to', 'tommy', 'tommy,', 'town', 'trolololo', 'ugly', 'ugly,', 'use', 'walt', 'way!', \"we'll\", 'werent', 'where', 'will', 'with', 'wizard', 'wizards!', 'wook', 'word', 'xd', 'yey!!', 'you', '👎', '👻', '💚💚💚', '😁💚']\n",
      "wizards -> wizard\n",
      "supposed -> suppose\n",
      "towns -> town\n",
      "authors -> author\n",
      "released -> release\n",
      "comments -> comment\n",
      "blue, -> blue\n",
      "books! -> book\n",
      "author! -> author\n",
      "wizards -> wizard\n",
      "book?? -> book\n",
      ":/. -> :\n",
      "yey!! -> yey\n",
      "*amazing* -> amazing\n",
      "supposed -> suppose\n",
      "!! -> \n",
      "book, -> book\n",
      "towns -> town\n",
      "t-shirts! -> t-shirt\n",
      "cost? -> cost\n",
      "$$$ -> \n",
      "blue. -> blue\n",
      "juno. -> juno\n",
      "100% -> \n",
      "http:// -> http:\n",
      "authors -> author\n",
      "released -> release\n",
      "series! -> series\n",
      "wizards! -> wizard\n",
      "author, -> author\n",
      "daniel, -> daniel\n",
      "engels, -> engels\n",
      "jonas!, -> jonas\n",
      "sagan, -> sagan\n",
      "chapplin$, -> chapplin\n",
      "disney%, -> disney\n",
      "https://asoftmurmur.com/ -> https://asoftmurmur.com\n",
      "comments -> comment\n",
      "tommy, -> tommy\n",
      "blue, -> blue\n",
      "book, -> book\n",
      "fantasy, -> fantasy\n",
      "amazing, -> amazing\n",
      "is -> be\n",
      "did -> do\n",
      "is -> be\n",
      "wizards -> wizard\n",
      "got -> get\n",
      "is -> be\n",
      "supposed -> suppose\n",
      "is -> be\n",
      "are -> be\n",
      "going -> go\n",
      "towns -> town\n",
      "does -> do\n",
      "authors -> author\n",
      "has -> have\n",
      "released -> release\n",
      "is -> be\n",
      "comments -> comment\n",
      "is -> be\n",
      "did -> do\n",
      "is -> be\n",
      "wizards -> wizard\n",
      "got -> get\n",
      "is -> be\n",
      "supposed -> suppose\n",
      "is -> be\n",
      "are -> be\n",
      "going -> go\n",
      "towns -> town\n",
      "does -> do\n",
      "authors -> author\n",
      "has -> have\n",
      "released -> release\n",
      "is -> be\n",
      "comments -> comment\n",
      "is -> be\n",
      "did -> do\n",
      "is -> be\n",
      "wizards -> wizard\n",
      "got -> get\n",
      "is -> be\n",
      "supposed -> suppose\n",
      "is -> be\n",
      "are -> be\n",
      "going -> go\n",
      "towns -> town\n",
      "does -> do\n",
      "authors -> author\n",
      "has -> have\n",
      "released -> release\n",
      "is -> be\n",
      "comments -> comment\n",
      "released -> release\n",
      "wizards -> wizard\n",
      "supposed -> suppose\n",
      "towns -> town\n",
      "authors -> author\n",
      "released -> release\n",
      "comments -> comment\n",
      "boring -> bore\n",
      "wizards -> wizard\n",
      "supposed -> suppose\n",
      "towns -> town\n",
      "authors -> author\n",
      "released -> release\n",
      "comments -> comment\n",
      "wizards -> wizard\n",
      "supposed -> suppose\n",
      "towns -> town\n",
      "authors -> author\n",
      "released -> release\n",
      "comments -> comment\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                           submission  \\\n",
       "0   The author Tolkiens son has released a new fan...   \n",
       "1   The author Tolkiens son has released a new fan...   \n",
       "2   The author Tolkiens son has released a new fan...   \n",
       "3   The author Tolkiens son has released a new fan...   \n",
       "4   The author Tolkiens son has released a new fan...   \n",
       "5   The author Tolkiens son has released a new fan...   \n",
       "6   The author Tolkiens son has released a new fan...   \n",
       "7   The author Tolkiens son has released a new fan...   \n",
       "8   The author Tolkiens son has released a new fan...   \n",
       "9   The author Tolkiens son has released a new fan...   \n",
       "10  The author Tolkiens son has released a new fan...   \n",
       "11  The author Tolkiens son has released a new fan...   \n",
       "12  The author Tolkiens son has released a new fan...   \n",
       "13  The author Tolkiens son has released a new fan...   \n",
       "14  The author Tolkiens son has released a new fan...   \n",
       "15  The author Tolkiens son has released a new fan...   \n",
       "16  The author Tolkiens son has released a new fan...   \n",
       "17  The author Tolkiens son has released a new fan...   \n",
       "18  The author Tolkiens son has released a new fan...   \n",
       "19  The author Tolkiens son has released a new fan...   \n",
       "20  The author Tolkiens son has released a new fan...   \n",
       "\n",
       "                                                 body  lnk  top-cos-sim  \\\n",
       "0   Hey Tommy the blue, this is a great book for y...    0     0.082471   \n",
       "1   Awesome books! Awesome author! Where did you g...    0     0.000000   \n",
       "2   I hate this author :C he is really bad he can ...    0     0.114108   \n",
       "3   No way! The blue wizards finally got a book?? ...    0     0.153396   \n",
       "4   Fantasy is boring :/. I'd rather dive into som...    0     0.093090   \n",
       "5    :D:D YEY!! *amazing* werent they supposed to be?    0     0.118058   \n",
       "6               this is an old bag with herpes book 👎    0     0.117727   \n",
       "7                             this wook I like 💚💚💚 !!    0     0.000000   \n",
       "8                              marry this book I will    0     0.123411   \n",
       "9   if you are going to buy this book, make sure y...    0     0.093090   \n",
       "10                             how much does it cost?    0     0.000000   \n",
       "11                                      I can pay $$$    0     0.000000   \n",
       "12                      BLUE. JUNO. 100% sh!t http://    1     0.000000   \n",
       "13          loooooooooooooooooooooooooooool trolololo    0     0.000000   \n",
       "14  The authors son has released a new fantasy ser...    0     1.000000   \n",
       "15                                author, blue wizard    0     0.123411   \n",
       "16  Andrei svensson Daniel, Laura Marx Engels, Jon...    0     0.000000   \n",
       "17  marry fantasy blue Nietzsche awsome Tommy trol...    0     0.176606   \n",
       "18                           https://asoftmurmur.com/    1     0.000000   \n",
       "19  we'll use one word from each in the next comme...    0     0.000000   \n",
       "20  Tommy, blue, awesome book, ugly, fantasy, amaz...    0     0.081873   \n",
       "\n",
       "     cos-sim  tfidf-mean  nam  wc  sw  bw  smil+  smil-  smil&  \n",
       "0   0.254778    0.202954    1   6   6   0      2      0      0  \n",
       "1   0.232763    0.240204    0   5   4   0      1      0      0  \n",
       "2   0.213965    0.131112    0   4  10   2      0      1      0  \n",
       "3   0.296867    0.186447    0   7   8   1      1      0      1  \n",
       "4   0.208668    0.162718    1   6   9   0      0      0      1  \n",
       "5   0.244467    0.319057    0   4   3   0      2      0      0  \n",
       "6   0.179406    0.198450    0   3   5   2      0      1      0  \n",
       "7   0.192876    0.288675    0   3   3   0      3      0      0  \n",
       "8   0.173056    0.343660    0   3   2   0      0      0      0  \n",
       "9   0.261426    0.116227    0   6  15   0      0      0      0  \n",
       "10  0.111357    0.200000    0   1   4   0      0      0      0  \n",
       "11  0.157483    0.353553    0   2   2   0      0      0      0  \n",
       "12  0.222714    0.447214    0   4   0   1      0      0      1  \n",
       "13  0.150209    0.705688    0   2   0   0      0      0      0  \n",
       "14  0.316661    0.143155    1  10  12   1      2      0      0  \n",
       "15  0.173056    0.572766    0   3   0   0      0      0      0  \n",
       "16  0.431284    0.258199    6  15   0   0      0      0      0  \n",
       "17  0.262707    0.376452    2   7   0   0      0      0      0  \n",
       "18  0.111357    1.000000    0   1   0   0      0      0      1  \n",
       "19  0.210445    0.111166    0   5  12   0      0      0      0  \n",
       "20  0.297772    0.402757    0   7   0   0      0      0      0  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>submission</th>\n      <th>body</th>\n      <th>lnk</th>\n      <th>top-cos-sim</th>\n      <th>cos-sim</th>\n      <th>tfidf-mean</th>\n      <th>nam</th>\n      <th>wc</th>\n      <th>sw</th>\n      <th>bw</th>\n      <th>smil+</th>\n      <th>smil-</th>\n      <th>smil&amp;</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>Hey Tommy the blue, this is a great book for y...</td>\n      <td>0</td>\n      <td>0.082471</td>\n      <td>0.254778</td>\n      <td>0.202954</td>\n      <td>1</td>\n      <td>6</td>\n      <td>6</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>Awesome books! Awesome author! Where did you g...</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>0.232763</td>\n      <td>0.240204</td>\n      <td>0</td>\n      <td>5</td>\n      <td>4</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>I hate this author :C he is really bad he can ...</td>\n      <td>0</td>\n      <td>0.114108</td>\n      <td>0.213965</td>\n      <td>0.131112</td>\n      <td>0</td>\n      <td>4</td>\n      <td>10</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>No way! The blue wizards finally got a book?? ...</td>\n      <td>0</td>\n      <td>0.153396</td>\n      <td>0.296867</td>\n      <td>0.186447</td>\n      <td>0</td>\n      <td>7</td>\n      <td>8</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>Fantasy is boring :/. I'd rather dive into som...</td>\n      <td>0</td>\n      <td>0.093090</td>\n      <td>0.208668</td>\n      <td>0.162718</td>\n      <td>1</td>\n      <td>6</td>\n      <td>9</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>:D:D YEY!! *amazing* werent they supposed to be?</td>\n      <td>0</td>\n      <td>0.118058</td>\n      <td>0.244467</td>\n      <td>0.319057</td>\n      <td>0</td>\n      <td>4</td>\n      <td>3</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>this is an old bag with herpes book 👎</td>\n      <td>0</td>\n      <td>0.117727</td>\n      <td>0.179406</td>\n      <td>0.198450</td>\n      <td>0</td>\n      <td>3</td>\n      <td>5</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>this wook I like 💚💚💚 !!</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>0.192876</td>\n      <td>0.288675</td>\n      <td>0</td>\n      <td>3</td>\n      <td>3</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>marry this book I will</td>\n      <td>0</td>\n      <td>0.123411</td>\n      <td>0.173056</td>\n      <td>0.343660</td>\n      <td>0</td>\n      <td>3</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>if you are going to buy this book, make sure y...</td>\n      <td>0</td>\n      <td>0.093090</td>\n      <td>0.261426</td>\n      <td>0.116227</td>\n      <td>0</td>\n      <td>6</td>\n      <td>15</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>how much does it cost?</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>0.111357</td>\n      <td>0.200000</td>\n      <td>0</td>\n      <td>1</td>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>I can pay $$$</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>0.157483</td>\n      <td>0.353553</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>BLUE. JUNO. 100% sh!t http://</td>\n      <td>1</td>\n      <td>0.000000</td>\n      <td>0.222714</td>\n      <td>0.447214</td>\n      <td>0</td>\n      <td>4</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>loooooooooooooooooooooooooooool trolololo</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>0.150209</td>\n      <td>0.705688</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>The authors son has released a new fantasy ser...</td>\n      <td>0</td>\n      <td>1.000000</td>\n      <td>0.316661</td>\n      <td>0.143155</td>\n      <td>1</td>\n      <td>10</td>\n      <td>12</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>author, blue wizard</td>\n      <td>0</td>\n      <td>0.123411</td>\n      <td>0.173056</td>\n      <td>0.572766</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>Andrei svensson Daniel, Laura Marx Engels, Jon...</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>0.431284</td>\n      <td>0.258199</td>\n      <td>6</td>\n      <td>15</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>marry fantasy blue Nietzsche awsome Tommy trol...</td>\n      <td>0</td>\n      <td>0.176606</td>\n      <td>0.262707</td>\n      <td>0.376452</td>\n      <td>2</td>\n      <td>7</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>https://asoftmurmur.com/</td>\n      <td>1</td>\n      <td>0.000000</td>\n      <td>0.111357</td>\n      <td>1.000000</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>we'll use one word from each in the next comme...</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>0.210445</td>\n      <td>0.111166</td>\n      <td>0</td>\n      <td>5</td>\n      <td>12</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>The author Tolkiens son has released a new fan...</td>\n      <td>Tommy, blue, awesome book, ugly, fantasy, amaz...</td>\n      <td>0</td>\n      <td>0.081873</td>\n      <td>0.297772</td>\n      <td>0.402757</td>\n      <td>0</td>\n      <td>7</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel, sigmoid_kernel, rbf_kernel, polynomial_kernel, laplacian_kernel, cosine_similarity\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "\n",
    "class LemmaTokenizer:\n",
    "    ignoretokens = []\n",
    "    stopwords = []\n",
    "    vocab = []\n",
    "    lcase = True\n",
    "    def __init__(self, stopwords=[], vocabulary=[], ignoretokens=[], lcase=True):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "        self.stopwords = stopwords\n",
    "        self.vocab = vocabulary\n",
    "        self.ignoretokens = ignoretokens\n",
    "        self.lcase = lcase\n",
    "    def __call__(self, document):\n",
    "        lemmas = []\n",
    "        tokenized_words = document.split(\" \")\n",
    "        for word, tag in pos_tag(tokenized_words):\n",
    "            lower_cased_tag = tag[0].lower()\n",
    "            stripped_word = word.strip(''.join(self.ignoretokens))\n",
    "            if (word != stripped_word):\n",
    "                #print(f\"{word} -> {stripped_word}\")\n",
    "                pass\n",
    "            wn_tag = lower_cased_tag if lower_cased_tag in ['a', 'r', 'n', 'v'] else None\n",
    "            if not wn_tag:\n",
    "                lemma = stripped_word\n",
    "            else:\n",
    "                lemma = self.wnl.lemmatize(stripped_word, wn_tag)\n",
    "            if lemma not in list(self.stopwords): # and word in self.vocab:\n",
    "                lemmas.append(lemma.lower() if self.lcase else lemma)\n",
    "                if word != lemma:\n",
    "                    print(f\"{word} -> {lemma}\")\n",
    "        return lemmas\n",
    "\n",
    "swords = stopwords()\n",
    "bwords = badwords()\n",
    "cwords = commonwords()\n",
    "ignore = ignoretokens()\n",
    "smil = emoticons()\n",
    "nam = names()\n",
    "\n",
    "#print(cwords)\n",
    "\n",
    "# All vocabs\n",
    "cva = CountVectorizer(lowercase=True, tokenizer=LemmaTokenizer())\n",
    "asdf = cva.fit_transform(df.get(\"body\"))\n",
    "all_vocabs = cva.get_feature_names()\n",
    "print(all_vocabs)\n",
    "\n",
    "# Filtered vocabs\n",
    "cvf = CountVectorizer(lowercase=True, tokenizer=LemmaTokenizer(stopwords=list(swords)+list(bwords)))\n",
    "asdf = cvf.fit_transform(df.get(\"body\"))\n",
    "filtered_vocab = cvf.get_feature_names()\n",
    "\n",
    "# Significant words count\n",
    "cv = CountVectorizer(lowercase=True, tokenizer=LemmaTokenizer(stopwords=list(swords)+list(bwords), ignoretokens=ignore))\n",
    "wc_data = cv.fit_transform(df.get(\"body\"))\n",
    "wc_df = pd.DataFrame(wc_data.sum(axis=1))\n",
    "wc_df.columns = [\"wc\"]\n",
    "\n",
    "# Bad words count\n",
    "cv = CountVectorizer(vocabulary=bwords, stop_words=None, lowercase=True, tokenizer=LemmaTokenizer(), ngram_range=(1,2)) # finds \"old bag\"\n",
    "bw_data = cv.fit_transform(df.get(\"body\"))\n",
    "bw_df = pd.DataFrame(bw_data.sum(axis=1))\n",
    "bw_df.columns = [\"bw\"]\n",
    "\n",
    "# Stop words count\n",
    "cv = CountVectorizer(vocabulary=swords, stop_words=None, lowercase=True, tokenizer=LemmaTokenizer())\n",
    "sw_data = cv.fit_transform(df.get(\"body\"))\n",
    "sw_df = pd.DataFrame(sw_data.sum(axis=1))\n",
    "sw_df.columns = [\"sw\"]\n",
    "\n",
    "# Names count\n",
    "cv = CountVectorizer(vocabulary=nam, stop_words=None, lowercase=False, tokenizer=LemmaTokenizer(lcase=False))\n",
    "nam_data = cv.fit_transform(df.get(\"body\"))\n",
    "nam_df = pd.DataFrame(nam_data.sum(axis=1))\n",
    "nam_df.columns = [\"nam\"]\n",
    "\n",
    "# Positive smilies\n",
    "cv = CountVectorizer(vocabulary=smil[\"pos\"], analyzer=\"char\", ngram_range=(1,2), stop_words=None, lowercase=False) # char + 2 ngram = \":D\"\n",
    "smilp_data = cv.fit_transform(df.get(\"body\"))\n",
    "smilp_df = pd.DataFrame(smilp_data.sum(axis=1))\n",
    "smilp_df.columns = [\"smil+\"]\n",
    "\n",
    "# Negative smilies count\n",
    "cv = CountVectorizer(vocabulary=smil[\"neg\"], analyzer=\"char\", ngram_range=(1,2), stop_words=None, lowercase=False) # char + 2 ngram = \":D\"\n",
    "sniln_data = cv.fit_transform(df.get(\"body\"))\n",
    "smiln_df = pd.DataFrame(sniln_data.sum(axis=1))\n",
    "smiln_df.columns = [\"smil-\"]\n",
    "\n",
    "# Neutral smilies count\n",
    "cv = CountVectorizer(vocabulary=smil[\"neu\"], analyzer=\"char\", ngram_range=(1,2), stop_words=None, lowercase=False) # char + 2 ngram = \":D\"\n",
    "smile_data = cv.fit_transform(df.get(\"body\"))\n",
    "smile_df = pd.DataFrame(smile_data.sum(axis=1))\n",
    "smile_df.columns = [\"smil&\"]\n",
    "\n",
    "# TF-IDF cosine similarity toawrd topic\n",
    "submission_with_comments = [submission] + list(df.get(\"body\").array)\n",
    "tfidfv = TfidfVectorizer(vocabulary=filtered_vocab, lowercase=True, ngram_range=(1,1), smooth_idf=True, tokenizer=LemmaTokenizer(stopwords=list(swords)))\n",
    "tfidf_data = tfidfv.fit_transform(submission_with_comments)\n",
    "cosine_similarities = cosine_similarity(tfidf_data[0:1], tfidf_data[1:]).flatten()\n",
    "top_simi_df = pd.DataFrame(cosine_similarities)\n",
    "top_simi_df.columns = [\"top-cos-sim\"]\n",
    "\n",
    "# TF-IDF cosine similarity towards all documents\n",
    "# (possible to do just as toward topic by changing first element from submission to \" \".join(all_vocabs))\n",
    "submission_with_comments = [\" \".join(all_vocabs)] + list(df.get(\"body\").array)\n",
    "tfidfv = TfidfVectorizer(vocabulary=filtered_vocab, lowercase=True, ngram_range=(1,1), smooth_idf=True, tokenizer=LemmaTokenizer(stopwords=list(swords)))\n",
    "tfidf_data = tfidfv.fit_transform(submission_with_comments)\n",
    "cosine_similarities = cosine_similarity(tfidf_data[0:1], tfidf_data[1:]).flatten()\n",
    "all_simi_df = pd.DataFrame(cosine_similarities)\n",
    "all_simi_df.columns = [\"cos-sim\"]\n",
    "\n",
    "#tfidfv = TfidfVectorizer(vocabulary=all_vocabs, lowercase=True, ngram_range=(1,1), smooth_idf=True)\n",
    "#tfidf_data = tfidfv.fit_transform(df.get(\"body\"))\n",
    "#cosine_similarities = cosine_similarity(tfidf_data, tfidf_data)\n",
    "#cosine_similarities[cosine_similarities == 0] = np.nan\n",
    "#cosine_similarities = np.nanmean(cosine_similarities, axis=1)\n",
    "#all_simi_df = pd.DataFrame(cosine_similarities)\n",
    "#all_simi_df.columns = [\"cos-sim\"]\n",
    "\n",
    "# TF-IDF mean value (checks across all documents)\n",
    "tfidfv = TfidfVectorizer(vocabulary=all_vocabs, lowercase=True, ngram_range=(1,1), smooth_idf=True, tokenizer=LemmaTokenizer(stopwords=list(swords)))\n",
    "tfidf_data = tfidfv.fit_transform(df.get(\"body\")).todense()\n",
    "means = [0]*tfidf_data.shape[0]\n",
    "#means = np.nanmean(tfidf_data, axis=1)\n",
    "for i in range(0, tfidf_data.shape[0]):\n",
    "    word_count = wc_df.get(\"wc\")[i] + bw_df.get(\"bw\")[i] + sw_df.get(\"sw\")[i]\n",
    "    means[i] = tfidf_data[i].sum()/word_count if word_count > 0 else 0\n",
    "    #tfidf_data[i][tfidf_data[i] == 0] = np.nan\n",
    "tfidf_df = pd.DataFrame(means)\n",
    "tfidf_df.columns = [\"tfidf-mean\"]\n",
    "\n",
    "# Has link\n",
    "df['lnk'] = [1 if \"http\" in row[['body']].to_string() else 0 for i,row in df.iterrows()]\n",
    "cv_dataframe = pd.concat([df, top_simi_df, all_simi_df, tfidf_df, nam_df, wc_df, sw_df, bw_df, smilp_df, smiln_df, smile_df], axis=1)\n",
    "cv_dataframe\n"
   ]
  }
 ]
}