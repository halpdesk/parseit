{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.3 64-bit ('python_modules')",
   "display_name": "Python 3.8.3 64-bit ('python_modules')",
   "metadata": {
    "interpreter": {
     "hash": "9634d4b890a726f71d8044046bc71cdc391c406dc663847f104c7db04bcb4cdc"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "\"None of [Index(['topic_similarity', 'tfidf_score', 'words_count', 'stop_words_count',\\n       'bad_words_count'],\\n      dtype='object')] are in the [columns]\"",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-931ad56fb9cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#display(data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CODE/reddit-parser/python_modules/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2906\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2907\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2908\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2910\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CODE/reddit-parser/python_modules/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1252\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1254\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_read_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1255\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CODE/reddit-parser/python_modules/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmissing\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m                 \u001b[0maxis_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1298\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"None of [{key}] are in the [{axis_name}]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1300\u001b[0m             \u001b[0;31m# We (temporarily) allow for some missing keys with .loc, except in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['topic_similarity', 'tfidf_score', 'words_count', 'stop_words_count',\\n       'bad_words_count'],\\n      dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "#Import Gaussian Naive Bayes model\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.preprocessing import MinMaxScaler, KBinsDiscretizer\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "\n",
    "# Drop indexs more from lower score and less from higher score\n",
    "# https://stackoverflow.com/questions/28556942/pandas-remove-rows-at-random-without-shuffling-dataset/28557333#28557333\n",
    "# np.random.seed(10)\n",
    "\n",
    "feature_list = [\"topic_similarity\", \"tfidf_score\", \"words_count\", \"stop_words_count\", \"bad_words_count\"]\n",
    "\n",
    "#display(data)\n",
    "\n",
    "X = df[feature_list]\n",
    "y = df[[\"label\"]]\n",
    "\n",
    "# X = df[df.label > 1000][feature_list]\n",
    "# y = df[df.label > 1000][[\"label\"]]\n",
    "# print(y.shape)\n",
    "# print(X.shape)\n",
    "\n",
    "# Bin all of y first\n",
    "# uniform = All bins in each feature have identical widths.\n",
    "# quantile = All bins in each feature have the same number of points.\n",
    "# kmeans = Values in each bin have the same nearest center of a 1D k-means cluster.\n",
    "bins = 5\n",
    "est = KBinsDiscretizer(n_bins=bins, encode='ordinal', strategy=\"uniform\") #\n",
    "est.fit(y)\n",
    "yt = pd.DataFrame(data=est.transform(y), columns=[\"y\"])\n",
    "\n",
    "display(est.transform(y))\n",
    "\n",
    "y_filtered_drop = yt[yt.y == 0][:9900]\n",
    "yt_filtered = yt.drop(index=y_filtered_drop.index)\n",
    "X_filtered = X.drop(index=y_filtered_drop.index)\n",
    "\n",
    "#print(y_filtered_drop.shape)\n",
    "#print(yt_filtered.shape)\n",
    "#print(X_filtered.shape)\n",
    "\n",
    "display(yt_filtered[\"y\"].value_counts())\n",
    "\n",
    "\n",
    "#df[\"est_label\"] = yt\n",
    "#display(pd.DataFrame(data=yt, columns=[\"y\"])[\"y\"].value_counts())\n",
    "#display(df[[*feature_list, \"label\", \"est_label\"]])\n",
    "#display(df[\"label\"])\n",
    "\n",
    "\n",
    "# Bad idea to do random split with so skewed data   \n",
    "#data = scaler.transform(data)\n",
    "#X_train, X_validate, y_train, y_validate = train_test_split(X, yt, test_size=0.7) \n",
    "\n",
    "\n",
    "# https://stats.stackexchange.com/questions/131255/class-imbalance-in-supervised-machine-learning\n",
    "# https://datascience.stackexchange.com/questions/32818/train-test-split-of-unbalanced-dataset-classification/32820#32820\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html\n",
    "skf = StratifiedKFold(n_splits=50, random_state=42, shuffle=True)\n",
    "# skf.get_n_splits(X, yt)\n",
    "balanced_accuracies = []\n",
    "accuracies = []\n",
    "predictions = np.array([])\n",
    "true_values = np.array([])\n",
    "\n",
    "for train_index, test_index in skf.split(X_filtered, yt_filtered):\n",
    "    # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_validate = X_filtered.iloc[train_index], X_filtered.iloc[test_index]\n",
    "    y_train, y_true = yt_filtered.iloc[train_index], yt_filtered.iloc[test_index]\n",
    "\n",
    "    #scaler = MinMaxScaler(min=0, max=1)\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(X_train, y_train) # Don't cheat: fit only training data. y is ignored, but needs same shape[rows]\n",
    "\n",
    "    model = MultinomialNB()\n",
    "\n",
    "    # Train the model using the training sets - scale only features (?)\n",
    "    model.fit(scaler.transform(X_train), y_train)\n",
    "\n",
    "    # Predic the model\n",
    "    y_pred = model.predict(scaler.transform(X_validate))\n",
    "\n",
    "    # At this point, the model can classify all 0 positively, but no more\n",
    "    accuracy = metrics.accuracy_score(y_true, y_pred, normalize=True)\n",
    "    # TODO: balanced - https://stackoverflow.com/questions/55838262/why-does-cross-validation-give-consistently-higher-scores-than-normal-fitting-an\n",
    "    balanced_accuracy = metrics.balanced_accuracy_score(y_true, y_pred, adjusted=False)\n",
    "    # TODO: https://stats.stackexchange.com/questions/99667/naive-bayes-with-unbalanced-classes\n",
    "\n",
    "    # display\n",
    "    \n",
    "    accuracies.append(accuracy)\n",
    "    balanced_accuracies.append(balanced_accuracy)\n",
    "    predictions = np.append(predictions, y_pred)\n",
    "    true_values = np.append(true_values, y_true)\n",
    "    # print(accuracy)\n",
    "\n",
    "    # accuracy_zeros = metrics.accuracy_score(y_validate, [0]*len(y_validate), normalize=True)\n",
    "    # print(accuracy_zeros)\n",
    "\n",
    "# TODO: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html\n",
    "\n",
    "print(\"\\n\\nMean:\", np.mean(accuracies))\n",
    "print(\"\\n\\nMean balanced:\", np.mean(balanced_accuracies))\n",
    "results = pd.DataFrame(data={\"pred\": predictions, \"true\": true_values})\n",
    "counts = pd.DataFrame(data={\"pred\": results[\"pred\"].value_counts(), \"true\": results[\"true\"].value_counts()})\n",
    "display(counts)\n",
    "#results[\"pred_values\"].value_counts()\n",
    "#results[\"true_values\"].value_counts()\n",
    "#display(results[[\"pred_values\", \"true_values\"]])\n"
   ]
  }
 ]
}